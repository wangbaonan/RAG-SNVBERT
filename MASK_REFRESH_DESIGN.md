# Dynamic Mask with Periodic Refresh - è®¾è®¡æ–¹æ¡ˆ

**ç›®æ ‡**: åœ¨ä¿æŒè¯­ä¹‰å¯¹é½çš„å‰æä¸‹ï¼Œæ”¯æŒmaskçš„æ•°æ®å¢å¼º

---

## ğŸ¯ è®¾è®¡åŸåˆ™

### ç”¨æˆ·çš„æ­£ç¡®æ€è·¯

1. **æ£€ç´¢é˜¶æ®µ**ï¼šQueryå’ŒReferenceéƒ½ç”¨ç›¸åŒçš„mask
   - ç›®çš„ï¼šè¯­ä¹‰å¯¹é½ï¼Œæ£€ç´¢åŸºäºç›¸åŒçš„"å·²çŸ¥ä½ç‚¹"

2. **ä½¿ç”¨é˜¶æ®µ**ï¼šRetrieved sequencesæ˜¯å®Œæ•´çš„ï¼ˆæ— maskï¼‰
   - ç›®çš„ï¼šä¸ºæ¨¡å‹æä¾›å®Œæ•´çš„å‚è€ƒä¿¡æ¯

3. **æ•°æ®å¢å¼º**ï¼šMask patternå®šæœŸå˜åŒ–
   - ç›®çš„ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆç‰¹å®šmask pattern
   - æ–¹æ³•ï¼šæ¯Nä¸ªbatchæˆ–æ¯Mä¸ªepoché‡æ–°ç”Ÿæˆmask

---

## ğŸ“‹ å®Œæ•´æµç¨‹

### åˆå§‹åŒ–é˜¶æ®µ

```python
# 1. ç”Ÿæˆåˆå§‹mask (seed=0)
for w_idx in range(window_count):
    np.random.seed(w_idx)  # æ¯ä¸ªçª—å£ä¸åŒseed
    raw_mask = self.generate_mask(window_len)
    self.window_masks[w_idx] = raw_mask

# 2. ç”¨åˆå§‹maskç¼–ç Reference (ç”¨äºæ£€ç´¢)
for w_idx in range(window_count):
    mask = self.window_masks[w_idx]
    ref_tokenized_masked = self.tokenize(raw_ref, mask)  # â† ç”¨mask!
    ref_emb_masked = embedding_layer(ref_tokenized_masked, af, pos=True)

    # æ„å»ºFAISSç´¢å¼•
    index.add(ref_emb_masked.flatten())
    self.embedding_indexes[w_idx] = index

# 3. åŒæ—¶ä¿å­˜å®Œæ•´çš„Reference (ç”¨äºåç»­è¿”å›)
for w_idx in range(window_count):
    ref_tokenized_complete = self.tokenize(raw_ref, zero_mask)  # â† æ— mask!
    ref_emb_complete = embedding_layer(ref_tokenized_complete, af, pos=True)
    self.ref_embeddings_complete[w_idx] = ref_emb_complete  # ä¿å­˜å®Œæ•´ç‰ˆæœ¬
```

### è®­ç»ƒé˜¶æ®µ

```python
for epoch in range(num_epochs):
    for batch_idx, batch in enumerate(dataloader):
        # === æ­¥éª¤1: æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°mask ===
        if should_refresh_mask(epoch, batch_idx):
            # é‡æ–°ç”Ÿæˆæ‰€æœ‰çª—å£çš„mask
            dataset.regenerate_masks(seed=get_current_seed(epoch, batch_idx))

            # é‡å»ºFAISSç´¢å¼• (ç”¨æ–°maskç¼–ç Reference)
            dataset.rebuild_indexes(embedding_layer)
            # æ³¨æ„: ref_embeddings_completeä¸éœ€è¦é‡å»ºï¼Œæ°¸è¿œæ˜¯å®Œæ•´çš„

        # === æ­¥éª¤2: æ£€ç´¢ (Queryå’ŒReferenceéƒ½ç”¨å½“å‰mask) ===
        # Queryç”¨å½“å‰mask
        query_masked = tokenize(query, current_mask)  # â† mask
        query_emb = embedding_layer(query_masked, af, pos=True)

        # æ£€ç´¢ (ç´¢å¼•ä¸­çš„Referenceä¹Ÿæ˜¯ç”¨ç›¸åŒmaskç¼–ç çš„)
        topk_indices = faiss_index.search(query_emb.flatten(), k=16)

        # === æ­¥éª¤3: è¿”å›å®Œæ•´çš„Reference ===
        retrieved_complete = []
        for idx in topk_indices:
            # ä»å®Œæ•´ç‰ˆæœ¬ä¸­è·å–
            retrieved_complete.append(ref_embeddings_complete[idx])  # â† æ— mask!

        # === æ­¥éª¤4: æ¨¡å‹ä½¿ç”¨å®Œæ•´Reference ===
        output = model(query_emb, retrieved_complete)
        loss = criterion(output, target)
        loss.backward()
```

---

## ğŸ”§ å…³é”®å®ç°ç»†èŠ‚

### 1. æ•°æ®ç»“æ„

```python
class EmbeddingRAGDataset:
    def __init__(self, ...):
        # Maskç›¸å…³
        self.window_masks = []  # å½“å‰mask pattern [window_count]
        self.mask_version = 0   # å½“å‰maskç‰ˆæœ¬å·

        # Referenceçš„ä¸¤ä¸ªç‰ˆæœ¬
        self.ref_tokens_masked = []    # ç”¨äºæ„å»ºç´¢å¼• (masked)
        self.ref_tokens_complete = []  # ç”¨äºè¿”å› (complete)

        self.ref_embeddings_complete = []  # å®Œæ•´embedding [window_count][num_haps, L, D]

        # FAISSç´¢å¼• (åŸºäºmasked embeddings)
        self.embedding_indexes = []  # [window_count]
```

### 2. Maskåˆ·æ–°å‡½æ•°

```python
def regenerate_masks(self, seed: int):
    """é‡æ–°ç”Ÿæˆæ‰€æœ‰çª—å£çš„mask"""
    self.mask_version += 1
    print(f"\n{'='*80}")
    print(f"â–£ åˆ·æ–°Mask (Version {self.mask_version}, Seed={seed})")
    print(f"{'='*80}")

    for w_idx in range(self.window_count):
        window_len = self.window.window_info[w_idx, 1] - \
                     self.window.window_info[w_idx, 0]

        # ç”Ÿæˆæ–°mask
        np.random.seed(seed * 10000 + w_idx)
        raw_mask = self.generate_mask(window_len)
        padded_mask = VCFProcessingModule.sequence_padding(raw_mask, dtype='int')

        self.window_masks[w_idx] = padded_mask

def rebuild_indexes(self, embedding_layer, device='cuda'):
    """ç”¨å½“å‰maské‡å»ºFAISSç´¢å¼•"""
    print(f"  â†’ é‡å»ºFAISSç´¢å¼• (åŸºäºæ–°mask)")
    start_time = time.time()

    with torch.no_grad():
        for w_idx in tqdm(range(self.window_count), desc="é‡å»ºç´¢å¼•"):
            # è·å–å®Œæ•´çš„reference tokens
            ref_tokens_complete = self.ref_tokens_complete[w_idx]  # [num_haps, L]
            ref_af = self.ref_af_windows[w_idx]  # [L]

            # åº”ç”¨å½“å‰mask
            current_mask = self.window_masks[w_idx]  # [L]
            ref_tokens_masked = self.apply_mask(ref_tokens_complete, current_mask)

            # ç”¨maskedç‰ˆæœ¬ç¼–ç  (ç”¨äºæ£€ç´¢)
            ref_tokens_tensor = torch.LongTensor(ref_tokens_masked).to(device)
            ref_af_tensor = torch.FloatTensor(ref_af).to(device)
            ref_emb_masked = embedding_layer(ref_tokens_tensor, af=ref_af_tensor, pos=True)

            # é‡å»ºç´¢å¼•
            ref_emb_flat = ref_emb_masked.reshape(num_haps, -1).cpu().numpy().astype(np.float32)
            self.embedding_indexes[w_idx].reset()
            self.embedding_indexes[w_idx].add(ref_emb_flat)

    print(f"  âœ“ é‡å»ºå®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}s")

def apply_mask(self, tokens, mask):
    """åº”ç”¨maskåˆ°tokenåºåˆ—"""
    masked_tokens = tokens.copy()
    mask_token_id = 4  # [MASK] token
    masked_tokens[mask == 1] = mask_token_id
    return masked_tokens
```

### 3. Collateå‡½æ•°ä¿®æ”¹

```python
def embedding_rag_collate_fn(batch_list, dataset, embedding_layer, k_retrieve, device='cuda'):
    """
    å…³é”®ä¿®æ”¹:
    1. Queryç”¨å½“å‰maskç¼–ç 
    2. æ£€ç´¢ (ç´¢å¼•ä¸­Referenceä¹Ÿæ˜¯ç”¨ç›¸åŒmask)
    3. è¿”å›å®Œæ•´çš„Reference embeddings
    """
    final_batch = defaultdict(list)

    # æŒ‰çª—å£åˆ†ç»„
    window_groups = defaultdict(list)
    for sample in batch_list:
        win_idx = int(sample['window_idx'])
        window_groups[win_idx].append(sample)

    with torch.no_grad():
        for win_idx, group in window_groups.items():
            index = dataset.embedding_indexes[win_idx]  # Masked index
            ref_emb_complete = dataset.ref_embeddings_complete[win_idx]  # Complete embeddings
            current_mask = dataset.window_masks[win_idx]  # å½“å‰mask

            # === Queryç¼–ç  (ç”¨å½“å‰mask) ===
            # æ³¨æ„: sample['hap_1'] å·²ç»åœ¨__getitem__ä¸­ç”¨ç›¸åŒmask tokenizedäº†
            h1_tokens = torch.stack([s['hap_1'] for s in group]).to(device)
            h2_tokens = torch.stack([s['hap_2'] for s in group]).to(device)
            af_batch = torch.stack([s['af'] for s in group]).to(device)

            h1_emb = embedding_layer(h1_tokens, af=af_batch, pos=True)
            h2_emb = embedding_layer(h2_tokens, af=af_batch, pos=True)

            # === æ£€ç´¢ (åœ¨masked space) ===
            h1_flat = h1_emb.reshape(B, -1).cpu().numpy().astype(np.float32)
            h2_flat = h2_emb.reshape(B, -1).cpu().numpy().astype(np.float32)

            D1, I1 = index.search(h1_flat, k=k_retrieve)
            D2, I2 = index.search(h2_flat, k=k_retrieve)

            # === è¿”å›å®Œæ•´Reference ===
            for i, sample in enumerate(group):
                topk_h1 = []
                for k in range(k_retrieve):
                    ref_idx = I1[i, k]
                    # è¿”å›å®Œæ•´ç‰ˆæœ¬! â† å…³é”®!
                    topk_h1.append(ref_emb_complete[ref_idx])
                sample['rag_emb_h1'] = torch.stack(topk_h1)

                topk_h2 = []
                for k in range(k_retrieve):
                    ref_idx = I2[i, k]
                    topk_h2.append(ref_emb_complete[ref_idx])
                sample['rag_emb_h2'] = torch.stack(topk_h2)

            # æ”¶é›†æ•°æ®
            for sample in group:
                for key in sample:
                    final_batch[key].append(sample[key])

    # Stack
    for key in final_batch:
        if key not in ["window_idx", "hap1_nomask", "hap2_nomask"]:
            final_batch[key] = torch.stack(final_batch[key])

    return dict(final_batch)
```

### 4. è®­ç»ƒè„šæœ¬ä¿®æ”¹

```python
# train_embedding_rag.py

# é…ç½®
REFRESH_MASK_EVERY_N_EPOCHS = 1  # æ¯ä¸ªepochåˆ·æ–°ä¸€æ¬¡
# æˆ–
# REFRESH_MASK_EVERY_N_BATCHES = 500  # æ¯500ä¸ªbatchåˆ·æ–°ä¸€æ¬¡

for epoch in range(args.epochs):
    # === Epochå¼€å§‹æ—¶åˆ·æ–°mask ===
    if epoch > 0 and epoch % REFRESH_MASK_EVERY_N_EPOCHS == 0:
        print(f"\n{'='*80}")
        print(f"â–£ Epoch {epoch}: åˆ·æ–°Maskå’Œç´¢å¼•")
        print(f"{'='*80}")

        # é‡æ–°ç”Ÿæˆmask
        rag_train_loader.regenerate_masks(seed=epoch)
        rag_val_loader.regenerate_masks(seed=epoch)  # éªŒè¯é›†ç”¨ç›¸åŒmask

        # é‡å»ºç´¢å¼• (ç”¨æ–°mask)
        rag_train_loader.rebuild_indexes(model.embedding, device=device)
        rag_val_loader.rebuild_indexes(model.embedding, device=device)

        print(f"âœ“ åˆ·æ–°å®Œæˆ!\n")

    # æ›´æ–°datasetçš„epoch (ç”¨äºdynamic mask seed)
    rag_train_loader.current_epoch = epoch
    rag_val_loader.current_epoch = epoch

    # === è®­ç»ƒ ===
    model.train()
    for batch_idx, batch in enumerate(train_dataloader):
        # (å¯é€‰) æ¯Nä¸ªbatchåˆ·æ–°
        # if batch_idx > 0 and batch_idx % REFRESH_MASK_EVERY_N_BATCHES == 0:
        #     rag_train_loader.regenerate_masks(seed=epoch * 10000 + batch_idx)
        #     rag_train_loader.rebuild_indexes(model.embedding, device)

        loss = train_step(batch)

    # === éªŒè¯ ===
    validate(...)

    # === Epochç»“æŸæ—¶åˆ·æ–°æ¨¡å‹embeddings (ä¿æŒåŸæœ‰é€»è¾‘) ===
    # è¿™é‡Œåˆ·æ–°çš„æ˜¯ref_embeddings_complete (å®Œæ•´ç‰ˆæœ¬)
    print(f"\n{'='*80}")
    print(f"â–£ Epoch {epoch+1}: åˆ·æ–°Reference Embeddings (å®Œæ•´ç‰ˆæœ¬)")
    print(f"{'='*80}")
    rag_train_loader.refresh_complete_embeddings(model.embedding, device)
    print(f"âœ“ å®Œæˆ!\n")
```

---

## âš–ï¸ åˆ·æ–°é¢‘ç‡çš„Trade-off

### Option A: æ¯ä¸ªEpochåˆ·æ–°ä¸€æ¬¡ (æ¨è)

```python
REFRESH_MASK_EVERY_N_EPOCHS = 1
```

**ä¼˜ç‚¹**:
- è®¡ç®—å¼€é”€å¯æ§ (æ¯ä¸ªepoch ~8åˆ†é’Ÿé¢å¤–å¼€é”€)
- è¶³å¤Ÿçš„æ•°æ®å¢å¼º (20 epochs = 20ä¸ªä¸åŒmask)
- å®ç°ç®€å•

**ç¼ºç‚¹**:
- æ¯ä¸ªepochå†…maskå›ºå®š
- æ•°æ®å¢å¼ºæ•ˆæœæœ‰é™

**é€‚ç”¨**: å¤§å¤šæ•°æƒ…å†µï¼Œå¹³è¡¡æ€§èƒ½å’Œå¼€é”€

---

### Option B: æ¯Nä¸ªBatchåˆ·æ–°ä¸€æ¬¡

```python
REFRESH_MASK_EVERY_N_BATCHES = 500  # ~æ¯2å°æ—¶åˆ·æ–°ä¸€æ¬¡
```

**ä¼˜ç‚¹**:
- æ›´é¢‘ç¹çš„æ•°æ®å¢å¼º
- é˜²æ­¢overfittingåˆ°ç‰¹å®šmask

**ç¼ºç‚¹**:
- è®¡ç®—å¼€é”€æ›´é«˜
- è®­ç»ƒå¯èƒ½æ›´æ…¢

**é€‚ç”¨**: å¦‚æœå‘ç°è¿‡æ‹Ÿåˆmask pattern

---

### Option C: è‡ªé€‚åº”åˆ·æ–°

```python
# æ ¹æ®éªŒè¯é›†æ€§èƒ½å†³å®šæ˜¯å¦åˆ·æ–°
if val_f1_plateau_for_N_epochs:
    # æ€§èƒ½åœæ» â†’ åˆ·æ–°maskå°è¯•escape
    refresh_mask()
```

**ä¼˜ç‚¹**:
- æ™ºèƒ½ï¼Œåªåœ¨éœ€è¦æ—¶åˆ·æ–°
- æœ€å°åŒ–ä¸å¿…è¦çš„è®¡ç®—

**ç¼ºç‚¹**:
- å®ç°å¤æ‚
- å¯èƒ½ä¸ç¨³å®š

---

## ğŸ“Š æ€§èƒ½é¢„ä¼°

### åˆå§‹åŒ– (é¦–æ¬¡)

```
åŸæ¥: 15åˆ†é’Ÿ (æ„å»ºmaskedç´¢å¼• + æ„å»ºcomplete embeddings)
ç°åœ¨: 18åˆ†é’Ÿ (å¤šä¸€æ¬¡complete embeddingsçš„ç¼–ç )
å¢åŠ : +20%
```

### æ¯ä¸ªEpoch

```
è®­ç»ƒ: 1.3å°æ—¶ (ä¸å˜)
åˆ·æ–°mask + é‡å»ºç´¢å¼•: ~8åˆ†é’Ÿ (ä¸åŸæ¥çš„refresh_embeddingsç›¸åŒ)
åˆ·æ–°complete embeddings: ~8åˆ†é’Ÿ (åŸæœ‰é€»è¾‘)
æ€»è®¡: ~1.5å°æ—¶/epoch
```

### æ¯500 Batchåˆ·æ–° (å¦‚æœé€‰æ‹©Option B)

```
è®­ç»ƒ500 batch: ~20åˆ†é’Ÿ
åˆ·æ–°mask + é‡å»ºç´¢å¼•: ~8åˆ†é’Ÿ
æ€»è®¡: ~28åˆ†é’Ÿ/500batch
å¼€é”€: +40%
```

---

## ğŸ¯ æ¨èé…ç½®

### åˆæœŸè®­ç»ƒ (æ¢ç´¢é˜¶æ®µ)

```python
# æ¯ä¸ªepochåˆ·æ–°ä¸€æ¬¡
REFRESH_MASK_EVERY_N_EPOCHS = 1
REFRESH_EMBEDDINGS_EVERY_N_EPOCHS = 1  # åŸæœ‰é€»è¾‘

# é¢„æœŸ:
# - æ¯ä¸ªepoch: ~1.5å°æ—¶
# - 20 epochs: ~30å°æ—¶
# - 20ä¸ªä¸åŒçš„mask pattern
```

### å¦‚æœè¿‡æ‹Ÿåˆ

```python
# æ›´é¢‘ç¹åˆ·æ–°
REFRESH_MASK_EVERY_N_BATCHES = 500
```

---

## ğŸ” ä¸V17å¯¹æ¯”

| ç‰¹æ€§ | V17 (ä¿®å¤å) | V18 (æ­¤æ–¹æ¡ˆ) |
|------|-------------|-------------|
| **Maskå¯¹é½** | âœ… Reference=Query | âœ… Reference=Query (æ£€ç´¢é˜¶æ®µ) |
| **å®Œæ•´ä¿¡æ¯** | âŒ Referenceæ°¸è¿œmasked | âœ… è¿”å›å®Œæ•´Reference |
| **Dynamic mask** | âŒ ä¸æ”¯æŒ | âœ… å®šæœŸåˆ·æ–° |
| **æ•°æ®å¢å¼º** | âŒ | âœ… 20ä¸ªmask (if æ¯epochåˆ·æ–°) |
| **è®¡ç®—å¼€é”€** | 4.2h/epoch | 1.5h/epoch (ä»å¿«3x) |

---

## ğŸ“ æ€»ç»“

### å…³é”®è®¾è®¡

1. **ä¸¤å¥—Reference Embeddings**:
   - Maskedç‰ˆæœ¬ï¼šç”¨äºæ„å»ºFAISSç´¢å¼•å’Œæ£€ç´¢
   - Completeç‰ˆæœ¬ï¼šç”¨äºè¿”å›ç»™æ¨¡å‹

2. **å®šæœŸåˆ·æ–°Mask**:
   - æ¯Nä¸ªepochæˆ–æ¯Mä¸ªbatch
   - é‡æ–°ç”Ÿæˆmask â†’ é‡å»ºç´¢å¼• (maskedç‰ˆæœ¬)
   - Completeç‰ˆæœ¬ä¸éœ€è¦é‡å»º

3. **ä¿æŒè¯­ä¹‰å¯¹é½**:
   - Queryå’ŒReferenceéƒ½ç”¨ç›¸åŒçš„maskç¼–ç  (æ£€ç´¢æ—¶)
   - æ£€ç´¢åè¿”å›å®Œæ•´Reference (ä½¿ç”¨æ—¶)

### å®ç°å¤æ‚åº¦

- **ä»£ç æ”¹åŠ¨**: ä¸­ç­‰ (~200è¡Œ)
- **è®¡ç®—å¼€é”€**: +20% (å¦‚æœæ¯epochåˆ·æ–°)
- **æ”¶ç›Š**: æ•°æ®å¢å¼º + è¯­ä¹‰å¯¹é½ + å®Œæ•´ä¿¡æ¯

---

**å»ºè®®**: å…ˆå®ç°Option A (æ¯epochåˆ·æ–°)ï¼Œè¿è¡Œå®éªŒï¼Œæ ¹æ®ç»“æœå†³å®šæ˜¯å¦éœ€è¦æ›´é¢‘ç¹åˆ·æ–°ã€‚
