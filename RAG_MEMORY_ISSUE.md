# RAGç»„ä»¶å†…å­˜çˆ†ç‚¸é—®é¢˜åˆ†æ

## ğŸ”´ é—®é¢˜ç°è±¡

**é…ç½®**: dims=192, layers=10, heads=6, batch=64
**GPU**: 81GB A100
**ç»“æœ**: OOM trying to allocate 1.52 GiB

**è¿™æä¸æ­£å¸¸** - ç†è®ºä¸Šè¿™ä¸ªé…ç½®åªéœ€è¦~9GBå†…å­˜ã€‚

---

## ğŸ” æ ¹æœ¬åŸå› 

### RAGç»„ä»¶çš„éšè—å†…å­˜æ¶ˆè€—

**ä»£ç ä½ç½®**: `src/model/bert.py` Line 86-113

```python
def encode_rag_segments(self, rag_h1, rag_h2, rag_pos, rag_af, rag_type_idx):
    # rag_h1, rag_h2: [B, L] - retrieved reference sequences

    # å…³é”®é—®é¢˜: è¿™é‡ŒæŠŠretrieved sequencesè¿‡å®Œæ•´BERT!
    rag_h1 = self.embedding(rag_h1, rag_pos, rag_af, rag_type_idx)  # [B, L, D]
    rag_h2 = self.embedding(rag_h2, rag_pos, rag_af, rag_type_idx)

    # è¿‡10å±‚Transformer! (æ¯å±‚éƒ½ä¿ç•™ä¸­é—´æ¿€æ´»ç”¨äºbackward)
    for transformer in self.transformer_blocks:
        rag_h1 = transformer.forward(rag_h1)  # 10å±‚!
        rag_h2 = transformer.forward(rag_h2)

    return rag_h1, rag_h2  # [B, L, D]
```

**é—®é¢˜åˆ†æ**:

1. **æ¯ä¸ªbatchéƒ½è¦ç¼–ç RAG sequences**:
   - Original sequences (h1, h2): è¿‡BERTä¸€æ¬¡
   - Retrieved sequences (rag_h1, rag_h2): **åˆè¿‡BERTä¸€æ¬¡**
   - ç›¸å½“äºæ¯ä¸ªbatchè¦è¿‡2å€çš„BERT forward

2. **ä¸­é—´æ¿€æ´»ä¿ç•™ç”¨äºbackward**:
   ```
   æ¯å±‚Transformerä¿ç•™çš„æ¿€æ´»:
   - Attention scores: [B, heads, L, L]
   - Attention output: [B, L, D]
   - FFN intermediate: [B, L, 4D]

   10å±‚ Ã— 2ä¸ªhaplotype Ã— ä¸Šè¿°æ¿€æ´» = å·¨å¤§å†…å­˜
   ```

3. **å®é™…å†…å­˜æ¶ˆè€—**:
   ```
   Batch=64, L=1030, D=192, Heads=6, Layers=10

   æ¯ä¸ªsequenceçš„æ¿€æ´»å†…å­˜:
   - Attention scores: 64 * 6 * 1030 * 1030 * 4 = 1.6 GB
   - Layer outputs: 64 * 1030 * 192 * 4 * 10 = 500 MB
   - FFN intermediate: 64 * 1030 * 768 * 4 * 10 = 2 GB

   Original sequences (h1 + h2): 4.1 GB
   RAG sequences (rag_h1 + rag_h2): 4.1 GB
   Total forward: 8.2 GB

   Backward (æ¢¯åº¦): 8.2 GB
   Mixed precision copies: 2 GB
   Temporary buffers: 2 GB

   Total: ~20 GB per batch (ä¸æ˜¯é¢„æœŸçš„9GB!)
   ```

---

## ğŸ“Š å®Œæ•´å†…å­˜åˆ†è§£

### ç»„ä»¶1: æ¨¡å‹å‚æ•° (~75 MB)

```
Embedding: 5 * 192 = 960 params
Transformer (10å±‚):
  - Self-attention: 192^2 * 4 * 6 = 884K per layer
  - FFN: 192 * 768 * 2 = 295K per layer
  - Total per layer: ~1.2M
  - 10 layers: 12M
Classifiers: ~1M
Total: ~15M params * 4 bytes = 60 MB (float32)
        ~15M params * 2 bytes = 30 MB (mixed precision)
```

### ç»„ä»¶2: Batchæ•°æ® (~3 MB)

```
Input tensors: [B, L]
- hap_seq, af, pos, type_idx, etc.
- ~10 fields Ã— 64 Ã— 1030 Ã— 4 bytes = 2.6 MB
```

### ç»„ä»¶3: Forward Activations (8-20 GB!)

**Original sequences**:
```
h1, h2ç»è¿‡embedding + 10å±‚Transformer
æ¯å±‚ä¿ç•™æ¿€æ´»:
- Attention: 64 * 6 * 1030^2 * 4 = 1.6 GB
- Outputs: 64 * 1030 * 192 * 4 = 50 MB
- FFN: 64 * 1030 * 768 * 4 = 200 MB

Per layer: ~1.85 GB
10 layers: 18.5 GB
2 haplotypes: 18.5 GB (å…±äº«æƒé‡ä½†ä¸å…±äº«æ¿€æ´»)
```

**RAG sequences** (è‡´å‘½é—®é¢˜):
```
rag_h1, rag_h2ä¹Ÿè¦è¿‡å®Œæ•´BERT!
åˆæ˜¯ 18.5 GB

Total: 37 GB ä»…forwardæ¿€æ´»!
```

### ç»„ä»¶4: Backward Gradients (~37 GB)

- æ¯ä¸ªæ¿€æ´»éƒ½éœ€è¦å­˜å‚¨æ¢¯åº¦
- ä¸forwardç›¸åŒå¤§å°

### ç»„ä»¶5: Optimizer States

```
Adam optimizer:
- Momentum: 15M params * 4 = 60 MB
- Velocity: 15M params * 4 = 60 MB
Total: ~120 MB (negligible)
```

### ç»„ä»¶6: Gradient Accumulation

```
å¦‚æœgrad_accum_steps=4:
- éœ€è¦ç´¯ç§¯4ä¸ªbatchçš„æ¢¯åº¦
- é¢å¤–å†…å­˜: 4 * 37 GB = 148 GB!
```

### ç»„ä»¶7: CUDAå†…å­˜ç¢ç‰‡åŒ– (+30%)

```
å®é™…åˆ†é…æ¯”ç†è®ºå€¼é«˜30%
Total Ã— 1.3
```

---

## ğŸ’£ ä¸ºä»€ä¹ˆ81GBä¸å¤Ÿ

**é…ç½®**: batch=64, dims=192, layers=10

```
Forward (original + RAG): 37 GB
Backward (gradients): 37 GB
Mixed precision copies: 4 GB
Temporary tensors: 5 GB
Subtotal: 83 GB â† å·²ç»è¶…è¿‡81GB!

å¦‚æœå¯ç”¨grad_accum:
+ Gradient buffer: 37 GB
Total: 120 GB â† è¿œè¶…GPUå®¹é‡
```

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: æå°Batch (ç«‹å³å¯ç”¨)

```bash
--train_batch_size 16   # 64 â†’ 16 (å‡å°‘75%)
--grad_accum_steps 4    # ä¿æŒç­‰æ•ˆbatch=64
```

**å†…å­˜è®¡ç®—**:
```
Forward: 37 * (16/64) = 9.25 GB
Backward: 9.25 GB
Total: ~22 GB (å®‰å…¨!)
```

**ç¼ºç‚¹**: è®­ç»ƒé€Ÿåº¦æ…¢4å€

---

### æ–¹æ¡ˆ2: ç¦ç”¨RAG (æµ‹è¯•ç”¨)

å¦‚æœåªæ˜¯æƒ³éªŒè¯æ¨¡å‹è®­ç»ƒ,å¯ä»¥ä¸´æ—¶ç¦ç”¨RAG:

ä¿®æ”¹ `src/model/bert.py`:
```python
def forward(self, x):
    # ...
    # ä¸´æ—¶æ³¨é‡ŠRAGéƒ¨åˆ†
    # rag_h1_encoded, rag_h2_encoded = self.encode_rag_segments(...)
    rag_h1_encoded = None
    rag_h2_encoded = None
    # ...
```

**æ•ˆæœ**: å†…å­˜å‡åŠ (37 GB â†’ 18.5 GB)

---

### æ–¹æ¡ˆ3: é¢„ç¼–ç RAG Sequences (æœ€ä¼˜,éœ€è¦ä»£ç æ”¹åŠ¨)

**æ€è·¯**: åœ¨datasetåˆå§‹åŒ–æ—¶é¢„å…ˆç¼–ç æ‰€æœ‰reference sequences,å­˜å‚¨embeddingè€Œä¸æ˜¯raw tokens

**ä¿®æ”¹ `src/dataset/rag_train_dataset.py`**:

```python
def _build_faiss_indexes(self, ref_vcf_path):
    # åŠ è½½reference data
    ref_gt, ref_pos = self._load_ref_data(ref_vcf_path)

    # æ–°å¢: é¢„ç¼–ç æ‰€æœ‰reference sequences
    print("Pre-encoding reference sequences...")
    self.ref_embeddings = []

    with torch.no_grad():
        for w_idx in range(self.window_count):
            # è·å–è¯¥windowçš„reference sequences
            ref_seqs = ...  # [num_refs, seq_len]

            # è¿‡BERTä¸€æ¬¡,å­˜å‚¨embedding
            ref_emb = self.bert_model.embedding(ref_seqs, ...)
            self.ref_embeddings.append(ref_emb.cpu())  # å­˜åœ¨CPU

    print("âœ“ Pre-encoded all reference sequences")
```

**ä¿®æ”¹ `src/model/bert.py`**:

```python
def forward(self, x, pre_encoded_rag=None):
    # ...
    if pre_encoded_rag is not None:
        # ç›´æ¥ä½¿ç”¨é¢„ç¼–ç çš„embedding,è·³è¿‡BERT
        rag_h1_encoded = pre_encoded_rag['h1']  # [B, L, D]
        rag_h2_encoded = pre_encoded_rag['h2']
    else:
        # åŸé€»è¾‘
        rag_h1_encoded, rag_h2_encoded = self.encode_rag_segments(...)
```

**æ•ˆæœ**:
- å†…å­˜: 37 GB â†’ 20 GB (å‡å°‘45%)
- é€Ÿåº¦: æå‡30% (ä¸éœ€è¦é‡å¤ç¼–ç )

---

### æ–¹æ¡ˆ4: Gradient Checkpointing

å¯ç”¨PyTorchçš„gradient checkpointing,tradeè®¡ç®—æ¢å†…å­˜:

```python
# åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶
from torch.utils.checkpoint import checkpoint

class TransformerBlock(nn.Module):
    def forward(self, x):
        # ä½¿ç”¨checkpointing
        return checkpoint(self._forward, x)

    def _forward(self, x):
        # åŸforwardé€»è¾‘
        ...
```

**æ•ˆæœ**: å†…å­˜å‡å°‘~50%,è®¡ç®—æ—¶é—´å¢åŠ ~30%

---

## ğŸš€ ç«‹å³è¿è¡Œ (æ–¹æ¡ˆ1)

```bash
cd /cpfs01/projects-HDD/humPOG_HDD/wbn_24110700074/RAG_Version/VCF-Bert/00_RAG-SNVBERT-packup
git pull origin main
bash run_v17_extreme_memory_fix.sh
```

**é…ç½®**:
- batch_size=16 (ä»64é™åˆ°1/4)
- grad_accum_steps=4 (ç­‰æ•ˆbatchä»ç„¶æ˜¯64)
- é¢„æœŸå†…å­˜: ~22 GB (å®‰å…¨èŒƒå›´)

**ç¼ºç‚¹**:
- è®­ç»ƒé€Ÿåº¦æ…¢4å€
- ä½†è‡³å°‘èƒ½è·‘èµ·æ¥

---

## ğŸ“ˆ é•¿æœŸä¼˜åŒ– (æ–¹æ¡ˆ3)

å®ç°RAGé¢„ç¼–ç éœ€è¦:

1. ä¿®æ”¹ `rag_train_dataset.py` (_build_faiss_indexes)
2. ä¿®æ”¹ `bert.py` (forward)
3. ä¿®æ”¹ `rag_train_dataset.py` (__getitem__)
4. æµ‹è¯•éªŒè¯

**é¢„è®¡å·¥ä½œé‡**: 2-3å°æ—¶
**æ”¶ç›Š**:
- å†…å­˜å‡å°‘45%
- é€Ÿåº¦æå‡30%
- batch_sizeå¯ä»¥æé«˜åˆ°32-48

---

## ğŸ”¬ è¯Šæ–­å‘½ä»¤

### æ£€æŸ¥å®é™…GPUä½¿ç”¨

```bash
# è¿è¡Œè®­ç»ƒæ—¶,å¦ä¸€ä¸ªç»ˆç«¯æ‰§è¡Œ:
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¯¦ç»†å†…å­˜åˆ†é…
nvidia-smi --query-gpu=memory.used,memory.free --format=csv -l 1
```

### åœ¨ä»£ç ä¸­æ‰“å°å†…å­˜

åœ¨ `pretrain_with_val_optimized.py` çš„training loopä¸­æ·»åŠ :

```python
import torch

def _run_epoch(self, epoch, dataloader, train=True):
    for i, data in enumerate(data_iter):
        # åœ¨forwardå‰
        if i % 100 == 0:
            print(f"Before forward - GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB")

        output = self.model(gpu_data)

        # åœ¨forwardå
        if i % 100 == 0:
            print(f"After forward - GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB")

        # backward
        loss.backward()

        # åœ¨backwardå
        if i % 100 == 0:
            print(f"After backward - GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB")
```

---

## ğŸ“Š é¢„æœŸå†…å­˜ä½¿ç”¨å¯¹æ¯”

| é…ç½® | Forward | Backward | Total | çŠ¶æ€ |
|------|---------|----------|-------|------|
| v16 (batch=64) | 37 GB | 37 GB | 83 GB | âŒ OOM |
| v17 (batch=16, accum=4) | 9 GB | 9 GB | 22 GB | âœ… OK |
| v16 + é¢„ç¼–ç RAG | 20 GB | 20 GB | 45 GB | âœ… OK |
| v16 + Checkpointing | 18 GB | 18 GB | 40 GB | âœ… OK |

---

## ğŸ¯ æ€»ç»“

**é—®é¢˜**: RAGç»„ä»¶å¯¹retrieved sequencesä¹Ÿè¿‡å®Œæ•´BERT,å¯¼è‡´å†…å­˜ç¿»å€

**ç«‹å³æ–¹æ¡ˆ**: batch=16, grad_accum=4 (run_v17)

**é•¿æœŸæ–¹æ¡ˆ**: é¢„ç¼–ç RAG sequences,å­˜å‚¨embedding

**æ ¹æœ¬é—®é¢˜**: RAGè®¾è®¡æ²¡æœ‰è€ƒè™‘å†…å­˜ä¼˜åŒ–,æ¯ä¸ªbatchéƒ½é‡å¤ç¼–ç ç›¸åŒçš„reference sequences

---

**åˆ›å»ºæ—¶é—´**: 2025-12-02
**GPU**: 81GB A100
**é—®é¢˜**: å³ä½¿81GBä¹ŸOOM
**æ ¹å› **: RAGå¯¹retrieved seqsè¿‡å®Œæ•´BERT,å†…å­˜æ¶ˆè€—ç¿»å€
