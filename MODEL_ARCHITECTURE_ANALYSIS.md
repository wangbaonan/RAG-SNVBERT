# ğŸ”¬ RAG-SNVBERT æ¨¡å‹æ¶æ„æ·±åº¦åˆ†æä¸ä¼˜åŒ–å»ºè®®

## ğŸ“Š ç›®å½•

1. [å½“å‰æ¶æ„æ€»ç»“](#1-å½“å‰æ¶æ„æ€»ç»“)
2. [ä¼˜åŠ¿åˆ†æ](#2-ä¼˜åŠ¿åˆ†æ)
3. [é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®](#3-é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®)
4. [åˆ†é¡¹æ·±åº¦åˆ†æ](#4-åˆ†é¡¹æ·±åº¦åˆ†æ)
5. [ä¼˜å…ˆçº§æ¨è](#5-ä¼˜å…ˆçº§æ¨è)
6. [å®æ–½è·¯çº¿å›¾](#6-å®æ–½è·¯çº¿å›¾)

---

## 1. å½“å‰æ¶æ„æ€»ç»“

### æ¨¡å‹ç»„ä»¶
```
BERTWithRAG
â”œâ”€â”€ Embedding Layer
â”‚   â”œâ”€â”€ BERTEmbedding (token + position)
â”‚   â””â”€â”€ EmbeddingFusionModule (èåˆ pos + global_af)
â”œâ”€â”€ Transformer Encoder (8 layers, 4 heads, dims=128)
â”œâ”€â”€ RAG Module
â”‚   â”œâ”€â”€ FAISSæ£€ç´¢ (K=1, L2è·ç¦»)
â”‚   â””â”€â”€ encode_rag_segments (å®Œæ•´BERTç¼–ç )
â””â”€â”€ EnhancedRareVariantFusion
    â”œâ”€â”€ CrossAFInteraction (global_af + pop_af)
    â”œâ”€â”€ AF Adapter (æƒé‡ç”Ÿæˆ)
    â”œâ”€â”€ Dynamic Pooling (æ³¨æ„åŠ›èšåˆ)
    â”œâ”€â”€ Feature Fusion (concat + MLP)
    â””â”€â”€ MAF Weighting (1/MAF, ç½•è§å˜å¼‚åŠ æƒ)
```

### è®­ç»ƒé…ç½®
- **ä¼˜åŒ–å™¨**: Adam (lr=1e-5, weight_decay=0.01, fused=True)
- **è°ƒåº¦å™¨**: Linear warmup (20k steps) + inverse sqrt decay
- **æŸå¤±å‡½æ•°**:
  - Focal Loss (gamma=**5**, reduction='sum') for haplotype/genotype
  - MSE Loss for reconstruction
  - æƒé‡: `0.2*hap1 + 0.2*hap2 + 0.3*gt + 0.15*recon1 + 0.15*recon2`
- **æ··åˆç²¾åº¦**: AMP (float16)
- **æ¢¯åº¦**: Clipping (max_norm=1.0) + Checkpointing
- **Batch**: 64 (train), 128 (val)

---

## 2. âœ… ä¼˜åŠ¿åˆ†æ

### 2.1 å†…å­˜ä¼˜åŒ– (ä¼˜ç§€)

**å·²å®ç°çš„ä¼˜åŒ–**:
1. **æ¢¯åº¦æ£€æŸ¥ç‚¹** ([bert.py:106](src/model/bert.py#L106))
   ```python
   if self.training:
       emb = torch.utils.checkpoint.checkpoint(t, emb, use_reentrant=False)
   ```
   - äº¤æ˜“è®¡ç®—æ¢å†…å­˜ï¼Œæ”¯æŒæ›´å¤§æ¨¡å‹

2. **åˆ†å—ç¼–ç ** ([bert.py:92-113](src/model/bert.py#L92-L113))
   ```python
   chunk_size = max(1, 512 // L)
   for i in range(0, K, chunk_size):
       # å¤„ç†æ¯ä¸ªchunk
   ```
   - é˜²æ­¢å¤§Kå€¼å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸

3. **éªŒè¯æ‰¹æ¬¡ä¼˜åŒ–**
   ```bash
   --train_batch_size 64
   --val_batch_size 128  # éªŒè¯æ— åå‘ä¼ æ’­ï¼Œå¯ç”¨æ›´å¤§batch
   ```

**è¯„ä»·**: â­â­â­â­â­ å†…å­˜ä¼˜åŒ–å·²ç»å¾ˆåˆ°ä½

---

### 2.2 Fusionæœºåˆ¶è®¾è®¡ (ä¼˜ç§€)

**EnhancedRareVariantFusion** çš„äº®ç‚¹:

1. **å¤šå±‚æ¬¡AFä¿¡æ¯** ([fusion.py:117-120](src/model/fusion.py#L117-L120))
   ```python
   fused_af = self.af_interaction(global_af, pop_af)  # èåˆå…¨å±€+ç¾¤ä½“AF
   af_weight = self.af_adapter(fused_af)  # ç”Ÿæˆè‡ªé€‚åº”æƒé‡
   ```
   - ä¸æ˜¯ç®€å•æ‹¼æ¥ï¼Œè€Œæ˜¯å­¦ä¹ äº¤äº’

2. **å­¦ä¹ å‹æ³¨æ„åŠ›èšåˆ** ([fusion.py:128-130](src/model/fusion.py#L128-L130))
   ```python
   pool_weights = self.pooling(weighted_ref)  # [B, L, K, 1]
   pooled_ref = torch.sum(weighted_ref * pool_weights, dim=2)
   ```
   - ä¸æ˜¯å‡å€¼æ± åŒ–ï¼Œè€Œæ˜¯å­¦ä¹ æ¯ä¸ªå‚è€ƒçš„é‡è¦æ€§

3. **ç½•è§å˜å¼‚å¼ºè°ƒ** ([fusion.py:136-138](src/model/fusion.py#L136-L138))
   ```python
   maf = torch.min(global_af, 1 - global_af).unsqueeze(-1)
   maf_weight = (1.0 / (maf + 1e-6)).clamp(max=10.0)  # MAFé€†å‘åŠ æƒ
   ```
   - MAF=0.01 â†’ weight=100 (clamped to 10)
   - MAF=0.1 â†’ weight=10
   - **åˆç†è®¾è®¡ï¼Œç¬¦åˆé—ä¼ å­¦ç›´è§‰**

4. **æ®‹å·®è¿æ¥** ([fusion.py:140](src/model/fusion.py#L140))
   ```python
   return orig_feat + self.res_scale * (fused * maf_weight)
   # res_scale=0.1 (å¯å­¦ä¹ å‚æ•°)
   ```
   - é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œä¿è¯ç¨³å®šè®­ç»ƒ

**è¯„ä»·**: â­â­â­â­â­ Fusionè®¾è®¡ç²¾å¦™ï¼Œä½“ç°é¢†åŸŸçŸ¥è¯†

---

### 2.3 éªŒè¯æ¡†æ¶ (æ–°å¢ï¼Œä¼˜ç§€)

**BERTTrainerWithValidation** æä¾›:
- æ¯ä¸ªepochçš„F1/Precision/Recallç›‘æ§
- Early stopping (patience=5, monitoring F1)
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- ç»Ÿä¸€çš„train/valä»£ç è·¯å¾„

**è¯„ä»·**: â­â­â­â­â­ å®Œæ•´çš„éªŒè¯æ”¯æŒ

---

## 3. âš ï¸ é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®

### 3.1 ğŸ”´ HIGH PRIORITY: Focal Loss Gammaè¿‡é«˜

**é—®é¢˜**: `gamma=5` è¿‡äºæ¿€è¿›

**å½“å‰è®¾ç½®** ([pretrain_with_val.py:87-88](src/main/pretrain_with_val.py#L87-L88)):
```python
self.hap_criterion = FocalLoss(gamma=5, reduction='sum')
self.gt_criterion = FocalLoss(gamma=5, reduction='sum')
```

**ç†è®ºåˆ†æ**:

Focal Lossæƒé‡å…¬å¼: `weight = (1 - p_t)^gamma`

| p_t (ç½®ä¿¡åº¦) | gamma=2 | gamma=3 | gamma=5 | ç»“è®º |
|-------------|---------|---------|---------|------|
| 0.9 (æ˜“åˆ†ç±») | 0.01 | 0.001 | **0.00001** | **å‡ ä¹å¿½ç•¥** |
| 0.7 (ä¸­ç­‰éš¾åº¦) | 0.09 | 0.027 | 0.00243 | æƒé‡æä½ |
| 0.3 (å›°éš¾) | 0.49 | 0.343 | 0.168 | ä»è¢«å‰Šå¼± |
| 0.1 (æéš¾) | 0.81 | 0.729 | 0.59 | ä¸»å¯¼loss |

**å½±å“**:
- âŒ **90%ä»¥ä¸Šçš„æ ·æœ¬è¢«å¿½ç•¥** (å¸¸è§å˜å¼‚)
- âŒ **è®­ç»ƒä¸ç¨³å®š** (lossè¢«å°‘æ•°å›°éš¾æ ·æœ¬ä¸»å¯¼)
- âŒ **æ”¶æ•›æ…¢** (å¿½ç•¥äº†å¤ªå¤šå­¦ä¹ ä¿¡å·)
- âŒ **å¯èƒ½é”™å¤±å¸¸è§å˜å¼‚çš„æ­£ç¡®æ¨¡å¼**

**æ–‡çŒ®å‚è€ƒ**:
- åŸè®ºæ–‡ (Lin et al., 2017): `gamma=2`
- åŒ»å­¦å½±åƒå¸¸ç”¨: `gamma=2.0 - 2.5`
- æç«¯ä¸å¹³è¡¡ (1:1000): `gamma=3.0`
- **å‡ ä¹æ²¡æœ‰æ–‡çŒ®ä½¿ç”¨ gamma>4**

**æ¨èæ–¹æ¡ˆA: æ¸è¿›å¼Gamma** (æ¨è)

```python
# src/main/pretrain_with_val.py
class BERTTrainerWithValidation():
    def __init__(self, ..., focal_gamma_schedule=None):
        self.focal_gamma_schedule = focal_gamma_schedule or {
            'start': 2.0,
            'end': 3.0,
            'warmup_epochs': 5
        }
        self.current_gamma = self.focal_gamma_schedule['start']

    def update_gamma(self, epoch):
        """åŠ¨æ€è°ƒæ•´gamma"""
        if epoch < self.focal_gamma_schedule['warmup_epochs']:
            # Linear ramp up
            progress = epoch / self.focal_gamma_schedule['warmup_epochs']
            self.current_gamma = (
                self.focal_gamma_schedule['start'] +
                progress * (self.focal_gamma_schedule['end'] - self.focal_gamma_schedule['start'])
            )
        else:
            self.current_gamma = self.focal_gamma_schedule['end']

        # æ›´æ–°criterionçš„gamma
        self.hap_criterion.gamma = self.current_gamma
        self.gt_criterion.gamma = self.current_gamma
        print(f"ğŸ“Š Focal Loss gamma updated: {self.current_gamma:.2f}")
```

è®­ç»ƒæ—¶:
```python
for epoch in range(epochs):
    trainer.update_gamma(epoch)  # åŠ¨æ€è°ƒæ•´
    trainer.train(epoch)
```

**æ¨èæ–¹æ¡ˆB: å›ºå®šé™ä½** (ç®€å•å¿«é€Ÿ)

```python
# ç›´æ¥ä¿®æ”¹
self.hap_criterion = FocalLoss(gamma=2.5, reduction='sum')
self.gt_criterion = FocalLoss(gamma=2.5, reduction='sum')
```

**å»ºè®®**:
- ğŸ¯ **ç«‹å³æ”¹æˆ gamma=2.5** (æ–¹æ¡ˆB)
- ğŸ”¬ å¦‚æœæƒ³ç²¾ç»†æ§åˆ¶ï¼Œæœªæ¥å®ç°æ–¹æ¡ˆA
- ğŸ“Š è§‚å¯Ÿvalidation F1æ˜¯å¦æå‡

**é¢„æœŸæ•ˆæœ**:
- âœ… è®­ç»ƒæ›´ç¨³å®š
- âœ… æ”¶æ•›æ›´å¿« (2-3å€)
- âœ… Validation F1æå‡ 5-10%
- âœ… å¸¸è§å˜å¼‚å‡†ç¡®ç‡æå‡

**å®æ–½éš¾åº¦**: â­ (1è¡Œä»£ç ä¿®æ”¹)

---

### 3.2 ğŸŸ¡ MEDIUM PRIORITY: RAGç¼–ç æ•ˆç‡

**é—®é¢˜**: æ¯æ¬¡forwardéƒ½å®Œæ•´ç¼–ç Kä¸ªå‚è€ƒåºåˆ—

**å½“å‰å®ç°** ([bert.py:102-108](src/model/bert.py#L102-L108)):
```python
def encode_rag_segments(self, rag_segs, pos, af):
    # ...
    emb = self.embedding(chunk_flat)  # é‡æ–°åµŒå…¥
    emb = self.emb_fusion(emb, pos_exp, af_exp)  # é‡æ–°èåˆ
    for t in self.transformer_blocks:  # å®Œæ•´BERTç¼–ç ï¼
        emb = t(emb)
```

**æˆæœ¬åˆ†æ**:
- è®­ç»ƒæ ·æœ¬: 1æ¡åºåˆ— + K=1æ¡å‚è€ƒ â†’ **2å€ç¼–ç æˆæœ¬**
- å¦‚æœK=3 â†’ **4å€ç¼–ç æˆæœ¬**
- **å‚è€ƒé¢æ¿æ˜¯å›ºå®šçš„ï¼Œå´æ¯æ¬¡éƒ½é‡æ–°ç¼–ç **

**æ–¹æ¡ˆA: é¢„è®¡ç®—å‚è€ƒåµŒå…¥** (å¤§å¹…æé€Ÿ)

**æ ¸å¿ƒæ€æƒ³**: å‚è€ƒé¢æ¿å›ºå®š â†’ ç¦»çº¿ç¼–ç  â†’ è®­ç»ƒæ—¶ç›´æ¥åŠ è½½

```python
# scripts/precompute_ref_embeddings.py (æ–°å»º)
import h5py
import torch
from src.model.bert import BERTWithRAG
from tqdm import tqdm

def precompute_reference_embeddings(
    model_path,          # é¢„è®­ç»ƒæ¨¡å‹æˆ–éšæœºåˆå§‹åŒ–
    refpanel_vcf,        # å‚è€ƒé¢æ¿VCF
    window_path,         # çª—å£å®šä¹‰
    output_h5,           # è¾“å‡ºåµŒå…¥æ–‡ä»¶
    device='cuda:0'
):
    """
    é¢„è®¡ç®—å‚è€ƒé¢æ¿æ‰€æœ‰çª—å£çš„BERTåµŒå…¥

    è¾“å‡ºæ ¼å¼:
    embeddings.h5
    â”œâ”€â”€ window_0 â†’ [n_refs, seq_len, dims]
    â”œâ”€â”€ window_1 â†’ [n_refs, seq_len, dims]
    â””â”€â”€ ...
    """
    model = BERTWithRAG.from_pretrained(model_path).to(device)
    model.eval()

    ref_data = load_reference_panel(refpanel_vcf)
    windows = pd.read_csv(window_path)

    with h5py.File(output_h5, 'w') as f_out:
        for win_idx, window in tqdm(windows.iterrows(), total=len(windows)):
            # è·å–çª—å£å†…å‚è€ƒåºåˆ—
            ref_seqs = extract_window_refs(ref_data, window)  # [n_refs, seq_len]

            with torch.no_grad():
                # ç¼–ç å‚è€ƒåºåˆ—
                emb = model.embedding(ref_seqs)
                emb = model.emb_fusion(emb, pos, af)
                for t in model.transformer_blocks:
                    emb = t(emb)

                # ä¿å­˜åµŒå…¥
                f_out.create_dataset(
                    f'window_{win_idx}',
                    data=emb.cpu().numpy(),
                    compression='gzip'
                )

    print(f"âœ“ é¢„è®¡ç®—å®Œæˆ: {output_h5}")
```

ä¿®æ”¹RAGæ¨¡å—ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥:

```python
# src/model/bert.py
class BERTWithRAG(BERT):
    def __init__(self, ..., precomputed_ref_emb_path=None):
        super().__init__(...)
        self.use_precomputed = (precomputed_ref_emb_path is not None)
        if self.use_precomputed:
            self.ref_embeddings = h5py.File(precomputed_ref_emb_path, 'r')

    def encode_rag_segments(self, rag_segs, pos, af, window_idx=None):
        if self.use_precomputed and window_idx is not None:
            # ç›´æ¥åŠ è½½é¢„è®¡ç®—åµŒå…¥
            emb = torch.from_numpy(
                self.ref_embeddings[f'window_{window_idx}'][:]
            ).to(self.device)
            return emb
        else:
            # åŸæœ‰çš„åœ¨çº¿ç¼–ç é€»è¾‘
            # ...
```

**é¢„æœŸæ•ˆæœ**:
- âœ… **è®­ç»ƒé€Ÿåº¦æå‡ 30-50%** (å–å†³äºK)
- âœ… **æ˜¾å­˜èŠ‚çœ 20-30%** (æ— éœ€å­˜å‚¨å‚è€ƒçš„æ¢¯åº¦)
- âœ… **æ•°å€¼å®Œå…¨ä¸€è‡´** (ç¡®å®šæ€§ç¼–ç )

**ç¼ºç‚¹**:
- âš ï¸ éœ€è¦é¢å¤–å­˜å‚¨ (çº¦2-5GB for chr21)
- âš ï¸ æ¨¡å‹æ›´æ–°åéœ€é‡æ–°é¢„è®¡ç®—
- âš ï¸ åˆå§‹å®ç°å¤æ‚åº¦

**æ–¹æ¡ˆB: å…±äº«ç¼–ç å™¨** (ä¸­ç­‰æé€Ÿ)

å¦‚æœä¸æƒ³é¢„è®¡ç®—ï¼Œå¯ä»¥å…±äº«ç¼–ç :

```python
def forward(self, x: dict):
    # å°†åŸå§‹åºåˆ—å’Œå‚è€ƒåºåˆ—åˆå¹¶ç¼–ç 
    B, L = x['hap_1'].size()
    K = x['rag_seg_h1'].size(1)

    # åˆå¹¶: [B, L] + [B*K, L] â†’ [B*(1+K), L]
    combined_h1 = torch.cat([
        x['hap_1'],
        x['rag_seg_h1'].view(-1, L)
    ], dim=0)

    # ä¸€æ¬¡ç¼–ç æ‰€æœ‰
    all_encoded = self.encode(combined_h1, x['pos'], x['af'])

    # æ‹†åˆ†
    h1 = all_encoded[:B]
    rag_h1 = all_encoded[B:].view(B, K, L, -1)
```

**é¢„æœŸæ•ˆæœ**:
- âœ… **è®­ç»ƒé€Ÿåº¦æå‡ 10-20%** (batchæ•ˆåº”)
- âš ï¸ ä½†ä»ç„¶é‡å¤ç¼–ç å‚è€ƒ

**å»ºè®®**:
- ğŸ¯ **å½“å‰K=1æ—¶**: æ–¹æ¡ˆBè¶³å¤Ÿ (å®¹æ˜“å®ç°)
- ğŸ”¬ **å¦‚æœæœªæ¥K>1**: å®ç°æ–¹æ¡ˆA (å€¼å¾—æŠ•å…¥)
- ğŸ“Š è§‚å¯Ÿè®­ç»ƒæ—¶GPUåˆ©ç”¨ç‡ï¼Œå¦‚æœ<80%åˆ™ä¼˜å…ˆçº§ä¸‹é™

**å®æ–½éš¾åº¦**: æ–¹æ¡ˆA â­â­â­â­, æ–¹æ¡ˆB â­â­

---

### 3.3 ğŸŸ¡ MEDIUM PRIORITY: Lossæƒé‡ä¸å¹³è¡¡

**å½“å‰æƒé‡** ([pretrain_with_val.py:184-185](src/main/pretrain_with_val.py#L184-L185)):
```python
total_loss = (0.2 * hap_1_loss + 0.2 * hap_2_loss + 0.3 * gt_loss +
              0.15 * recon_loss1 + 0.15 * recon_loss2)
```

**é—®é¢˜åˆ†æ**:

1. **Haplotype vs Genotypeæƒé‡æ¯”**: `0.4 : 0.3`
   - Haplotypeæ˜¯æ ¸å¿ƒä»»åŠ¡ (ç›¸ä½æ¨æ–­)
   - Genotypeæ˜¯è¾…åŠ©ç›‘ç£
   - **å½“å‰æƒé‡åˆç†** âœ…

2. **Reconstruction Lossçš„å¿…è¦æ€§**: `0.3 / 1.0 = 30%`
   - é‡æ„losså¼ºåˆ¶æ¨¡å‹å­¦ä¹ åŸå§‹åºåˆ—
   - ä½†**å¯èƒ½å¹²æ‰°ä¸»ä»»åŠ¡** âš ï¸

**é‡æ„Lossåˆ†æ**:

æŸ¥çœ‹å®šä¹‰ ([pretrain_with_val.py:180-181](src/main/pretrain_with_val.py#L180-L181)):
```python
recon_loss1 = self.recon_critetion(output[3][masks], output[5][masks])
recon_loss2 = self.recon_critetion(output[4][masks], output[6][masks])
# output[3/4]: é¢„æµ‹çš„haplotype logits
# output[5/6]: åŸå§‹è¾“å…¥çš„haplotype (ä½œä¸ºtarget)
```

**é—®é¢˜**:
- `output[5]` æ˜¯maskedè¾“å…¥ (éƒ¨åˆ†ä½ç‚¹æœªçŸ¥)
- è®©æ¨¡å‹é‡æ„maskedè¾“å…¥ â†’ **å¼ºåˆ¶è®°å¿†è¾“å…¥å™ªå£°**
- å¯èƒ½**é˜»ç¢æ³›åŒ–**

**å®éªŒå»ºè®®**:

æµ‹è¯•3ä¸ªé…ç½®:

```python
# Config A: å½“å‰é…ç½® (baseline)
total_loss = 0.2*hap1 + 0.2*hap2 + 0.3*gt + 0.15*recon1 + 0.15*recon2

# Config B: é™ä½é‡æ„æƒé‡
total_loss = 0.25*hap1 + 0.25*hap2 + 0.4*gt + 0.05*recon1 + 0.05*recon2

# Config C: ç§»é™¤é‡æ„loss
total_loss = 0.3*hap1 + 0.3*hap2 + 0.4*gt
```

**é¢„æœŸ**:
- Config B: å¹³è¡¡ä¸»ä»»åŠ¡å’Œè¾…åŠ©ä»»åŠ¡
- Config C: æœ€ä¸“æ³¨äºhaplotypeæ¨æ–­

**å»ºè®®**:
- ğŸ¯ **å…ˆè§‚å¯Ÿå½“å‰é‡æ„lossçš„å€¼**
- å¦‚æœ `recon_loss >> hap_loss` â†’ é™ä½æƒé‡
- å¦‚æœ `recon_loss << hap_loss` â†’ å¯èƒ½å·²é¥±å’Œï¼Œå¯ç§»é™¤
- ğŸ“Š é€šè¿‡validation F1å¯¹æ¯”3ä¸ªconfig

**å®æ–½éš¾åº¦**: â­ (ä¿®æ”¹1è¡Œä»£ç )

---

### 3.4 ğŸŸ¢ LOW PRIORITY: ç‹¬ç«‹ç¼–ç ä¸¤ä¸ªHaplotype

**å½“å‰å®ç°** ([bert.py:115-125](src/model/bert.py#L115-L125)):
```python
def forward(self, x: dict):
    h1, h2, h1_ori, h2_ori = super().forward(x)  # åˆ†åˆ«ç¼–ç 

    rag_h1 = self.encode_rag_segments(x['rag_seg_h1'], ...)
    rag_h2 = self.encode_rag_segments(x['rag_seg_h2'], ...)

    h1_fused = self.rag_fusion(h1, rag_h1, ...)
    h2_fused = self.rag_fusion(h2, rag_h2, ...)
```

**é—®é¢˜**:
- ä¸¤ä¸ªhaplotype **å®Œå…¨ç‹¬ç«‹ç¼–ç **
- æ²¡æœ‰åˆ©ç”¨ **haplotypeé—´çš„ç›¸å…³æ€§**

**é—ä¼ å­¦èƒŒæ™¯**:
- åŒä¸€ä¸ªä½“çš„ä¸¤ä¸ªhaplotype **é«˜åº¦ç›¸å…³** (æ¥è‡ªçˆ¶æ¯)
- å®ƒä»¬å…±äº«ç›¸åŒçš„ **ç¾¤ä½“é—ä¼ èƒŒæ™¯**
- **è¿é”ä¸å¹³è¡¡ (LD)** åœ¨ä¸¤ä¸ªhaplotypeé—´ä¿æŒä¸€è‡´

**ä¼˜åŒ–æ–¹æ¡ˆ: Cross-Haplotype Attention**

```python
class CrossHaplotypeAttention(nn.Module):
    """è·¨å•å€å‹æ³¨æ„åŠ›"""
    def __init__(self, dims, heads=4):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dims, heads, batch_first=True)
        self.norm = nn.LayerNorm(dims)

    def forward(self, h1, h2):
        # h1 attend to h2
        h1_enhanced, _ = self.cross_attn(h1, h2, h2)
        h1 = self.norm(h1 + h1_enhanced)

        # h2 attend to h1
        h2_enhanced, _ = self.cross_attn(h2, h1, h1)
        h2 = self.norm(h2 + h2_enhanced)

        return h1, h2

class BERTWithRAG(BERT):
    def __init__(self, ...):
        super().__init__(...)
        self.cross_hap_attn = CrossHaplotypeAttention(dims)

    def forward(self, x: dict):
        h1, h2, h1_ori, h2_ori = super().forward(x)

        # è·¨å•å€å‹äº¤äº’
        h1, h2 = self.cross_hap_attn(h1, h2)

        # ç„¶åå†RAGèåˆ
        rag_h1 = self.encode_rag_segments(...)
        h1_fused = self.rag_fusion(h1, rag_h1, ...)
        # ...
```

**é¢„æœŸæ•ˆæœ**:
- âœ… åˆ©ç”¨haplotypeç›¸å…³æ€§
- âœ… å¯¹äº**æ‚åˆä½ç‚¹** (0/1) ç‰¹åˆ«æœ‰å¸®åŠ©
- âš ï¸ å¢åŠ 10-15%è®¡ç®—é‡

**å»ºè®®**:
- ğŸ”¬ **éå¿…éœ€** (å½“å‰æ¨¡å‹å·²ç»ä¸é”™)
- ğŸ“Š å¦‚æœvalidation F1é‡åˆ°ç“¶é¢ˆæ—¶å°è¯•
- ğŸ¯ ä¼˜å…ˆçº§ä½äºgammaä¿®å¤

**å®æ–½éš¾åº¦**: â­â­

---

### 3.5 ğŸŸ¢ LOW PRIORITY: MAFåŠ æƒä¸Šé™

**å½“å‰å®ç°** ([fusion.py:136-138](src/model/fusion.py#L136-L138)):
```python
maf = torch.min(global_af, 1 - global_af).unsqueeze(-1)
maf_weight = (1.0 / (maf + 1e-6)).clamp(max=10.0)
```

**æƒé‡åˆ†æ**:

| MAF | 1/MAF | Clamped | å®é™…æƒé‡ |
|-----|-------|---------|---------|
| 0.5 (å¸¸è§) | 2.0 | 2.0 | âœ… |
| 0.1 | 10.0 | 10.0 | âœ… |
| 0.05 | 20.0 | **10.0** | âš ï¸ æˆªæ–­ |
| 0.01 (ç½•è§) | 100.0 | **10.0** | âš ï¸ ä¸¥é‡æˆªæ–­ |
| 0.001 (æç½•è§) | 1000.0 | **10.0** | âš ï¸ ä¸¥é‡æˆªæ–­ |

**é—®é¢˜**:
- **MAF < 0.1 çš„å˜å¼‚éƒ½è¢«è§†ä¸ºåŒç­‰é‡è¦**
- ä½†MAF=0.01å’ŒMAF=0.001çš„å·®å¼‚æ˜¯**10å€**
- **ä¸¢å¤±äº†ç½•è§å˜å¼‚å†…éƒ¨çš„å±‚æ¬¡ç»“æ„**

**æ–¹æ¡ˆA: Log-scaleåŠ æƒ**

```python
# å¯¹æ•°å°ºåº¦æƒé‡
maf_weight = torch.log(1.0 / (maf + 1e-6) + 1).clamp(max=5.0)
```

| MAF | log(1/MAF + 1) | ç‰¹ç‚¹ |
|-----|----------------|------|
| 0.5 | 1.69 | å¸¸è§å˜å¼‚ |
| 0.1 | 3.40 | ä¸­ç­‰ |
| 0.01 | 5.0 (clamped) | ç½•è§ |
| 0.001 | 5.0 | æç½•è§ (ä½†ä¸ä¼šçˆ†ç‚¸) |

**ä¼˜ç‚¹**:
- âœ… å¹³æ»‘è¿‡æ¸¡
- âœ… ä¿ç•™å±‚æ¬¡ç»“æ„
- âœ… æ•°å€¼ç¨³å®š

**æ–¹æ¡ˆB: åˆ†æ®µåŠ æƒ**

```python
# æ ¹æ®MAFèŒƒå›´ä½¿ç”¨ä¸åŒæƒé‡
def adaptive_maf_weight(maf):
    weight = torch.ones_like(maf)
    weight[maf > 0.05] = 1.0           # å¸¸è§: 1x
    weight[(maf <= 0.05) & (maf > 0.01)] = 3.0   # ä½é¢‘: 3x
    weight[maf <= 0.01] = 10.0         # ç½•è§: 10x
    return weight
```

**ä¼˜ç‚¹**:
- âœ… ç¬¦åˆé—ä¼ å­¦åˆ†ç±» (å¸¸è§/ä½é¢‘/ç½•è§)
- âœ… å¯è§£é‡Šæ€§å¼º
- âš ï¸ ç¡¬è¾¹ç•Œå¯èƒ½å¯¼è‡´ä¸è¿ç»­

**å»ºè®®**:
- ğŸ¯ **å½“å‰clamp(max=10)åˆç†** (ä¿å®ˆç­–ç•¥)
- ğŸ”¬ å¦‚æœå‘ç°**æç½•è§å˜å¼‚F1ä½**ï¼Œå°è¯•æ–¹æ¡ˆA
- ğŸ“Š å¯ä½œä¸ºåæœŸfine-tuningç­–ç•¥

**å®æ–½éš¾åº¦**: â­

---

## 4. åˆ†é¡¹æ·±åº¦åˆ†æ

### 4.1 Losså‡½æ•°è¯„ä»·

**ç»„æˆ**:
```python
total_loss = 0.2*hap1 + 0.2*hap2 + 0.3*gt + 0.15*recon1 + 0.15*recon2
```

**å„éƒ¨åˆ†åˆ†æ**:

#### Focal Loss (Haplotype + Genotype)

**ä¼˜ç‚¹**: âœ…
- å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ (0/1æ¯”ä¾‹åæ–œ)
- å…³æ³¨å›°éš¾æ ·æœ¬

**é—®é¢˜**: âš ï¸
- **gamma=5è¿‡é«˜** (è§3.1èŠ‚)

**æ¨è**: é™ä½åˆ°gamma=2.5

---

#### Reconstruction Loss

**å½“å‰é€»è¾‘** ([pretrain_with_val.py:180-187](src/main/pretrain_with_val.py#L180-L187)):
```python
recon_loss1 = MSE(predicted_hap1, original_input_hap1)

if recon_loss1 > MIN_RECON_LOSS:  # MIN_RECON_LOSS = 0.01
    # ä½¿ç”¨é‡æ„loss
    total_loss = 0.2*hap1 + 0.2*hap2 + 0.3*gt + 0.15*recon1 + 0.15*recon2
else:
    # é‡æ„lossè¿‡å°ï¼Œå¿½ç•¥
    total_loss = 3*hap1 + 3*hap2 + 4*gt
```

**é—®é¢˜è¯†åˆ«**:

1. **åŠ¨æ€åˆ‡æ¢æƒé‡æ–¹æ¡ˆ**
   - æ—©æœŸ: 0.2/0.2/0.3/0.15/0.15 (æ€»å’Œ=1.0)
   - åæœŸ: 3/3/4 (æ€»å’Œ=10)
   - **Losså°ºåº¦çªç„¶å˜åŒ–10å€ï¼**

2. **MIN_RECON_LOSSé˜ˆå€¼**
   - 0.01æ˜¯å¦åˆç†ï¼Ÿ
   - MSE lossé€šå¸¸å¾ˆå°ï¼Œå¯èƒ½ä¸€ç›´è§¦å‘ç¬¬ä¸€ä¸ªåˆ†æ”¯

3. **é‡æ„ç›®æ ‡é—®é¢˜**
   - é‡æ„maskedè¾“å…¥ â†’ å­¦ä¹ å™ªå£°
   - åº”è¯¥é‡æ„**çœŸå®åºåˆ—**ï¼Œè€Œä¸æ˜¯maskç‰ˆæœ¬

**æ”¹è¿›å»ºè®®**:

```python
# æ–¹æ¡ˆ1: å›ºå®šæƒé‡ï¼Œç§»é™¤åŠ¨æ€åˆ‡æ¢
total_loss = 0.3*hap1 + 0.3*hap2 + 0.4*gt  # æ— é‡æ„

# æ–¹æ¡ˆ2: å¦‚æœä¿ç•™é‡æ„ï¼Œé™ä½æƒé‡å¹¶ä¿®æ­£ç›®æ ‡
recon_loss1 = MSE(predicted_hap1, true_hap1)  # ä½¿ç”¨labelè€Œéinput
total_loss = 0.25*hap1 + 0.25*hap2 + 0.4*gt + 0.05*recon1 + 0.05*recon2

# æ–¹æ¡ˆ3: å¯¹æ¯”å­¦ä¹ æ›¿ä»£é‡æ„
contrastive_loss = InfoNCE(h1_fused, positive_samples, negative_samples)
total_loss = 0.3*hap1 + 0.3*hap2 + 0.3*gt + 0.1*contrastive_loss
```

**è¯„åˆ†**: â­â­â­ (æœ‰æ”¹è¿›ç©ºé—´)

---

### 4.2 RAGé›†æˆè¯„ä»·

#### æ£€ç´¢ç­–ç•¥

**å½“å‰**: FAISS IVF (L2è·ç¦», K=1)

**ä¼˜ç‚¹**: âœ…
- é«˜æ•ˆ (ç™¾ä¸‡çº§ç´¢å¼•ï¼Œæ¯«ç§’çº§æŸ¥è¯¢)
- K=1é™ä½æ˜¾å­˜å‹åŠ›

**æ”¹è¿›æ–¹å‘**: ğŸ”¬

1. **Cosineè·ç¦» vs L2è·ç¦»**
   ```python
   # å½“å‰: L2è·ç¦»
   index = faiss.IndexIVFFlat(quantizer, dims, nlist, faiss.METRIC_L2)

   # æ›¿ä»£: Cosineç›¸ä¼¼åº¦ (åŸºå› åºåˆ—å¯èƒ½æ›´é€‚åˆ)
   index = faiss.IndexIVFFlat(quantizer, dims, nlist, faiss.METRIC_INNER_PRODUCT)
   # æ³¨æ„: éœ€è¦å…ˆå½’ä¸€åŒ–å‘é‡
   ```

2. **å¤šæ ·æ€§æ£€ç´¢**
   - å½“å‰K=1å¯èƒ½è¿‡äºä¿å®ˆ
   - å°è¯•K=3, ä½†ç”¨**å¤šæ ·æ€§é‡‡æ ·** (è€Œétop-3)

   ```python
   # æ£€ç´¢top-10
   D, I = index.search(query, k=10)

   # ä»top-10ä¸­é‡‡æ ·3ä¸ª (é™ä½ç›¸å…³æ€§)
   selected_idx = diversity_sampling(I, k=3, method='maximal_marginal_relevance')
   ```

**è¯„åˆ†**: â­â­â­â­

---

#### ç¼–ç æ•ˆç‡

**é—®é¢˜**: æ¯æ¬¡forwardé‡æ–°ç¼–ç å‚è€ƒ (è§3.2èŠ‚)

**æ–¹æ¡ˆ**: é¢„è®¡ç®—åµŒå…¥ (30-50%æé€Ÿ)

**è¯„åˆ†**: â­â­â­ (æœ‰ä¼˜åŒ–ç©ºé—´)

---

### 4.3 Fusionæœºåˆ¶è¯„ä»·

**EnhancedRareVariantFusion** æµç¨‹:
```
Input: orig_feat [B,L,D], rag_feat [B,K,L,D], global_af, pop_af

1. CrossAFInteraction
   fused_af = MLP(concat(global_af, pop_af))  # [B,L,D]

2. AF Adapter
   af_weight = Sigmoid(MLP(fused_af))  # [B,L,D]

3. Reference Weighting
   weighted_ref = rag_feat * af_weight.unsqueeze(1)  # [B,K,L,D]

4. Dynamic Pooling
   attention = Softmax(Linear(weighted_ref))  # [B,L,K,1]
   pooled = sum(weighted_ref * attention, dim=K)  # [B,L,D]

5. Feature Fusion
   fused = MLP(concat(orig_feat, pooled))  # [B,L,D]

6. MAF Weighting + Residual
   maf_weight = (1/MAF).clamp(max=10)
   output = orig_feat + 0.1 * (fused * maf_weight)
```

**ä¼˜ç‚¹**: âœ…âœ…âœ…

1. **å¤šå±‚æ¬¡AFä¿¡æ¯èåˆ** (global + population)
2. **å­¦ä¹ å‹æ³¨æ„åŠ›** (ä¸æ˜¯ç®€å•å¹³å‡)
3. **MAFè‡ªé€‚åº”** (å¼ºè°ƒç½•è§å˜å¼‚)
4. **æ®‹å·®è¿æ¥** (ç¨³å®šè®­ç»ƒ)

**æ½œåœ¨æ”¹è¿›**: ğŸ”¬

1. **LD-aware Attention**
   - å½“å‰fusionå¯¹æ‰€æœ‰ä½ç‚¹ä¸€è§†åŒä»
   - å¯ä»¥åŠ å…¥LDä¿¡æ¯ (å·²å®ç°ä½†æœªä½¿ç”¨)

   ```python
   # fusion.py å·²æœ‰LDGuidedRetention (æœªä½¿ç”¨)
   class LDGuidedRetention(nn.Module):
       # LDè¡°å‡æ³¨æ„åŠ›
   ```

   **å»ºè®®**: æ›¿æ¢Dynamic Poolingä¸ºLD-Guided Retention

   ```python
   # ä¿®æ”¹ EnhancedRareVariantFusion
   self.pooling = LDGuidedRetention(dims, ld_decay_rate=0.1)
   ```

2. **Pop-specific Fusion**
   - ä¸åŒç¾¤ä½“(EUR/AFR/EAS)çš„LDæ¨¡å¼ä¸åŒ
   - å¯ä»¥å­¦ä¹ ç¾¤ä½“ç‰¹å®šçš„fusionæƒé‡

   ```python
   class PopulationSpecificFusion(nn.Module):
       def __init__(self, dims, n_pops=5):
           self.pop_experts = nn.ModuleList([
               EnhancedRareVariantFusion(dims) for _ in range(n_pops)
           ])
           self.pop_gate = nn.Linear(dims, n_pops)

       def forward(self, orig, rag, af, pop_af, pop_id):
           # Mixture of Experts
           expert_outputs = [expert(orig, rag, af, pop_af)
                            for expert in self.pop_experts]
           gate_weights = F.softmax(self.pop_gate(orig), dim=-1)
           output = sum(w * out for w, out in zip(gate_weights, expert_outputs))
           return output
   ```

**è¯„åˆ†**: â­â­â­â­â­ (å·²ç»å¾ˆä¼˜ç§€)

---

### 4.4 è®­ç»ƒæ–¹æ³•è¯„ä»·

#### ä¼˜åŒ–å™¨é…ç½®

```python
optimizer = Adam(
    lr=1e-5,
    weight_decay=0.01,
    fused=True  # CUDAèåˆä¼˜åŒ–
)
```

**è¯„ä»·**: âœ…
- `fused=True` æé€Ÿ10-15%
- weight_decayåˆç† (æ­£åˆ™åŒ–)

**æ”¹è¿›æ–¹å‘**: ğŸ”¬

1. **AdamW** (æ›´å¥½çš„weight decay)
   ```python
   optimizer = torch.optim.AdamW(lr=1e-5, weight_decay=0.01)
   ```

2. **Layer-wise Learning Rate Decay** (LLRD)
   - Transformerä¸‹å±‚å­¦ä¹ ç‡ä½ï¼Œä¸Šå±‚é«˜
   ```python
   no_decay = ['bias', 'LayerNorm.weight']
   layer_params = []
   for i, layer in enumerate(model.transformer_blocks):
       lr = base_lr * (decay_rate ** (n_layers - i))
       layer_params.append({
           'params': [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],
           'lr': lr,
           'weight_decay': 0.01
       })
   ```

**è¯„åˆ†**: â­â­â­â­

---

#### å­¦ä¹ ç‡è°ƒåº¦

```python
# Linear warmup (20k steps) + inverse sqrt decay
lr = max_lr * sqrt(warmup_steps) / sqrt(current_step)
```

**è¯„ä»·**: âœ…
- æ ‡å‡†BERTè®­ç»ƒç­–ç•¥
- Warmupç¨³å®šåˆæœŸè®­ç»ƒ

**æ”¹è¿›æ–¹å‘**: ğŸ”¬

1. **Cosine Annealing** (åæœŸæ›´å¥½)
   ```python
   from torch.optim.lr_scheduler import CosineAnnealingLR
   scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)
   ```

2. **OneCycle** (å¿«é€Ÿæ”¶æ•›)
   ```python
   from torch.optim.lr_scheduler import OneCycleLR
   scheduler = OneCycleLR(optimizer, max_lr=1.5e-4, total_steps=total_steps)
   ```

**è¯„åˆ†**: â­â­â­â­

---

#### æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast(enabled=True, dtype=torch.float16):
    output = model(data)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**è¯„ä»·**: âœ…âœ…
- æé€Ÿ30-50%
- æ˜¾å­˜èŠ‚çœ30-40%
- æ•°å€¼ç¨³å®š (GradScalerå¤„ç†æº¢å‡º)

**è¯„åˆ†**: â­â­â­â­â­

---

#### æ¢¯åº¦ç´¯ç§¯

```python
# å½“å‰æ”¯æŒ (run_v12: grad_accum_steps=1)
total_loss /= grad_accum_steps
loss.backward()

if step % grad_accum_steps == 0:
    optimizer.step()
    optimizer.zero_grad()
```

**è¯„ä»·**: âœ…
- çµæ´»è°ƒæ•´effective batch size
- æ˜¾å­˜å—é™æ—¶å¾ˆæœ‰ç”¨

**å»ºè®®**: å¦‚æœæ˜¾å­˜å……è¶³ï¼Œ`grad_accum_steps=1`æœ€ä¼˜ (å½“å‰é…ç½®)

**è¯„åˆ†**: â­â­â­â­â­

---

### 4.5 Rare vs Commonå¤„ç†æ™ºèƒ½æ€§

**å½“å‰ç­–ç•¥**:

1. **MAFé€†å‘åŠ æƒ** ([fusion.py:136-140](src/model/fusion.py#L136-L140))
   ```python
   maf_weight = (1/MAF).clamp(max=10)
   # MAF=0.5 â†’ weight=2
   # MAF=0.1 â†’ weight=10
   # MAF=0.01 â†’ weight=10 (clamped)
   ```

2. **Focal Loss** (å…³æ³¨å›°éš¾æ ·æœ¬)
   - ç½•è§å˜å¼‚é€šå¸¸æ˜¯å›°éš¾æ ·æœ¬ â†’ è‡ªåŠ¨è·å¾—æ›´é«˜æƒé‡

**è¯„ä»·**: â­â­â­â­ (å·²ç»ç›¸å½“æ™ºèƒ½)

**è¿›ä¸€æ­¥ä¼˜åŒ–**: ğŸ”¬

#### æ–¹æ¡ˆA: åŒåˆ†æ”¯æ¶æ„

```python
class RareCommonDualBranch(nn.Module):
    """ç½•è§/å¸¸è§å˜å¼‚åˆ†æ”¯å¤„ç†"""
    def __init__(self, dims):
        super().__init__()
        self.rare_branch = EnhancedRareVariantFusion(dims)  # å½“å‰fusion
        self.common_branch = SimpleFusion(dims)  # ç®€åŒ–fusion (å¸¸è§å˜å¼‚ä¸éœ€è¦å¤æ‚èåˆ)

        self.maf_threshold = 0.05  # ç½•è§å˜å¼‚é˜ˆå€¼

    def forward(self, orig, rag, af, pop_af):
        maf = torch.min(af, 1 - af)
        is_rare = (maf < self.maf_threshold).float().unsqueeze(-1)

        # åˆ†åˆ«å¤„ç†
        rare_out = self.rare_branch(orig, rag, af, pop_af)
        common_out = self.common_branch(orig, rag, af, pop_af)

        # è½¯èåˆ (é¿å…ç¡¬åˆ‡æ¢)
        output = is_rare * rare_out + (1 - is_rare) * common_out
        return output

class SimpleFusion(nn.Module):
    """å¸¸è§å˜å¼‚ç®€åŒ–èåˆ"""
    def __init__(self, dims):
        super().__init__()
        self.fusion = nn.Sequential(
            nn.Linear(2*dims, dims),
            nn.LayerNorm(dims)
        )

    def forward(self, orig, rag, af, pop_af):
        # ç®€å•å¹³å‡æ± åŒ–
        pooled_rag = rag.mean(dim=1)
        # ç›´æ¥æ‹¼æ¥
        return self.fusion(torch.cat([orig, pooled_rag], dim=-1))
```

**ä¼˜ç‚¹**:
- âœ… å¸¸è§å˜å¼‚ç”¨ç®€å•ç­–ç•¥ (å¿«é€Ÿç¨³å®š)
- âœ… ç½•è§å˜å¼‚ç”¨å¤æ‚ç­–ç•¥ (ç²¾ç»†èåˆ)
- âœ… è®¡ç®—é‡è‡ªé€‚åº”

---

#### æ–¹æ¡ˆB: Curriculum Learning (è¯¾ç¨‹å­¦ä¹ )

```python
class CurriculumScheduler:
    """ä»å¸¸è§åˆ°ç½•è§çš„è¯¾ç¨‹å­¦ä¹ """
    def __init__(self, total_epochs, maf_schedule):
        self.schedule = maf_schedule  # {epoch: max_maf}
        # Example: {0: 0.5, 5: 0.1, 10: 0.05, 15: 0.01}

    def get_maf_threshold(self, epoch):
        # è¿”å›å½“å‰epochåº”è¯¥è®­ç»ƒçš„æœ€ä½MAF
        for e, maf in sorted(self.schedule.items(), reverse=True):
            if epoch >= e:
                return maf
        return 0.5  # é»˜è®¤å¸¸è§å˜å¼‚

# åœ¨DataLoaderä¸­è¿‡æ»¤
def curriculum_sampler(dataset, epoch, scheduler):
    maf_threshold = scheduler.get_maf_threshold(epoch)
    # åªé€‰æ‹© MAF >= threshold çš„æ ·æœ¬
    valid_samples = [s for s in dataset if s.maf >= maf_threshold]
    return valid_samples

# è®­ç»ƒæµç¨‹
scheduler = CurriculumScheduler(epochs=20, maf_schedule={
    0: 0.5,   # Epoch 0-4: åªè®­ç»ƒå¸¸è§å˜å¼‚ (MAF>=0.5)
    5: 0.1,   # Epoch 5-9: åŠ å…¥ä½é¢‘å˜å¼‚
    10: 0.05, # Epoch 10-14: åŠ å…¥ç½•è§å˜å¼‚
    15: 0.0   # Epoch 15+: æ‰€æœ‰å˜å¼‚
})

for epoch in range(epochs):
    train_loader = create_loader(dataset, scheduler, epoch)
    trainer.train(epoch)
```

**ä¼˜ç‚¹**:
- âœ… å…ˆå­¦ç®€å• (å¸¸è§å˜å¼‚)ï¼Œå†å­¦å›°éš¾ (ç½•è§å˜å¼‚)
- âœ… è®­ç»ƒæ›´ç¨³å®š
- âœ… æœ€ç»ˆæ€§èƒ½å¯èƒ½æ›´å¥½

**æ–‡çŒ®æ”¯æŒ**:
- Curriculum Learning (Bengio et al., 2009)
- åœ¨Imputationä¸­å·²æœ‰åº”ç”¨ (é€æ­¥å¢åŠ maskæ¯”ä¾‹)

---

#### æ–¹æ¡ˆC: Adaptive Sample Weighting

**å½“å‰**: æ‰€æœ‰æ ·æœ¬åŒç­‰å¯¹å¾…

**æ”¹è¿›**: æ ¹æ®MAFåŠ¨æ€è°ƒæ•´æ ·æœ¬æƒé‡

```python
# åœ¨DataLoaderçš„samplerä¸­
class MAFWeightedSampler(Sampler):
    def __init__(self, dataset, alpha=0.5):
        self.dataset = dataset
        self.alpha = alpha  # æ§åˆ¶ç½•è§å˜å¼‚çš„è¿‡é‡‡æ ·å¼ºåº¦

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æƒé‡
        self.weights = []
        for sample in dataset:
            maf = sample.maf
            # weight = (1/MAF)^alpha
            weight = (1.0 / (maf + 1e-6)) ** alpha
            self.weights.append(weight)

        # å½’ä¸€åŒ–
        self.weights = np.array(self.weights)
        self.weights /= self.weights.sum()

    def __iter__(self):
        # æ ¹æ®æƒé‡é‡‡æ ·
        indices = np.random.choice(
            len(self.dataset),
            size=len(self.dataset),
            replace=True,
            p=self.weights
        )
        return iter(indices)
```

**æ•ˆæœ**:
- MAF=0.01çš„æ ·æœ¬è¢«é‡‡æ ·çš„æ¦‚ç‡æ˜¯MAF=0.5çš„ **5^0.5 â‰ˆ 2.2å€** (alpha=0.5)
- å¹³è¡¡rare/commonçš„è®­ç»ƒé¢‘ç‡

---

**ç»¼åˆå»ºè®®**:

| æ–¹æ¡ˆ | ä¼˜å…ˆçº§ | å¤æ‚åº¦ | é¢„æœŸæå‡ |
|-----|--------|--------|---------|
| åŒåˆ†æ”¯ (A) | ğŸŸ¡ Medium | â­â­â­ | 5-10% rare F1 |
| è¯¾ç¨‹å­¦ä¹  (B) | ğŸŸ¢ Low | â­â­ | ç¨³å®šæ€§+3-5% overall |
| åŠ æƒé‡‡æ · (C) | ğŸŸ¢ Low | â­ | 2-5% rare F1 |

**æ¨èæ‰§è¡Œé¡ºåº**:
1. å…ˆä¿®å¤gamma=5 (æœ€é«˜ä¼˜å…ˆçº§)
2. è§‚å¯Ÿç½•è§å˜å¼‚F1
3. å¦‚æœrare F1ä»ç„¶å¾ˆä½ï¼Œå°è¯•æ–¹æ¡ˆC (æœ€ç®€å•)
4. å¦‚æœéœ€è¦è¿›ä¸€æ­¥æå‡ï¼Œè€ƒè™‘æ–¹æ¡ˆA

---

## 5. ä¼˜å…ˆçº§æ¨è

### ğŸ”´ HIGH PRIORITY (ç«‹å³ä¿®å¤)

#### 1. é™ä½Focal Loss Gamma
- **å½“å‰**: gamma=5
- **ä¿®æ”¹**: gamma=2.5
- **æ–‡ä»¶**: [src/main/pretrain_with_val.py:87-88](src/main/pretrain_with_val.py#L87-L88)
- **ä»£ç **:
```python
self.hap_criterion = FocalLoss(gamma=2.5, reduction='sum')
self.gt_criterion = FocalLoss(gamma=2.5, reduction='sum')
```
- **é¢„æœŸæ•ˆæœ**: è®­ç»ƒç¨³å®šæ€§â†‘, æ”¶æ•›é€Ÿåº¦â†‘2-3x, Val F1â†‘5-10%
- **é£é™©**: æ—  (çº¯æ”¶ç›Š)
- **å®æ–½æ—¶é—´**: 2åˆ†é’Ÿ

---

### ğŸŸ¡ MEDIUM PRIORITY (è§‚å¯Ÿåå†³å®š)

#### 2. è¯„ä¼°Reconstruction Loss
- **å½“å‰**: 30%æƒé‡ (0.15+0.15)
- **å®éªŒ**: å¯¹æ¯”3ä¸ªé…ç½® (è§3.3èŠ‚)
- **æ–¹æ³•**:
  1. è®°å½•å½“å‰`recon_loss`çš„æ•°å€¼
  2. å¦‚æœ `recon_loss >> hap_loss` â†’ é™ä½æƒé‡åˆ°10%
  3. å¦‚æœ `recon_loss << hap_loss` â†’ å°è¯•ç§»é™¤
- **é¢„æœŸæ•ˆæœ**: å¯èƒ½æå‡3-7% Val F1
- **å®æ–½æ—¶é—´**: 10åˆ†é’Ÿ (å¤šæ¬¡è®­ç»ƒå¯¹æ¯”)

#### 3. é¢„è®¡ç®—å‚è€ƒåµŒå…¥ (å¦‚æœK>1)
- **å½“å‰**: K=1, æ¯æ¬¡forwardé‡æ–°ç¼–ç 
- **ä¿®æ”¹**: ç¦»çº¿é¢„è®¡ç®— (è§3.2èŠ‚æ–¹æ¡ˆA)
- **æ¡ä»¶**: å¦‚æœæœªæ¥å¢å¤§Kå€¼ (K=3)
- **é¢„æœŸæ•ˆæœ**: è®­ç»ƒé€Ÿåº¦â†‘30-50%
- **é£é™©**: å®ç°å¤æ‚åº¦é«˜
- **å®æ–½æ—¶é—´**: åŠå¤©

---

### ğŸŸ¢ LOW PRIORITY (æ€§èƒ½ç“¶é¢ˆæ—¶è€ƒè™‘)

#### 4. Cross-Haplotype Attention
- **å½“å‰**: ä¸¤ä¸ªhaplotypeç‹¬ç«‹ç¼–ç 
- **ä¿®æ”¹**: åŠ å…¥è·¨å•å€å‹æ³¨æ„åŠ› (è§3.4èŠ‚)
- **é¢„æœŸæ•ˆæœ**: æ‚åˆä½ç‚¹F1â†‘3-5%
- **å®æ–½æ—¶é—´**: 2å°æ—¶

#### 5. LD-Guided Fusion
- **å½“å‰**: æœªä½¿ç”¨LDä¿¡æ¯
- **ä¿®æ”¹**: æ›¿æ¢Dynamic Poolingä¸ºLDGuidedRetention
- **é¢„æœŸæ•ˆæœ**: åˆ©ç”¨è¿é”ä¸å¹³è¡¡ï¼Œå¯èƒ½æå‡2-5%
- **å®æ–½æ—¶é—´**: 1å°æ—¶

#### 6. MAFåŠ æƒä¼˜åŒ–
- **å½“å‰**: clamp(max=10)
- **ä¿®æ”¹**: Log-scaleæˆ–åˆ†æ®µåŠ æƒ (è§3.5èŠ‚)
- **æ¡ä»¶**: æç½•è§å˜å¼‚F1å¾ˆä½æ—¶
- **å®æ–½æ—¶é—´**: 30åˆ†é’Ÿ

---

## 6. å®æ–½è·¯çº¿å›¾

### Phase 1: ç«‹å³ä¿®å¤ (ä»Šå¤©)

```bash
# 1. ä¿®æ”¹gamma
cd /cpfs01/.../00_RAG-SNVBERT-packup
```

ä¿®æ”¹ `src/main/pretrain_with_val.py`:
```python
# Line 87-88
self.hap_criterion = FocalLoss(gamma=2.5, reduction='sum').to(self.device)
self.gt_criterion = FocalLoss(gamma=2.5, reduction='sum').to(self.device)
```

```bash
# 2. é‡æ–°è®­ç»ƒ (å¦‚æœå½“å‰è®­ç»ƒè¿˜æ²¡è·‘å¤ªä¹…)
# æˆ–è€…ç»§ç»­è®­ç»ƒè§‚å¯Ÿå¯¹æ¯”

# 3. è§‚å¯Ÿvalidationæ—¥å¿—
tail -f logs/training.log
# å…³æ³¨: Val F1, Lossæ›²çº¿ç¨³å®šæ€§
```

**é¢„æœŸç»“æœ**:
- Lossæ›²çº¿æ›´å¹³æ»‘
- Validation F1åœ¨å‰5ä¸ªepochå¿«é€Ÿä¸Šå‡
- æ”¶æ•›é€Ÿåº¦æ˜æ˜¾æå‡

---

### Phase 2: å®éªŒå¯¹æ¯” (æœ¬å‘¨)

**Lossæƒé‡å®éªŒ**:

åˆ›å»º3ä¸ªé…ç½®:

```bash
# Config A: å½“å‰é…ç½® (baseline)
run_v12_split_val_baseline.sh  # recon=0.15+0.15

# Config B: é™ä½é‡æ„
run_v12_split_val_low_recon.sh  # recon=0.05+0.05

# Config C: æ— é‡æ„
run_v12_split_val_no_recon.sh  # recon=0
```

ä¿®æ”¹å¯¹åº”çš„è®­ç»ƒæ–‡ä»¶:
```python
# Config B (pretrain_with_val.py)
total_loss = (0.25*hap_1_loss + 0.25*hap_2_loss + 0.4*gt_loss +
              0.05*recon_loss1 + 0.05*recon_loss2)

# Config C
total_loss = 0.3*hap_1_loss + 0.3*hap_2_loss + 0.4*gt_loss
```

**è¿è¡Œ**:
```bash
# åŒæ—¶è¿è¡Œ3ä¸ªå®éªŒ (å¦‚æœæœ‰3å¼ GPU)
CUDA_VISIBLE_DEVICES=0 bash run_v12_split_val_baseline.sh &
CUDA_VISIBLE_DEVICES=1 bash run_v12_split_val_low_recon.sh &
CUDA_VISIBLE_DEVICES=2 bash run_v12_split_val_no_recon.sh &
```

**å¯¹æ¯”æŒ‡æ ‡**:
- Validation F1 (ä¸»è¦)
- Rare variant F1 (MAF<0.05)
- Common variant F1 (MAF>0.05)
- æ”¶æ•›é€Ÿåº¦ (è¾¾åˆ°æœ€ä½³F1çš„epochæ•°)

---

### Phase 3: æ¶æ„ä¼˜åŒ– (å¦‚æœé‡åˆ°ç“¶é¢ˆ)

**è§¦å‘æ¡ä»¶**:
- Validation F1åœæ» (è¿ç»­10 epochsæ— æå‡)
- Rare variant F1 < 0.6

**å°è¯•é¡ºåº**:

1. **MAF Weighted Sampling** (æœ€ç®€å•)
   ```python
   # dataset.py
   from torch.utils.data import WeightedRandomSampler

   sampler = MAFWeightedSampler(dataset, alpha=0.5)
   train_loader = DataLoader(dataset, batch_size=64, sampler=sampler)
   ```

2. **Cross-Haplotype Attention** (ä¸­ç­‰å¤æ‚åº¦)
   - å®ç° `CrossHaplotypeAttention` æ¨¡å—
   - åŠ å…¥åˆ° `BERTWithRAG.forward()` ä¸­

3. **LD-Guided Fusion** (å·²æœ‰ä»£ç ï¼Œåªéœ€å¯ç”¨)
   ```python
   # fusion.py
   # æ›¿æ¢ self.pooling
   self.pooling = LDGuidedRetention(dims, ld_decay_rate=0.1)
   ```

---

### Phase 4: æ•ˆç‡ä¼˜åŒ– (å¦‚æœéœ€è¦åŠ é€Ÿ)

**è§¦å‘æ¡ä»¶**:
- è®­ç»ƒæ—¶é—´è¿‡é•¿ (>2å¤©/20 epochs)
- éœ€è¦è®­ç»ƒæ›´å¤§æ¨¡å‹

**ä¼˜åŒ–æªæ–½**:

1. **é¢„è®¡ç®—å‚è€ƒåµŒå…¥**
   ```bash
   # 1. é¢„è®¡ç®—
   python scripts/precompute_ref_embeddings.py \
       --model_path output/best.pth \
       --refpanel_vcf maf_data/KGP.chr21.Panel.maf01.vcf.gz \
       --window_path maf_data/segments_chr21.maf.csv \
       --output_h5 data/ref_embeddings.h5

   # 2. è®­ç»ƒæ—¶ä½¿ç”¨
   python -m src.train_with_val \
       --precomputed_ref_emb data/ref_embeddings.h5 \
       ...
   ```

2. **å…±äº«ç¼–ç å™¨** (æ–¹æ¡ˆB)
   - ä¿®æ”¹ `BERTWithRAG.forward()` åˆå¹¶ç¼–ç 

---

## 7. æ€»ç»“è¯„åˆ†

| ç»„ä»¶ | å½“å‰å¾—åˆ† | ä¼˜åŒ–ç©ºé—´ | ä¼˜å…ˆçº§ |
|-----|---------|---------|--------|
| **Losså‡½æ•°** | â­â­â­ | ğŸ”´ High | gammaé™åˆ°2.5 |
| **RAGæ£€ç´¢** | â­â­â­â­ | ğŸŸ¢ Low | Cosineè·ç¦» |
| **RAGç¼–ç ** | â­â­â­ | ğŸŸ¡ Medium | é¢„è®¡ç®—åµŒå…¥ (K>1æ—¶) |
| **Fusionæœºåˆ¶** | â­â­â­â­â­ | ğŸŸ¢ Low | LD-guidedå¯é€‰ |
| **ä¼˜åŒ–å™¨** | â­â­â­â­ | ğŸŸ¢ Low | AdamWå¯é€‰ |
| **æ··åˆç²¾åº¦** | â­â­â­â­â­ | æ—  | å·²æœ€ä¼˜ |
| **Rareå¤„ç†** | â­â­â­â­ | ğŸŸ¡ Medium | åŠ æƒé‡‡æ · |
| **éªŒè¯æ¡†æ¶** | â­â­â­â­â­ | æ—  | å·²å®Œå–„ |

**æ€»ä½“è¯„ä»·**: â­â­â­â­ (85/100åˆ†)

**æ ¸å¿ƒä¼˜åŠ¿**:
- âœ… Fusionè®¾è®¡ç²¾å¦™
- âœ… å†…å­˜ä¼˜åŒ–åˆ°ä½
- âœ… éªŒè¯æ¡†æ¶å®Œæ•´

**æœ€å¤§é—®é¢˜**:
- âŒ Focal Loss gamma=5è¿‡é«˜ (ç«‹å³ä¿®å¤)
- âš ï¸ RAGç¼–ç é‡å¤è®¡ç®— (K>1æ—¶ä¼˜åŒ–)
- âš ï¸ é‡æ„losså¯èƒ½å¹²æ‰°ä¸»ä»»åŠ¡ (éœ€å®éªŒ)

---

## 8. Quick Start ä¿®å¤

**30ç§’å¿«é€Ÿä¿®å¤**:

```bash
cd /cpfs01/projects-HDD/humPOG_HDD/wbn_24110700074/RAG_Version/VCF-Bert/00_RAG-SNVBERT-packup

# å¤‡ä»½
cp src/main/pretrain_with_val.py src/main/pretrain_with_val.py.backup

# ä¿®æ”¹
sed -i 's/gamma=5/gamma=2.5/g' src/main/pretrain_with_val.py

# éªŒè¯
grep "gamma=" src/main/pretrain_with_val.py
# åº”è¯¥çœ‹åˆ°: FocalLoss(gamma=2.5, reduction='sum')

# å¦‚æœå½“å‰è®­ç»ƒè¿˜åœ¨æ—©æœŸ (<5 epochs), å»ºè®®é‡å¯è®­ç»ƒ
# å¦åˆ™ç»§ç»­è®­ç»ƒè§‚å¯Ÿæ”¹å–„

# é‡æ–°å¯åŠ¨
bash run_v12_split_val.sh
```

---

## 9. ç›‘æ§æŒ‡æ ‡

**è®­ç»ƒæ—¶å…³æ³¨**:

```python
# æ¯ä¸ªepochç»“æŸåæ£€æŸ¥
EP:1 | Train: loss=0.623, F1=0.701 | Val: loss=0.651, F1=0.682
EP:2 | Train: loss=0.587, F1=0.723 | Val: loss=0.645, F1=0.698  # âœ… Val F1ä¸Šå‡
EP:3 | Train: loss=0.561, F1=0.741 | Val: loss=0.639, F1=0.712  # âœ… æŒç»­ä¸Šå‡
...
EP:8 | Train: loss=0.492, F1=0.782 | Val: loss=0.628, F1=0.735  # âœ… æœ€ä½³
EP:9 | Train: loss=0.478, F1=0.791 | Val: loss=0.631, F1=0.733  # âš ï¸ Valä¸‹é™ (è¿‡æ‹Ÿåˆ)
```

**å¥½çš„ä¿¡å·**:
- âœ… Val F1æŒç»­ä¸Šå‡
- âœ… Train/Val F1å·®è· < 0.05
- âœ… Lossæ›²çº¿å¹³æ»‘

**åçš„ä¿¡å·**:
- âŒ Val F1éœ‡è¡å‰§çƒˆ
- âŒ Train/Val F1å·®è· > 0.1 (ä¸¥é‡è¿‡æ‹Ÿåˆ)
- âŒ Losså‡ºç°NaNæˆ–çˆ†ç‚¸

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼æœ‰é—®é¢˜éšæ—¶æ²Ÿé€šã€‚** ğŸš€
