# Validation Bugä¿®å¤è¯´æ˜

## ğŸ› é—®é¢˜å‘ç°

ç”¨æˆ·è§‚å¯Ÿåˆ°ä¸¤ä¸ªä¸¥é‡å¼‚å¸¸:

1. **Val Lossæ˜¯Train Lossçš„2å€**
   ```
   Epoch 3:
   - Train Loss: 105.4314
   - Val Loss:   209.8843
   - Ratio: 209.8843 / 105.4314 = 1.99 (å‡ ä¹å®Œå…¨2å€)
   ```

2. **Val Lossæ¯ä¸ªepochå®Œå…¨ç›¸åŒ**
   ```
   Epoch 1: Val Loss = 209.8843
   Epoch 2: Val Loss = 209.8843
   Epoch 3: Val Loss = 209.8843
   (16ä½å°æ•°å®Œå…¨ä¸€è‡´!)
   ```

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### Bug 1: Val Lossæ˜¾ç¤º2å€ (æ˜¾ç¤ºé—®é¢˜,ä¸å½±å“è®­ç»ƒ)

**ä»£ç è·¯å¾„**: `src/main/pretrain_with_val_optimized.py`

**é—®é¢˜ä»£ç **:
```python
# Line 85-86: Loss functionä½¿ç”¨sum reduction
self.hap_criterion = FocalLoss(gamma=focal_gamma, reduction='sum')
self.gt_criterion = FocalLoss(gamma=focal_gamma, reduction='sum')

# Line 282: ç´¯ç§¯æ‰€æœ‰batchçš„lossæ€»å’Œ
eval_dict['hap_loss'] += (hap_1_loss.item() + hap_2_loss.item())

# Line 365: é™¤ä»¥batchæ•°é‡ (é”™è¯¯!)
print(f"Avg Loss: {eval_dict['hap_loss'] / num_batches:.4f}")
```

**æ•°å­¦åˆ†æ**:
```
Lossä½¿ç”¨reduction='sum':
- æ¯ä¸ªbatchçš„loss = è¯¥batchæ‰€æœ‰æ ·æœ¬çš„lossæ€»å’Œ
- Train batch size = 64
- Val batch size = 128 (2å€)

å½“å‰è®¡ç®—æ–¹å¼:
- Avg Loss = total_sum_of_losses / num_batches
- Valæ¯ä¸ªbatchæœ‰2å€æ ·æœ¬ â†’ æ¯ä¸ªbatchçš„lossæ˜¯2å€
- é™¤ä»¥batchæ•°æ—¶,Valæ˜¾ç¤ºä¸º2å€

åº”è¯¥çš„è®¡ç®—æ–¹å¼:
- Avg Loss = total_sum_of_losses / total_num_samples
- total_num_samples = num_batches * batch_size
```

**éªŒè¯**:
- Train: batch_size=64, Loss=105.4314
- Val: batch_size=128, Loss=209.8843
- Ratio: 209.8843 / 105.4314 = **1.99** âœ“ (å®Œå…¨ç¬¦åˆ2å€å…³ç³»)

---

### Bug 2: Val Losså®Œå…¨ä¸å˜ âš ï¸ **ä¸¥é‡é—®é¢˜**

**ä»£ç è·¯å¾„**: `src/dataset/rag_train_dataset.py`

**é—®é¢˜ä»£ç **:

1. **Maskåœ¨åˆå§‹åŒ–æ—¶ç”Ÿæˆ,æ°¸ä¹…å­˜å‚¨** (Line 38-71)
```python
def _build_faiss_indexes(self, ref_vcf_path: str):
    """æ„å»ºFAISSç´¢å¼•"""
    for w_idx in range(self.window_count):
        # Maskç”Ÿæˆä¸€æ¬¡
        raw_mask = self.generate_mask(window_len)  # åªè°ƒç”¨ä¸€æ¬¡!

        # å­˜å‚¨æ°¸ä¹…ä½¿ç”¨
        self.raw_window_masks.append(raw_mask)
        self.window_masks.append(padded_mask)      # å›ºå®šä¸å˜
```

2. **æ¯æ¬¡__getitem__ä½¿ç”¨ç›¸åŒmask** (Line 162)
```python
def __getitem__(self, item) -> dict:
    window_idx = item % self.window_count
    current_mask = self.window_masks[window_idx]  # æ°¸è¿œè¿”å›åŒä¸€ä¸ªmask!

    output['mask'] = current_mask
    output['hap_1'] = self.tokenize(output['hap1_nomask'], current_mask)
    output['hap_2'] = self.tokenize(output['hap2_nomask'], current_mask)
    return output
```

3. **Validationä¸shuffle** (`train_with_val_optimized.py` Line 160)
```python
val_dataloader = DataLoader(
    rag_val_loader,
    shuffle=False,  # æ¯ä¸ªepoché¡ºåºç›¸åŒ
    ...
)
```

**ç»“æœ**:
```
Validationæ¯ä¸ªepoch:
- ç›¸åŒçš„æ ·æœ¬é¡ºåº (shuffle=False)
- ç›¸åŒçš„maskä½ç½® (self.window_masks[idx])
- ç›¸åŒçš„é¢„æµ‹
â†’ å®Œå…¨ç›¸åŒçš„loss (209.8843)
```

**å¯¹æ¯”base TrainDataset** (`dataset.py` Line 497):
```python
def __getitem__(self, item) -> dict:
    # Base classæ¯æ¬¡åŠ¨æ€ç”Ÿæˆmask
    mask = self.generate_mask(gt_label.shape[0])  # æ¯æ¬¡è°ƒç”¨éƒ½ç”Ÿæˆæ–°mask
    return {...}
```

**å½±å“ä¸¥é‡æ€§**:
- âŒ éªŒè¯é›†æŒ‡æ ‡æ— æ³•åæ˜ æ¨¡å‹çœŸå®æ³›åŒ–èƒ½åŠ›
- âŒ Early stoppingå®Œå…¨å¤±æ•ˆ (val metricæ°¸è¿œä¸å˜)
- âŒ æ— æ³•åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆ
- âŒ æ— æ³•è¿½è¸ªè®­ç»ƒè¿›åº¦

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: æ­£ç¡®çš„Losså½’ä¸€åŒ–

**æ–‡ä»¶**: `src/main/pretrain_with_val_optimized.py`

**ä¿®æ”¹**:
```python
# Line 298: ä¼ é€’batch_size
self._print_epoch_summary(epoch, eval_dict, len(dataloader),
                         dataloader.batch_size, train=train)

# Line 332: å‡½æ•°ç­¾åå¢åŠ batch_sizeå‚æ•°
def _print_epoch_summary(self, epoch, eval_dict, num_batches, batch_size, train=True):

# Line 366-368: æŒ‰æ ·æœ¬æ•°å½’ä¸€åŒ–
total_samples = num_batches * batch_size
print(f"Avg Loss: {eval_dict['hap_loss'] / total_samples:.4f}")
```

**æ•ˆæœ**:
- Trainå’ŒValçš„losså°†åœ¨åŒä¸€æ•°é‡çº§
- å¯ä»¥ç›´æ¥å¯¹æ¯”Train vs Val loss
- Lossæ•°å€¼æ›´æœ‰å®é™…æ„ä¹‰ (æ¯ä¸ªæ ·æœ¬çš„å¹³å‡loss)

---

### ä¿®å¤2: åŠ¨æ€Maskç”Ÿæˆ

**æ–‡ä»¶**: `src/dataset/rag_train_dataset.py`

**ä¿®æ”¹1: æ·»åŠ æ§åˆ¶æ ‡å¿—** (Line 23-30)
```python
class RAGTrainDataset(TrainDataset):
    def __init__(self, vocab, vcf, pos, panel, freq, window,
                 type_to_idx, pop_to_idx, pos_to_idx,
                 ref_vcf_path=None, build_ref_data=True, n_gpu=1,
                 maf_mask_percentage=10,
                 use_dynamic_mask=False):  # æ–°å¢å‚æ•°
        super().__init__(...)
        self.use_dynamic_mask = use_dynamic_mask  # ä¿å­˜æ ‡å¿—
        # ... rest of init
```

**ä¿®æ”¹2: æ¡ä»¶ç”Ÿæˆmask** (Line 161-183)
```python
def __getitem__(self, item) -> dict:
    output = super().__getitem__(item)
    window_idx = item % self.window_count

    # æ ¹æ®æ ‡å¿—é€‰æ‹©é™æ€æˆ–åŠ¨æ€mask
    if self.use_dynamic_mask:
        # åŠ¨æ€ç”Ÿæˆ (æ¯æ¬¡è°ƒç”¨éƒ½ç”Ÿæˆæ–°mask)
        window_len = self.window.window_info[window_idx, 1] - \
                     self.window.window_info[window_idx, 0]
        raw_mask = self.generate_mask(window_len)
        current_mask = VCFProcessingModule.sequence_padding(raw_mask, dtype='int')
    else:
        # ä½¿ç”¨é¢„ç”Ÿæˆ (è®­ç»ƒæ—¶ä¿æŒä¸€è‡´æ€§)
        current_mask = self.window_masks[window_idx]

    output['mask'] = current_mask
    output['hap_1'] = self.tokenize(output['hap1_nomask'], current_mask)
    output['hap_2'] = self.tokenize(output['hap2_nomask'], current_mask)
    return output
```

**ä¿®æ”¹3: from_fileæ”¯æŒå‚æ•°** (Line 185-218)
```python
@classmethod
def from_file(cls, vocab, vcfpath, panelpath, ...,
              use_dynamic_mask=False):  # æ–°å¢å‚æ•°
    base_dataset = super().from_file(...)
    rag_dataset = cls(
        ...,
        use_dynamic_mask=use_dynamic_mask  # ä¼ é€’å‚æ•°
    )
    return rag_dataset
```

**ä¿®æ”¹4: Validationå¯ç”¨åŠ¨æ€mask** (`train_with_val_optimized.py` Line 153)
```python
rag_val_loader = RAGTrainDataset.from_file(
    vocab,
    args.val_dataset,
    args.val_panel,
    ...,
    use_dynamic_mask=True  # éªŒè¯é›†ä½¿ç”¨åŠ¨æ€mask
)
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### Lossæ˜¾ç¤ºä¿®å¤å

**ä¹‹å‰**:
```
Epoch 3:
  Train Loss: 105.4314  (per 833 samples)
  Val Loss:   209.8843  (per 147 samples)
  â†’ çœ‹èµ·æ¥Val lossæ˜¯Trainçš„2å€
```

**ä¿®å¤å**:
```
Epoch 3:
  Train Loss: ~1.64  (105.4314 / (4309*64/833))
  Val Loss:   ~1.64  (209.8843 / (381*128/147))
  â†’ Trainå’ŒVal losså¯æ¯”
```

å®é™…æ•°å­—éœ€è¦é‡æ–°è®­ç»ƒæ‰èƒ½çœ‹åˆ°,ä½†åº”è¯¥åœ¨åŒä¸€æ•°é‡çº§ã€‚

---

### åŠ¨æ€Maskä¿®å¤å

**ä¹‹å‰**:
```
Epoch 1: Val Loss = 209.8843, Val F1 = 0.9781
Epoch 2: Val Loss = 209.8843, Val F1 = 0.9781  â† å®Œå…¨ç›¸åŒ
Epoch 3: Val Loss = 209.8843, Val F1 = 0.9781  â† 16ä½å°æ•°ä¸€è‡´
```

**ä¿®å¤å**:
```
Epoch 1: Val Loss = X.XXXX, Val F1 = 0.97XX
Epoch 2: Val Loss = Y.YYYY, Val F1 = 0.98XX  â† æœ‰å˜åŒ–
Epoch 3: Val Loss = Z.ZZZZ, Val F1 = 0.98XX  â† åæ˜ çœŸå®è¿›æ­¥
```

**é¢„æœŸå˜åŒ–**:
- Val lossä¼šæœ‰è‡ªç„¶æ³¢åŠ¨ (Â±0.1-0.5)
- Val F1ä¼šéšè®­ç»ƒæ”¹å–„ (æˆ–ç¨³å®š,æˆ–ä¸‹é™ â†’ è¿‡æ‹Ÿåˆ)
- Early stoppingèƒ½æ­£å¸¸å·¥ä½œ
- å¯ä»¥è¿½è¸ªæ¨¡å‹çœŸå®çš„æ³›åŒ–èƒ½åŠ›

---

## ğŸ”¬ è®¾è®¡è§£é‡Š

### ä¸ºä»€ä¹ˆTrainingä½¿ç”¨é™æ€mask?

**ç†ç”±**:
1. **Curriculum Learning**: Trainingä½¿ç”¨`add_level()`é€æ­¥å¢åŠ maskæ¯”ä¾‹
   - Epoch 1: 10% mask
   - Epoch 2: 20% mask
   - ...
   - éœ€è¦maskæ¯”ä¾‹å¯æ§

2. **FAISSæ£€ç´¢ä¸€è‡´æ€§**: æ¯ä¸ªwindowçš„maskåœ¨åˆå§‹åŒ–æ—¶ç”Ÿæˆ,åŒæ—¶æ„å»ºFAISSç´¢å¼•
   - å¦‚æœåŠ¨æ€mask,æ£€ç´¢çš„ä¸Šä¸‹æ–‡ä¼šä¸ä¸€è‡´

3. **è®­ç»ƒç¨³å®šæ€§**: é™æ€maskä¿è¯åŒä¸€ä¸ªwindowåœ¨åŒä¸€ä¸ªepochå†…çœ‹åˆ°ç›¸åŒçš„mask pattern

### ä¸ºä»€ä¹ˆValidationä½¿ç”¨åŠ¨æ€mask?

**ç†ç”±**:
1. **çœŸå®è¯„ä¼°**: æ¯ä¸ªepochåº”è¯¥æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒmask patternä¸‹çš„æ³›åŒ–èƒ½åŠ›
2. **é¿å…è¿‡æ‹Ÿåˆ**: å¦‚æœmaskå›ºå®š,æ¨¡å‹å¯èƒ½è®°ä½ç‰¹å®šmaskçš„ç­”æ¡ˆ
3. **Early Stopping**: éœ€è¦çœ‹åˆ°çœŸå®çš„éªŒè¯æŒ‡æ ‡å˜åŒ–,æ‰èƒ½åˆ¤æ–­ä½•æ—¶åœæ­¢

### ä¸ºä»€ä¹ˆä¸ç®€å•åœ°shuffle validation?

**é—®é¢˜**: å³ä½¿shuffle,æ¯ä¸ªæ ·æœ¬çš„maskä»ç„¶å›ºå®š
```python
# å‡è®¾shuffleåé¡ºåºæ”¹å˜
Epoch 1: [sample_A (mask_A), sample_B (mask_B), sample_C (mask_C)]
Epoch 2: [sample_C (mask_C), sample_A (mask_A), sample_B (mask_B)]
# è™½ç„¶é¡ºåºä¸åŒ,ä½†æ¯ä¸ªæ ·æœ¬çš„maskä»ç„¶ç›¸åŒ!
```

**åŠ¨æ€maskçš„ä¼˜åŠ¿**: æ¯æ¬¡çœ‹åˆ°åŒä¸€ä¸ªsampleæ—¶,maskä½ç½®éƒ½ä¸åŒ
```python
Epoch 1: sample_A masks positions [10, 25, 67, ...]
Epoch 2: sample_A masks positions [15, 30, 72, ...]  â† ä¸åŒ!
```

---

## ğŸš€ é‡æ–°è¿è¡Œ

### æ­¥éª¤1: æ‹‰å–ä¿®å¤

```bash
cd /cpfs01/projects-HDD/humPOG_HDD/wbn_24110700074/RAG_Version/VCF-Bert/00_RAG-SNVBERT-packup
git pull origin main
```

### æ­¥éª¤2: é‡æ–°è®­ç»ƒ

```bash
bash run_v13_optimized.sh
```

### æ­¥éª¤3: è§‚å¯Ÿä¿®å¤æ•ˆæœ

**ç›‘æ§Val losså˜åŒ–**:
```bash
grep "VAL Summary" -A 2 logs/optimized_gamma25_norecon/latest.log
```

**åº”è¯¥çœ‹åˆ°**:
```
Epoch 1 VAL Summary
Avg Loss:      1.XXXX  â† æ–°çš„å½’ä¸€åŒ–loss
...

Epoch 2 VAL Summary
Avg Loss:      1.YYYY  â† åº”è¯¥æœ‰å˜åŒ–!
...

Epoch 3 VAL Summary
Avg Loss:      1.ZZZZ  â† ä¸åº”è¯¥å®Œå…¨ç›¸åŒ
```

**ç›‘æ§F1å˜åŒ–**:
```bash
grep "Rare Variants" logs/optimized_gamma25_norecon/latest.log | grep -A 1 "VAL"
```

**åº”è¯¥çœ‹åˆ°**:
```
Epoch 1: Rare F1 = 0.95XX
Epoch 2: Rare F1 = 0.96XX  â† åº”è¯¥æœ‰æå‡
Epoch 3: Rare F1 = 0.96XX  â† æˆ–ç¨³å®š,æˆ–ä¸‹é™
```

---

## ğŸ“ˆ æ€§èƒ½å½±å“

### è®¡ç®—å¼€é”€

**åŠ¨æ€maskç”Ÿæˆ**:
- æ¯ä¸ªæ ·æœ¬è°ƒç”¨`generate_mask(window_len)`ä¸€æ¬¡
- æ—¶é—´å¤æ‚åº¦: O(window_len) â‰ˆ O(100-200)
- ç›¸æ¯”æ•´ä¸ªforward pass (O(seq_len * d_model * layers)),å¯å¿½ç•¥

**å®é™…å½±å“**:
- Validationæ—¶é—´å¯èƒ½å¢åŠ  ~1-2% (negligible)
- Trainingæ—¶é—´ä¸å˜ (ä»ä½¿ç”¨é™æ€mask)

### å†…å­˜å¼€é”€

- åŠ¨æ€mask: ä¸´æ—¶ç”Ÿæˆ,forwardå®Œæˆåé‡Šæ”¾
- é™æ€mask: æ°¸ä¹…å­˜å‚¨åœ¨å†…å­˜ (ä½†è®­ç»ƒéœ€è¦)
- **å‡€å½±å“**: å‡ ä¹ä¸º0

---

## ğŸ¯ æ€»ç»“

| é—®é¢˜ | ä¸¥é‡æ€§ | ä¿®å¤éš¾åº¦ | å½±å“ |
|------|--------|---------|------|
| Val Loss 2å€æ˜¾ç¤º | ä½ (ä»…æ˜¾ç¤º) | ç®€å• | ä¿®å¤ålosså¯æ¯” |
| Val Losså®Œå…¨ä¸å˜ | **ä¸¥é‡** | ä¸­ç­‰ | ä¿®å¤åå¯è¿½è¸ªçœŸå®è¿›æ­¥ |

**å…³é”®ä¿®å¤**:
1. Losså½’ä¸€åŒ–: æŒ‰æ ·æœ¬æ•°è€Œä¸æ˜¯batchæ•°
2. åŠ¨æ€mask: Validationä½¿ç”¨`use_dynamic_mask=True`

**é¢„æœŸç»“æœ**:
- âœ… Trainå’ŒVal lossåœ¨åŒä¸€æ•°é‡çº§
- âœ… Val losså’ŒF1éšepochå˜åŒ–
- âœ… Early stoppingèƒ½æ­£å¸¸å·¥ä½œ
- âœ… å¯ä»¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆ
- âœ… å¯ä»¥è¿½è¸ªçœŸå®çš„è®­ç»ƒè¿›åº¦

---

**åˆ›å»ºæ—¶é—´**: 2025-12-02
**é—®é¢˜å‘ç°è€…**: User (æ•é”è§‚å¯Ÿ!)
**ä¿®å¤æäº¤**: commit f894017
