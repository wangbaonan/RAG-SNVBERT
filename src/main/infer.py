import numpy as np
import faiss
import tqdm
import torch
import torch.amp
import torch.nn as nn
from pathlib import Path
import os
import allel
import h5py
from typing import Optional, Dict, Tuple

from torch.utils.data import DataLoader

from concurrent.futures import as_completed
from concurrent.futures import ThreadPoolExecutor
from ..dataset import WordVocab, InferDataset, RAGInferDataset
from ..model import BERTFoundationModel, BERT
from ..dataset.utils  import VCFProcessingModule
from ..dataset.rag_train_dataset import rag_collate_fn_with_dataset
INFER_WINDOW_LEN = 1020
MAX_SEQ_LEN = 1030

class ProgressiveInferController:
    def __init__(self, orig_pos, initial_pos, initial_vcf, step_ratio=0.05):
        self.orig_pos = orig_pos.copy()  # å¿…é¡»ä¿æŒåŸå§‹é¡ºåº
        self.current_pos = initial_pos.copy()
        self.step_ratio = step_ratio     # æ­¥é•¿ä¸ºæ€»ä½ç‚¹çš„æ¯”ä¾‹
        self.vcf_data = initial_vcf.copy()
        self.total_count = len(orig_pos)  # æ€»ä½ç‚¹ç¼“å­˜
        
        # åˆå§‹åŒ–éªŒè¯
        if step_ratio <= 0 or step_ratio > 1:
            raise ValueError("step_ratio å¿…é¡»åœ¨ (0, 1] èŒƒå›´å†…")
        if not np.array_equal(np.sort(initial_pos), initial_pos):
            raise ValueError("åˆå§‹ä½ç½®æ•°ç»„å¿…é¡»å·²æ’åº")

    def get_next_positions(self) -> np.ndarray:
        """è·å–ä¸‹ä¸€æ‰¹ä½ç‚¹ï¼ˆä¸¥æ ¼åŸºäºæ€»ä½ç‚¹æ¯”ä¾‹ï¼‰"""
        remaining = self.orig_pos[~np.isin(self.orig_pos, self.current_pos)]
        remaining_count = len(remaining)
        
        # å…³é”®ä¿®æ­£ï¼šè®¡ç®—æ¯æ­¥åº”å¡«è¡¥çš„ç»å¯¹æ•°é‡
        step_count = max(1, int(self.total_count * self.step_ratio))
        # å¦‚æœå‰©ä½™ä¸è¶³æ­¥é•¿ï¼Œå…¨é€‰
        return remaining[:min(step_count, remaining_count)]

    def update_state(self, new_pos: np.ndarray):
        """ä¿æŒåŸå§‹é¡ºåºåˆå¹¶æ–°ä½ç‚¹"""
        combined = np.union1d(self.current_pos, new_pos)
        mask = np.isin(self.orig_pos, combined)
        self.current_pos = self.orig_pos[mask]

    def predict_progress(self, max_iter: int) -> Tuple[int, float]:
        """ç²¾ç¡®é¢„æµ‹æœ€ç»ˆå¡«è¡¥é‡"""
        simulated_pos = self.current_pos.copy()
        remaining = self.orig_pos[~np.isin(self.orig_pos, simulated_pos)]
        step_count = max(1, int(self.total_count * self.step_ratio))
        
        for _ in range(max_iter):
            fill_num = min(step_count, len(remaining))
            if fill_num == 0:
                break
            simulated_pos = np.union1d(simulated_pos, remaining[:fill_num])
            remaining = remaining[fill_num:]
        
        return len(simulated_pos), len(simulated_pos)/self.total_count*100

    @property
    def is_complete(self) -> bool:
        return np.array_equal(self.current_pos, self.orig_pos)


class BERTInfer():
    """
    This class contains all the information about inferring.
    """

    def __init__(self,
                 bert: BERT,
                 infer_dataloader: DataLoader = None,
                 vocab : WordVocab = None,
                 with_cuda: bool = True, 
                 cuda_devices = None, 
                 log_freq: int = 10,
                 state_dict = None,
                 output_path = None
                 ):
        """
        Attributes:

            bert : BERT model which you want to infer.
            infer_dataloader : infer dataset data loader.
            with_cuda : traning with cuda.
            log_freq : logging frequency of the batch iteration.
        """
        # Setup cuda device for BERT infering
        cuda_condition = torch.cuda.is_available() and with_cuda
        self.device = torch.device("cuda:" + str(cuda_devices[0]) if cuda_condition and cuda_devices is not None else "cpu")

        # This BERT model will be saved every epoch.
        self.bert = bert

        self.vocab = vocab

        self.output_path = output_path

        # Initialize the BERT Language Model.
        self.model = BERTFoundationModel(bert)

        # Distributed GPU infering if CUDA can detect more than 1 GPU.
        self.model.load_state_dict(state_dict=state_dict)
        print("Params' loading succeed.")

        # Load model into GPU.
        self.model = self.model.to(self.device)
        if with_cuda and torch.cuda.device_count() > 1:
            print("Using %d GPUS for BERT" % len(cuda_devices))
            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)

        # Set infer dataloader.
        self.infer_data = infer_dataloader

        self.log_freq = log_freq

        print("Total Parameters:", sum([p.nelement() for p in self.model.parameters()]))

        self.hap1_prob_mat = np.zeros((infer_dataloader.dataset.ori_pos.shape[0], infer_dataloader.dataset.vcf.shape[1]), dtype=np.float32)
        self.hap2_prob_mat = np.zeros((infer_dataloader.dataset.ori_pos.shape[0], infer_dataloader.dataset.vcf.shape[1]), dtype=np.float32)
        self.gt_prob_mat = np.zeros((infer_dataloader.dataset.ori_pos.shape[0], infer_dataloader.dataset.vcf.shape[1], 4), dtype=np.float32)
        
    def _core_infer_logic(self, dataloader: DataLoader) -> np.ndarray:
        """å°è£…çš„åŸå§‹æ¨ç†æ ¸å¿ƒé€»è¾‘ï¼ˆä¾›infer()å’Œprogressive_infer()å¤ç”¨ï¼‰"""
        self.model.eval()
    
        # åˆå§‹åŒ–ä¸´æ—¶å­˜å‚¨ï¼ˆé¿å…æ±¡æŸ“å®ä¾‹å˜é‡ï¼‰
        hap1_prob_mat = np.zeros_like(self.hap1_prob_mat)
        hap2_prob_mat = np.zeros_like(self.hap2_prob_mat)
        gt_prob_mat = np.zeros_like(self.gt_prob_mat)

        data_iter = tqdm.tqdm(enumerate(dataloader),
                            desc="INFER",
                            total=len(dataloader),
                            bar_format="{l_bar}{r_bar}")

        data_in_gpu = ['hap_1', 'hap_2', 'pos', 'af', 'af_p', 'ref', 'het', 'hom', 'rag_seg_h1', 'rag_seg_h2']

        for i, data in data_iter:
            gpu_data = {key: data[key].to(self.device) for key in data_in_gpu}
            with torch.no_grad():
                output = self.model.forward(gpu_data)[:3]

            for idx, tensor in enumerate(output):
                output[idx] = tensor.cpu().numpy()

            sample_idx = data['sample_idx'].numpy()
            start_idx = data['start_idx'].numpy()
            end_idx = data['end_idx'].numpy()

            for idx in range(sample_idx.shape[0]):
                sample_ = sample_idx[idx][0]
                start_ = start_idx[idx][0]
                end_ = end_idx[idx][0]
                len_ = end_ - start_ + 1

                hap1_prob_mat[start_:end_, sample_] = output[0][idx, 1:len_, 1]
                hap2_prob_mat[start_:end_, sample_] = output[1][idx, 1:len_, 1]
                gt_prob_mat[start_:end_, sample_, :] = output[2][idx, 1:len_, :]
                
            self.hap1_prob_mat = hap1_prob_mat
            self.hap2_prob_mat = hap2_prob_mat
            self.gt_prob_mat = gt_prob_mat

        return VCFProcessingModule.process_gt_prob_mat_with_progress(gt_prob_mat)

    def _core_infer_logic(self, dataloader: DataLoader) -> np.ndarray:
        self.model.eval()
        device = self.device
        
        # é¢„åˆ†é…ç»“æœç¼“å†²åŒº
        hap1_prob_mat = np.zeros_like(self.hap1_prob_mat)
        hap2_prob_mat = np.zeros_like(self.hap2_prob_mat)
        gt_prob_mat = np.zeros_like(self.gt_prob_mat)
        
        # æ··åˆç²¾åº¦é…ç½®
        amp_dtype = torch.float16 if 'cuda' in str(device) else torch.float32
        
        with torch.cuda.amp.autocast(dtype=amp_dtype), torch.no_grad():
            for i, data in tqdm.tqdm(enumerate(dataloader), 
                                total=len(dataloader),
                                desc="ä¼˜åŒ–æ¨ç†è¿›åº¦"):
                                
                # å¼‚æ­¥æ•°æ®ä¼ è¾“
                gpu_data = {
                    key: data[key].to(device, non_blocking=True) 
                    for key in data if torch.is_tensor(data[key])
                }
                
                # ç›´æ¥æ‰§è¡Œæ¨ç†ï¼ˆç§»é™¤CUDAå›¾ä¼˜åŒ–ï¼‰
                output = self.model(gpu_data)[:3]
                
                # è½¬æ¢åˆ°CPU
                output = [t.cpu().float() if t.dtype == torch.half else t.cpu() 
                        for t in output]
                
                # ç´¢å¼•è¾¹ç•Œä¿æŠ¤
                sample_idx = data['sample_idx'].numpy()
                start_idx = data['start_idx'].numpy()
                end_idx = data['end_idx'].numpy()
                
                for idx in range(sample_idx.shape[0]):
                    sample = sample_idx[idx][0]
                    start = start_idx[idx][0]
                    end = end_idx[idx][0]
                    
                    # æ–°å¢ç´¢å¼•æ ¡éªŒ
                    assert start >= 0, f"éæ³•èµ·å§‹ç´¢å¼•: {start}"
                    assert end <= hap1_prob_mat.shape[0], (
                        f"ç»“æŸç´¢å¼•è¶Šç•Œ: end={end}, çŸ©é˜µé•¿åº¦={hap1_prob_mat.shape[0]}")
                    
                    # ä½¿ç”¨å‘é‡åŒ–èµ‹å€¼
                    hap1_prob_mat[start:end, sample] = output[0][idx, 1:(end-start+1), 1].numpy()
                    hap2_prob_mat[start:end, sample] = output[1][idx, 1:(end-start+1), 1].numpy()
                    gt_prob_mat[start:end, sample, :] = output[2][idx, 1:(end-start+1), :].numpy()

        # åŸå­æ›´æ–°
        with torch.no_grad():
            self.hap1_prob_mat = hap1_prob_mat
            self.hap2_prob_mat = hap2_prob_mat
            self.gt_prob_mat = gt_prob_mat

        return VCFProcessingModule.process_gt_prob_mat_with_progress(gt_prob_mat)



    def infer(self):
        """Loop over the dataloader for infering.
        """
        self.model.eval()
        mode_code = "INFER"

        # Set tqdm bar
        data_iter = tqdm.tqdm(enumerate(self.infer_data),
                              desc="EP_%s" % (mode_code),
                              total=len(self.infer_data),
                              bar_format="{l_bar}{r_bar}")

        # Data in GPU.
        data_in_gpu = ['hap_1', 'hap_2', 'pos', 'af', 'af_p', 'ref', 'het', 'hom', 'rag_seg_h1', 'rag_seg_h2']


        for i, data in data_iter:
            # infer.
            gpu_data = {key: data[key].to(self.device) for key in data_in_gpu}
            with torch.no_grad():
                output  = self.model.forward(gpu_data)[:3]

            for idx, tensor in enumerate(output):
                output[idx] = tensor.cpu().numpy()
            
            sample_idx = data['sample_idx'].numpy()
            start_idx = data['start_idx'].numpy()
            end_idx = data['end_idx'].numpy()

            for idx in range(sample_idx.shape[0]):      # Batch size
                sample_ = sample_idx[idx][0]
                start_ = start_idx[idx][0]
                end_ = end_idx[idx][0]
                
                len_ = end_ - start_ + 1

                self.hap1_prob_mat[start_:end_, sample_] = output[0][idx, 1:len_, 1]
                self.hap2_prob_mat[start_:end_, sample_] = output[1][idx, 1:len_, 1]
                self.gt_prob_mat[start_:end_, sample_, :] = output[2][idx, 1:len_, :]
                #print("\n===== gt_prob_mat éªŒè¯ =====")

        vcf_data = VCFProcessingModule.process_gt_prob_mat_with_progress(self.gt_prob_mat)
        self.save_npy_result()
        return vcf_data

    def infer(self):
        """åŸå§‹å…¨é‡æ¨ç†æ–¹æ³•ï¼ˆä¿æŒå®Œå…¨ä¸€è‡´ï¼‰"""
        # é€šè¿‡æ ¸å¿ƒé€»è¾‘è·å–ç»“æœ
        vcf_data = self._core_infer_logic(self.infer_data)
    
        # ä»¥ä¸‹ä¿æŒåŸå§‹åå¤„ç†é€»è¾‘
        self.save_npy_result()
        return vcf_data

    def progressive_infer(self, 
                     step_ratio: float = 0.1,
                     max_iter: int = 100) -> np.ndarray:
        """æ¸è¿›å¼æ¨ç†å…¥å£"""
        # åˆå§‹åŒ–æ§åˆ¶å™¨
        iteration = 1
        controller = ProgressiveInferController(
            orig_pos=self.infer_data.dataset.ori_pos.copy(),
            initial_pos=self.infer_data.dataset.pos.copy(),
            initial_vcf=self.infer_data.dataset.vcf.copy(),
            step_ratio=step_ratio
        )
        pred_count, pred_percent = controller.predict_progress(max_iter)
        print(f"ğŸš€ å¼€å§‹æ¸è¿›å¼æ¨ç† | æ€»ä½ç‚¹: {len(controller.orig_pos)} | åˆå§‹å·²çŸ¥: {len(controller.current_pos)}")
        # æ¸è¿›å¼å¾ªç¯
        while not controller.is_complete and max_iter > 0:
            # æ‰“å°å½“å‰çŠ¶æ€
            remaining = len(controller.orig_pos) - len(controller.current_pos)
            print(f"ğŸ”„ è¿­ä»£ {iteration} | å‰©ä½™ä½ç‚¹: {remaining} | æœ€å¤§å‰©ä½™è¿­ä»£: {max_iter}")

            # åŠ¨æ€æ„å»ºæ•°æ®é›†
            new_dataset = RAGInferDataset(
                vocab=self.vocab,
                vcf=controller.vcf_data,
                pos=controller.current_pos,
                panel=self.infer_data.dataset.panel,
                freq=self.infer_data.dataset.freq,
                type_to_idx=self.infer_data.dataset.type_to_idx,
                pop_to_idx=self.infer_data.dataset.pop_to_idx,
                pos_to_idx=self.infer_data.dataset.pos_to_idx,
                ref_vcf_path=self.infer_data.dataset.ref_vcf_path,
                build_index=True
            )
            new_dataloader = DataLoader(new_dataset,
                                    batch_size=self.infer_data.batch_size,
                                    num_workers=self.infer_data.num_workers,
                                    collate_fn=lambda batch_list: rag_collate_fn_with_dataset(batch_list, new_dataset, 5))
        
            # æ‰§è¡Œæ¨ç†
            current_vcf = self._core_infer_logic(new_dataloader)
        
            # æ›´æ–°çŠ¶æ€
            new_pos = controller.get_next_positions()
            controller.vcf_data = current_vcf
            controller.update_state(new_pos)
            # æ‰“å°è¿›åº¦
            coverage = len(controller.current_pos) / len(controller.orig_pos) * 100
            print(f"âœ… å½“å‰è¦†ç›–: {coverage:.2f}% | ç´¯è®¡å·²çŸ¥ä½ç‚¹: {len(controller.current_pos)}\n")
            
            iteration += 1
            max_iter -= 1
        self.save_npy_result()
        print(f"ğŸ‰ æ¨ç†å®Œæˆï¼æœ€ç»ˆè¦†ç›–: {len(controller.current_pos)}/{len(controller.orig_pos)} ä¸ªä½ç‚¹")
        return controller.vcf_data
    
    def progressive_infer(self, step_ratio=0.5, max_iter=100):
        controller = ProgressiveInferController(
            self.infer_data.dataset.ori_pos,
            self.infer_data.dataset.pos,
            self.infer_data.dataset.vcf,
            step_ratio
        )
        
        print(f"ğŸš€ å¼€å§‹æ¸è¿›å¼æ¨ç† | æ€»ä½ç‚¹: {len(controller.orig_pos)} | åˆå§‹å·²çŸ¥: {len(controller.current_pos)}")
        
        iteration = 1
        while not controller.is_complete and max_iter > 0:
            remaining = len(controller.orig_pos) - len(controller.current_pos)
            coverage = len(controller.current_pos) / len(controller.orig_pos) * 100
            
            # ä¿æŒåŸæœ‰æ—¥å¿—æ ¼å¼
            print(f"\nğŸ”„ è¿­ä»£ {iteration} | å‰©ä½™ä½ç‚¹: {remaining} | å½“å‰è¦†ç›–: {coverage:.2f}%")
            
            new_dataset = RAGInferDataset(
                vocab=self.vocab,
                vcf=controller.vcf_data,
                pos=controller.current_pos,
                panel=self.infer_data.dataset.panel,
                freq=self.infer_data.dataset.freq,
                type_to_idx=self.infer_data.dataset.type_to_idx,
                pop_to_idx=self.infer_data.dataset.pop_to_idx,
                pos_to_idx=self.infer_data.dataset.pos_to_idx,
                ref_vcf_path=self.infer_data.dataset.ref_vcf_path,
                build_index=True
            )
            new_dataloader = DataLoader(
                new_dataset,
                batch_size=self.infer_data.batch_size,
                num_workers=self.infer_data.num_workers,
                collate_fn=lambda batch_list: rag_collate_fn_with_dataset(batch_list, new_dataset, 5)
            )
            
            current_vcf = self._core_infer_logic(new_dataloader)
            
            new_pos = controller.get_next_positions()
            controller.update_state(new_pos)
            
            # æ–°å¢å…³é”®åˆ¤æ–­
            if len(new_pos) == 0:
                print("âš ï¸ æ— æ–°ä½ç‚¹å¯æ·»åŠ ï¼Œæå‰ç»ˆæ­¢")
                break
                
            print(f"âœ… æ–°å¢ {len(new_pos)} ä¸ªä½ç‚¹ | ç´¯è®¡å·²çŸ¥: {len(controller.current_pos)}")
            
            iteration += 1
            max_iter -= 1

        # æœ€ç»ˆå¼ºåˆ¶è¦†ç›–ä¿éšœï¼ˆä¿®æ­£é€»è¾‘ï¼‰
        if not controller.is_complete:
            print("\nğŸ”š è¿›å…¥æœ€ç»ˆè¡¥å…¨é˜¶æ®µ")
            final_pos = controller.orig_pos[~np.isin(controller.orig_pos, controller.current_pos)]
            print(f"ğŸ”¥ æ­£åœ¨åŠ è½½æœ€å {len(final_pos)} ä¸ªä½ç‚¹")
            
            # å…³é”®ä¿®æ­£ï¼šå¿…é¡»é€šè¿‡update_stateæ¥ä¿è¯é¡ºåº
            controller.update_state(final_pos)
            
            # é‡æ–°æ„å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨æ›´æ–°åçš„current_posï¼‰
            final_dataset = RAGInferDataset(
                vocab=self.vocab,
                vcf=controller.vcf_data,
                pos=controller.current_pos,  # è¿™é‡Œå·²ç»æ˜¯åˆå¹¶åçš„æœ‰åºæ•°ç»„
                panel=self.infer_data.dataset.panel,
                freq=self.infer_data.dataset.freq,
                type_to_idx=self.infer_data.dataset.type_to_idx,
                pop_to_idx=self.infer_data.dataset.pop_to_idx,
                pos_to_idx=self.infer_data.dataset.pos_to_idx,
                ref_vcf_path=self.infer_data.dataset.ref_vcf_path,
                build_index=True
            )
            final_loader = DataLoader(
                final_dataset,
                batch_size=self.infer_data.batch_size,
                num_workers=self.infer_data.num_workers,
                collate_fn=lambda batch_list: rag_collate_fn_with_dataset(batch_list, final_dataset, 5)
            )
            controller.vcf_data = self._core_infer_logic(final_loader)

        # æ–°å¢æœ€ç»ˆéªŒè¯ï¼ˆç¡®ä¿å®Œå…¨å¯¹é½ï¼‰
        if not np.array_equal(controller.current_pos, controller.orig_pos):
            missing = len(controller.orig_pos) - len(controller.current_pos)
            raise RuntimeError(f"âŒ æœ€ç»ˆéªŒè¯å¤±è´¥ï¼ç¼ºå¤±ä½ç‚¹æ•°: {missing}")
        print(f"\nâœ… æœ€ç»ˆéªŒè¯é€šè¿‡ | æ€»ä½ç‚¹å®Œå…¨å¯¹é½")
        
        return controller.vcf_data

    def save_npy_result(self) -> None:
        """Call this func to save results from self.infer().
        """
        np.save(self.output_path + "/HAP1.npy", self.hap1_prob_mat)
        print("HAP1 saved.")

        np.save(self.output_path + "/HAP2.npy", self.hap2_prob_mat)
        print("HAP2 saved.")

        np.save(self.output_path + "/GT.npy", self.gt_prob_mat)
        print("GT saved.")

        np.save(self.output_path + "/POS.npy", self.infer_data.dataset.ori_pos)
        print("POS saved.")

        np.save(self.output_path + "/POS_Flag.npy", self.infer_data.dataset.position_needed)
        print("POS_FLAG saved.")
    
