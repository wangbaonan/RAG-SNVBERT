import numpy as np
import faiss
import tqdm
import torch
import torch.amp
import torch.nn as nn
from pathlib import Path
import os
import allel
import h5py
from typing import Optional, Dict, Tuple

from torch.utils.data import DataLoader

from concurrent.futures import as_completed
from concurrent.futures import ThreadPoolExecutor
from ..dataset import WordVocab, InferDataset, RAGInferDataset
from ..model import BERTFoundationModel, BERT
from ..dataset.utils  import VCFProcessingModule
from ..dataset.rag_train_dataset import rag_collate_fn_with_dataset
INFER_WINDOW_LEN = 1020
MAX_SEQ_LEN = 1030

'''
# ËæÖÂä©Á±ª
class ProgressiveInferController:
    """Ê∏êËøõÂºèÊé®ÁêÜÊéßÂà∂Âô®ÔºàÂÆåÂÖ®Áã¨Á´ãÁ±ªÔºâ"""
    def __init__(self, 
                 orig_pos: np.ndarray,
                 initial_pos: np.ndarray,
                 initial_vcf: np.ndarray,
                 step_ratio: float = 0.1):
        """
        Args:
            orig_pos: ÂÖ®Èáè‰ΩçÁÇπÂùêÊ†á (n_variants,)
            initial_pos: ÂàùÂßãÂ∑≤Áü•‰ΩçÁÇπ (m_variants,)
            initial_vcf: ÂàùÂßãVCFÊï∞ÊçÆ (n_variants, n_samples, 2)
            step_ratio: ÊØèÊ¨°Êñ∞Â¢û‰ΩçÁÇπÁöÑÊØî‰æã
        """
        # ÂùêÊ†áÁÆ°ÁêÜ
        self.orig_pos = orig_pos
        self.current_pos = initial_pos
        self.step_ratio = step_ratio
        
        # Êï∞ÊçÆÁÆ°ÁêÜ
        self.vcf_data = initial_vcf.copy()
        
    def get_next_positions(self) -> np.ndarray:
        """Ëé∑Âèñ‰∏ã‰∏ÄÊâπÈúÄË¶ÅÂºÄÊîæÁöÑ‰ΩçÁÇπ"""
        remaining = self.orig_pos[~np.isin(self.orig_pos, self.current_pos)]
        n_new = max(1, int(len(remaining) * self.step_ratio))
        return remaining[:n_new]
    
    def update_state(self, new_pos: np.ndarray):
        """Êõ¥Êñ∞Áä∂ÊÄÅÔºà‰∏ç‰øÆÊîπÊï∞ÊçÆÔºâ"""
        self.current_pos = np.union1d(self.current_pos, new_pos)
    
    @property
    def is_complete(self) -> bool:
        # ‰ºòÂÖàÊ£ÄÊü•ÈïøÂ∫¶Â∑ÆÂºÇÂø´ÈÄüËøîÂõû
        if len(self.current_pos) != len(self.orig_pos):
            return False
        # Á≤æÁ°ÆÊ£ÄÊü•ÊòØÂê¶ÂåÖÂê´ÊâÄÊúâÂÖÉÁ¥†
        return np.all(np.isin(self.orig_pos, self.current_pos))
'''

class ProgressiveInferController:
    def __init__(self, 
                 orig_pos: np.ndarray,
                 initial_pos: np.ndarray,
                 initial_vcf: np.ndarray,
                 step_ratio: float = 0.2):
        # ÂàùÂßãÂåñÈÄªËæë‰øùÊåÅ‰∏çÂèò
        self.orig_pos = orig_pos
        self.current_pos = initial_pos
        self.step_ratio = step_ratio
        self.vcf_data = initial_vcf.copy()

    """
    def get_next_positions(self) -> np.ndarray:
        ### ÊåâÂÖ®Èáè‰ΩçÁÇπÊÄªÊï∞ÁöÑÊØî‰æãËé∑Âèñ‰∏ã‰∏ÄÊâπ‰ΩçÁÇπ
        remaining = self.orig_pos[~np.isin(self.orig_pos, self.current_pos)]
        
        # Ê†∏ÂøÉ‰øÆÊîπÁÇπÔºöÂü∫‰∫éÂÖ®Èáè‰ΩçÁÇπÁöÑÊØî‰æãËÆ°ÁÆóÊñ∞Â¢ûÊï∞Èáè
        n_new_total = max(1, int(len(self.orig_pos) * self.step_ratio))
        n_new = min(n_new_total, len(remaining))  # ‰∏çË∂ÖËøáÂâ©‰ΩôÊï∞Èáè
        
        return remaining[:n_new]  # ‰øùÁïôÂéüÂßãÈ°∫Â∫è
    """

    def get_next_positions(self) -> np.ndarray:
        ### ÂÆâÂÖ®ÁöÑÊñ∞Â¢û‰ΩçÁÇπÈÄâÊã©
        # Á≤æÁ°ÆËÆ°ÁÆóÂâ©‰Ωô‰ΩçÁÇπ
        remaining_mask = ~np.isin(self.orig_pos, self.current_pos)
        remaining = self.orig_pos[remaining_mask]
        
        # Âü∫‰∫éÂâ©‰Ωô‰ΩçÁÇπÁöÑÂÆûÈôÖÊï∞ÈáèËÆ°ÁÆóÊñ∞Â¢û
        n_new = max(1, int(len(remaining) * self.step_ratio))
        return remaining[:n_new]
    
    """
    def update_state(self, new_pos: np.ndarray):
        # ÂéüÊúâÈÄªËæë‰øùÊåÅ‰∏çÂèò
        self.current_pos = np.union1d(self.current_pos, new_pos)
    """

    def update_state(self, new_pos: np.ndarray):
        """Â∏¶ÈáçÂ§çÊ†°È™åÁöÑÁä∂ÊÄÅÊõ¥Êñ∞"""
        # Á°Æ‰øùÊñ∞‰ΩçÁÇπÂ≠òÂú®‰∏îÂîØ‰∏Ä
        valid_new = np.intersect1d(new_pos, self.orig_pos, assume_unique=True)
        self.current_pos = np.unique(np.concatenate([self.current_pos, valid_new]))
        
        # ÂÆâÂÖ®Êà™Êñ≠
        if len(self.current_pos) > len(self.orig_pos):
            self.current_pos = np.intersect1d(self.current_pos, self.orig_pos)
            
    """
    @property
    def is_complete(self) -> bool:
        remaining = len(self.orig_pos) - len(self.current_pos)
        # ÂΩìÂâ©‰Ωô‰ΩçÁÇπ <= 1000 ÊàñÂÆåÂÖ®Ë¶ÜÁõñÊó∂ÁªàÊ≠¢
        return remaining <= 1000 or remaining == 0 
    """

    @property
    def is_complete(self) -> bool:
        remaining = len(self.orig_pos) - len(self.current_pos)
        return remaining <= 0  # ‰∏•Ê†ºÈùûË¥üÁªàÊ≠¢

class BERTInfer():
    """
    This class contains all the information about inferring.
    """

    def __init__(self,
                 bert: BERT,
                 infer_dataloader: DataLoader = None,
                 vocab : WordVocab = None,
                 with_cuda: bool = True, 
                 cuda_devices = None, 
                 log_freq: int = 10,
                 state_dict = None,
                 output_path = None
                 ):
        """
        Attributes:

            bert : BERT model which you want to infer.
            infer_dataloader : infer dataset data loader.
            with_cuda : traning with cuda.
            log_freq : logging frequency of the batch iteration.
        """
        # Setup cuda device for BERT infering
        cuda_condition = torch.cuda.is_available() and with_cuda
        self.device = torch.device("cuda:" + str(cuda_devices[0]) if cuda_condition and cuda_devices is not None else "cpu")

        # This BERT model will be saved every epoch.
        self.bert = bert

        self.vocab = vocab

        self.output_path = output_path

        # Initialize the BERT Language Model.
        self.model = BERTFoundationModel(bert)

        # Distributed GPU infering if CUDA can detect more than 1 GPU.
        self.model.load_state_dict(state_dict=state_dict)
        print("Params' loading succeed.")

        # Load model into GPU.
        self.model = self.model.to(self.device)
        if with_cuda and torch.cuda.device_count() > 1:
            print("Using %d GPUS for BERT" % len(cuda_devices))
            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)

        # Set infer dataloader.
        self.infer_data = infer_dataloader

        self.log_freq = log_freq

        print("Total Parameters:", sum([p.nelement() for p in self.model.parameters()]))

        self.hap1_prob_mat = np.zeros((infer_dataloader.dataset.ori_pos.shape[0], infer_dataloader.dataset.vcf.shape[1]), dtype=np.float32)
        self.hap2_prob_mat = np.zeros((infer_dataloader.dataset.ori_pos.shape[0], infer_dataloader.dataset.vcf.shape[1]), dtype=np.float32)
        self.gt_prob_mat = np.zeros((infer_dataloader.dataset.ori_pos.shape[0], infer_dataloader.dataset.vcf.shape[1], 4), dtype=np.float32)

    def _core_infer_logic(self, dataloader: DataLoader) -> np.ndarray:
        """Â∞ÅË£ÖÁöÑÂéüÂßãÊé®ÁêÜÊ†∏ÂøÉÈÄªËæëÔºà‰æõinfer()Âíåprogressive_infer()Â§çÁî®Ôºâ"""
        self.model.eval()
    
        # ÂàùÂßãÂåñ‰∏¥Êó∂Â≠òÂÇ®ÔºàÈÅøÂÖçÊ±°ÊüìÂÆû‰æãÂèòÈáèÔºâ
        hap1_prob_mat = np.zeros_like(self.hap1_prob_mat)
        hap2_prob_mat = np.zeros_like(self.hap2_prob_mat)
        gt_prob_mat = np.zeros_like(self.gt_prob_mat)

        data_iter = tqdm.tqdm(enumerate(dataloader),
                            desc="INFER",
                            total=len(dataloader),
                            bar_format="{l_bar}{r_bar}")

        data_in_gpu = ['hap_1', 'hap_2', 'pos', 'af', 'af_p', 'ref', 'het', 'hom', 'rag_seg_h1', 'rag_seg_h2']

        for i, data in data_iter:
            gpu_data = {key: data[key].to(self.device) for key in data_in_gpu}
            with torch.no_grad():
                output = self.model.forward(gpu_data)[:3]

            for idx, tensor in enumerate(output):
                output[idx] = tensor.cpu().numpy()

            sample_idx = data['sample_idx'].numpy()
            start_idx = data['start_idx'].numpy()
            end_idx = data['end_idx'].numpy()

            for idx in range(sample_idx.shape[0]):
                sample_ = sample_idx[idx][0]
                start_ = start_idx[idx][0]
                end_ = end_idx[idx][0]
                len_ = end_ - start_ + 1

                hap1_prob_mat[start_:end_, sample_] = output[0][idx, 1:len_, 1]
                hap2_prob_mat[start_:end_, sample_] = output[1][idx, 1:len_, 1]
                gt_prob_mat[start_:end_, sample_, :] = output[2][idx, 1:len_, :]
                
            self.hap1_prob_mat = hap1_prob_mat
            self.hap2_prob_mat = hap2_prob_mat
            self.gt_prob_mat = gt_prob_mat

        return VCFProcessingModule.process_gt_prob_mat_with_progress(gt_prob_mat)

    """
    def _core_infer_logic(self, dataloader: DataLoader) -> np.ndarray:
        ### Â∏¶mask‰ΩçÁÇπÊ£ÄÊü•ÁöÑÊ†∏ÂøÉÈÄªËæë
        self.model.eval()
        
        hap1_prob_mat = np.zeros_like(self.hap1_prob_mat)
        hap2_prob_mat = np.zeros_like(self.hap2_prob_mat)
        gt_prob_mat = np.zeros_like(self.gt_prob_mat)

        data_iter = tqdm.tqdm(enumerate(dataloader),
                            desc="INFER",
                            total=len(dataloader),
                            bar_format="{l_bar}{r_bar}")

        data_in_gpu = ['hap_1', 'hap_2', 'pos', 'af', 'af_p', 'ref', 'het', 'hom', 'rag_seg_h1', 'rag_seg_h2']

        for i, data in data_iter:
            gpu_data = {key: data[key].to(self.device) for key in data_in_gpu}
            with torch.no_grad():
                output = self.model.forward(gpu_data)[:3]

            # ËΩ¨Êç¢ËæìÂá∫Âà∞numpy
            output = [t.cpu().numpy() for t in output]

            for idx in range(data['hap_1'].size(0)):
                sample_id = data['sample_idx'][idx][0].item()
                start_pos = data['start_idx'][idx][0].item()
                end_pos = data['end_idx'][idx][0].item()
                
                # ÊèêÂèñÂéüÂßãÂ∫èÂàóÂíåmask‰ΩçÁΩÆ
                h1 = data['hap_1'][idx].cpu().numpy().flatten()
                h2 = data['hap_2'][idx].cpu().numpy().flatten()
                mask_positions = np.where((h1 == 4) | (h2 == 4))[0]
                
                # Êñ∞Â¢ûÂîØ‰∏ÄÂÄºÊ£ÄÊü•ÈÄªËæë
                if len(mask_positions) == 0:
                    unique_h1 = np.unique(h1)
                    unique_h2 = np.unique(h2)
                    #print(f"\nË≠¶ÂëäÔºöÊ†∑Êú¨ {sample_id} Êú™Ê£ÄÊµãÂà∞mask‰ΩçÁÇπ(4)")
                    #print(f"HAP1 ÂîØ‰∏ÄÂÄºÔºö{unique_h1}")
                    #print(f"HAP2 ÂîØ‰∏ÄÂÄºÔºö{unique_h2}")
                    #print("ËØ∑Ê£ÄÊü•ËæìÂÖ•Êï∞ÊçÆÊòØÂê¶ÂåÖÂê´maskÊ†áËÆ∞ÔºàÂÄº‰∏∫4ÁöÑÂÖÉÁ¥†Ôºâ")
                    continue  # Ë∑≥ËøáÂêéÁª≠maskÂ§ÑÁêÜ

                # ÂéümaskÂ§ÑÁêÜÈÄªËæë‰øùÊåÅ‰∏çÂèò
                if len(mask_positions) > 0:
                    print(f"\nÊ†∑Êú¨ {sample_id} Ê£ÄÊµãÂà∞{len(mask_positions)}‰∏™mask‰ΩçÁÇπ:")
                    
                    pred_h1 = output[0][idx, 1:, 1]
                    pred_h2 = output[1][idx, 1:, 1]
                    
                    for pos in mask_positions[:5]:
                        if pos >= len(pred_h1):
                            continue
                        
                        start = max(0, pos-5)
                        end = min(len(h1), pos+6)
                        
                        if h1[pos] == 4:
                            print(f"[HAP1] ‰ΩçÁÇπ {pos+start_pos}:")
                            print(f"‰∏ä‰∏ãÊñá: {' '.join(map(str, h1[start:end]))}")
                            print(f"È¢ÑÊµãÊ¶ÇÁéá: {pred_h1[pos]:.2f}")
                            print(f"RAGÂèÇËÄÉ: {' '.join(map(str, data['rag_seg_h1'][idx][0].cpu().numpy().flatten()[start:end]))}")
                        
                        if h2[pos] == 4:
                            print(f"[HAP2] ‰ΩçÁÇπ {pos+start_pos}:")
                            print(f"‰∏ä‰∏ãÊñá: {' '.join(map(str, h2[start:end]))}")
                            print(f"È¢ÑÊµãÊ¶ÇÁéá: {pred_h2[pos]:.2f}") 
                            print(f"RAGÂèÇËÄÉ: {' '.join(map(str, data['rag_seg_h2'][idx][0].cpu().numpy().flatten()[start:end]))}")

            # Â≠òÂÇ®È¢ÑÊµãÁªìÊûúÔºà‰øùÊåÅ‰∏çÂèòÔºâ
            sample_idx = data['sample_idx'].numpy()
            start_idx = data['start_idx'].numpy()
            end_idx = data['end_idx'].numpy()

            for idx in range(sample_idx.shape[0]):
                sample_ = sample_idx[idx][0]
                start_ = start_idx[idx][0]
                end_ = end_idx[idx][0]
                len_ = end_ - start_ + 1

                start_ = max(0, start_)
                end_ = min(hap1_prob_mat.shape[0]-1, end_)
                valid_len = end_ - start_ + 1
                
                hap1_prob_mat[start_:end_+1, sample_] = output[0][idx, 1:valid_len+1, 1][:valid_len]
                hap2_prob_mat[start_:end_+1, sample_] = output[1][idx, 1:valid_len+1, 1][:valid_len]
                gt_prob_mat[start_:end_+1, sample_, :] = output[2][idx, 1:valid_len+1, :][:valid_len]

        self.hap1_prob_mat = hap1_prob_mat
        self.hap2_prob_mat = hap2_prob_mat
        self.gt_prob_mat = gt_prob_mat

        return VCFProcessingModule.process_gt_prob_mat_with_progress(gt_prob_mat)
    """



    def infer(self):
        """Loop over the dataloader for infering.
        """
        self.model.eval()
        mode_code = "INFER"

        # Set tqdm bar
        data_iter = tqdm.tqdm(enumerate(self.infer_data),
                              desc="EP_%s" % (mode_code),
                              total=len(self.infer_data),
                              bar_format="{l_bar}{r_bar}")

        # Data in GPU.
        data_in_gpu = ['hap_1', 'hap_2', 'pos', 'af', 'af_p', 'ref', 'het', 'hom', 'rag_seg_h1', 'rag_seg_h2']


        for i, data in data_iter:
            # infer.
            gpu_data = {key: data[key].to(self.device) for key in data_in_gpu}
            with torch.no_grad():
                output  = self.model.forward(gpu_data)[:3]

            for idx, tensor in enumerate(output):
                output[idx] = tensor.cpu().numpy()
            
            sample_idx = data['sample_idx'].numpy()
            start_idx = data['start_idx'].numpy()
            end_idx = data['end_idx'].numpy()

            for idx in range(sample_idx.shape[0]):      # Batch size
                sample_ = sample_idx[idx][0]
                start_ = start_idx[idx][0]
                end_ = end_idx[idx][0]
                
                len_ = end_ - start_ + 1

                self.hap1_prob_mat[start_:end_, sample_] = output[0][idx, 1:len_, 1]
                self.hap2_prob_mat[start_:end_, sample_] = output[1][idx, 1:len_, 1]
                self.gt_prob_mat[start_:end_, sample_, :] = output[2][idx, 1:len_, :]
                #print("\n===== gt_prob_mat È™åËØÅ =====")

        vcf_data = VCFProcessingModule.process_gt_prob_mat_with_progress(self.gt_prob_mat)
        self.save_npy_result()
        return vcf_data

    def infer(self):
        """ÂéüÂßãÂÖ®ÈáèÊé®ÁêÜÊñπÊ≥ïÔºà‰øùÊåÅÂÆåÂÖ®‰∏ÄËá¥Ôºâ"""
        # ÈÄöËøáÊ†∏ÂøÉÈÄªËæëËé∑ÂèñÁªìÊûú
        vcf_data = self._core_infer_logic(self.infer_data)
    
        # ‰ª•‰∏ã‰øùÊåÅÂéüÂßãÂêéÂ§ÑÁêÜÈÄªËæë
        self.save_npy_result()
        return vcf_data

    def progressive_infer(self, 
                     step_ratio: float = 0.1,
                     max_iter: int = 100) -> np.ndarray:
        """Ê∏êËøõÂºèÊé®ÁêÜÂÖ•Âè£"""
        # ÂàùÂßãÂåñÊéßÂà∂Âô®
        iteration = 1
        controller = ProgressiveInferController(
            orig_pos=self.infer_data.dataset.ori_pos.copy(),
            initial_pos=self.infer_data.dataset.pos.copy(),
            initial_vcf=self.infer_data.dataset.vcf.copy(),
            step_ratio=step_ratio
        )
        print(f"üöÄ ÂºÄÂßãÊ∏êËøõÂºèÊé®ÁêÜ | ÊÄª‰ΩçÁÇπ: {len(controller.orig_pos)} | ÂàùÂßãÂ∑≤Áü•: {len(controller.current_pos)}")
        # Ê∏êËøõÂºèÂæ™ÁéØ
        while not controller.is_complete and max_iter > 0:
            # ÊâìÂç∞ÂΩìÂâçÁä∂ÊÄÅ
            remaining = len(controller.orig_pos) - len(controller.current_pos)
            print(f"üîÑ Ëø≠‰ª£ {iteration} | Ââ©‰Ωô‰ΩçÁÇπ: {remaining} | ÊúÄÂ§ßÂâ©‰ΩôËø≠‰ª£: {max_iter}")

            # Âä®ÊÄÅÊûÑÂª∫Êï∞ÊçÆÈõÜ
            new_dataset = RAGInferDataset(
                vocab=self.vocab,
                vcf=controller.vcf_data,
                pos=controller.current_pos,
                panel=self.infer_data.dataset.panel,
                freq=self.infer_data.dataset.freq,
                window=self.infer_data.dataset.window,
                type_to_idx=self.infer_data.dataset.type_to_idx,
                pop_to_idx=self.infer_data.dataset.pop_to_idx,
                pos_to_idx=self.infer_data.dataset.pos_to_idx,
                ref_vcf_path=self.infer_data.dataset.ref_vcf_path,
                build_index=True
            )
            new_dataloader = DataLoader(new_dataset,
                                    batch_size=self.infer_data.batch_size,
                                    num_workers=self.infer_data.num_workers,
                                    collate_fn=lambda batch_list: rag_collate_fn_with_dataset(batch_list, new_dataset, 5))
        
            # ÊâßË°åÊé®ÁêÜ
            current_vcf = self._core_infer_logic(new_dataloader)
        
            # Êõ¥Êñ∞Áä∂ÊÄÅ
            new_pos = controller.get_next_positions()
            controller.vcf_data = current_vcf
            controller.update_state(new_pos)
            # ÊâìÂç∞ËøõÂ∫¶
            coverage = len(controller.current_pos) / len(controller.orig_pos) * 100
            print(f"‚úÖ ÂΩìÂâçË¶ÜÁõñ: {coverage:.2f}% | Á¥ØËÆ°Â∑≤Áü•‰ΩçÁÇπ: {len(controller.current_pos)}\n")
            
            iteration += 1
            max_iter -= 1
        self.save_npy_result()
        print(f"üéâ Êé®ÁêÜÂÆåÊàêÔºÅÊúÄÁªàË¶ÜÁõñ: {len(controller.current_pos)}/{len(controller.orig_pos)} ‰∏™‰ΩçÁÇπ")
        return controller.vcf_data

    def save_npy_result(self) -> None:
        """Call this func to save results from self.infer().
        """
        np.save(self.output_path + "/HAP1.npy", self.hap1_prob_mat)
        print("HAP1 saved.")

        np.save(self.output_path + "/HAP2.npy", self.hap2_prob_mat)
        print("HAP2 saved.")

        np.save(self.output_path + "/GT.npy", self.gt_prob_mat)
        print("GT saved.")

        np.save(self.output_path + "/POS.npy", self.infer_data.dataset.ori_pos)
        print("POS saved.")

        np.save(self.output_path + "/POS_Flag.npy", self.infer_data.dataset.position_needed)
        print("POS_FLAG saved.")
    
