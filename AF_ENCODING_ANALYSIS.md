# AFç¼–ç ç­–ç•¥æ·±åº¦åˆ†æ

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**å½“å‰é—®é¢˜**: AF (1ç»´) vs Embedding (192ç»´) â†’ AFä¿¡æ¯è¢«ä¸¥é‡ç¨€é‡Š

**ä½ çš„å»ºè®®**: æŠŠAFåŠ å…¥åˆ°Embeddingä¸­

**æˆ‘çš„åˆ†æ**: è¿™æ˜¯æ­£ç¡®çš„æ–¹å‘ï¼Œä½†éœ€è¦ä»”ç»†è®¾è®¡

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ1: å½“å‰æ–¹å¼ (Late Fusion)

```python
emb = embedding_layer(tokens)  # [B, L, 192]
af_feat = af.unsqueeze(-1)      # [B, L, 1]
fused = concat([emb, af_feat])  # [B, L, 193]
output = Linear(193 â†’ 192)
```

**ä¼˜ç‚¹**:
- ç®€å•
- Embeddingå’ŒAFç‹¬ç«‹å­¦ä¹ 

**ç¼ºç‚¹**:
- âŒ AFåªå 0.5%ç»´åº¦
- âŒ Linearå±‚éš¾ä»¥æ•æ‰AFçš„éçº¿æ€§å½±å“
- âŒ AFä¿¡æ¯å®¹æ˜“è¢«æ·¹æ²¡

---

### æ–¹æ¡ˆ2: AF Encoding + Concat (Improved Late Fusion)

```python
emb = embedding_layer(tokens)   # [B, L, 192]
af_encoded = af_encoder(af)     # [B, L, 192]  â† ç¼–ç åˆ°åŒç­‰ç»´åº¦!
fused = concat([emb, af_encoded])  # [B, L, 384]
output = Linear(384 â†’ 192)
```

**ä¼˜ç‚¹**:
- âœ… AFå 50%ç»´åº¦ï¼Œä¸è¢«ç¨€é‡Š
- âœ… AFå¯ä»¥æœ‰å¤æ‚çš„éçº¿æ€§ç¼–ç 
- âœ… ä¿æŒæ¨¡å—ç‹¬ç«‹æ€§

**ç¼ºç‚¹**:
- å‚æ•°é‡å¢åŠ 
- ä»ç„¶æ˜¯late fusion

---

### æ–¹æ¡ˆ3: AF-Conditioned Embedding (Early Fusion) â­

```python
# Embeddingå±‚åœ¨ç”Ÿæˆæ—¶å°±è€ƒè™‘AF
token_emb = token_embedding(tokens)  # [B, L, 192]
af_emb = af_embedding(af)            # [B, L, 192]
emb = token_emb + af_emb             # [B, L, 192]  â† ç›´æ¥ç›¸åŠ !
```

**ä¼˜ç‚¹**:
- âœ… AFåœ¨æœ€æ—©é˜¶æ®µèå…¥
- âœ… ä¸å¢åŠ æœ€ç»ˆç»´åº¦
- âœ… AFä¿¡æ¯è´¯ç©¿æ•´ä¸ªæ¨¡å‹
- âœ… ç±»ä¼¼BERTçš„positional embedding

**ç¼ºç‚¹**:
- éœ€è¦è®¾è®¡AF embeddingæ–¹å¼

---

### æ–¹æ¡ˆ4: AFä½œä¸ºContinuous Embedding (æ¨è!) â­â­

```python
class AFEmbedding(nn.Module):
    """å°†è¿ç»­çš„AFå€¼ç¼–ç ä¸ºé«˜ç»´å‘é‡"""
    def __init__(self, embed_size=192, num_basis=32):
        super().__init__()
        # ä½¿ç”¨å¯å­¦ä¹ çš„basis functions
        self.basis_freqs = nn.Parameter(torch.randn(num_basis))
        self.basis_weights = nn.Linear(num_basis * 2, embed_size)  # sin + cos

    def forward(self, af):
        # af: [B, L] - è¿ç»­å€¼ 0-1
        # ä½¿ç”¨Fourier features (ç±»ä¼¼NeRF)
        af_expanded = af.unsqueeze(-1) * self.basis_freqs  # [B, L, num_basis]
        af_sin = torch.sin(2 * Ï€ * af_expanded)
        af_cos = torch.cos(2 * Ï€ * af_expanded)
        af_features = torch.cat([af_sin, af_cos], dim=-1)  # [B, L, 2*num_basis]

        return self.basis_weights(af_features)  # [B, L, embed_size]

# ä½¿ç”¨
token_emb = token_embedding(tokens)  # [B, L, 192]
af_emb = af_embedding(af)            # [B, L, 192]
final_emb = token_emb + af_emb       # [B, L, 192]
```

**ä¼˜ç‚¹**:
- âœ… AFè¢«ç¼–ç ä¸ºä¸token embeddingç­‰æƒçš„å‘é‡
- âœ… Fourier featuresèƒ½æ•æ‰AFçš„å‘¨æœŸæ€§å’Œéçº¿æ€§æ¨¡å¼
- âœ… å¯å­¦ä¹ basisè®©æ¨¡å‹è‡ªé€‚åº”å­¦ä¹ AFçš„é‡è¦æ¨¡å¼
- âœ… ä¸å¢åŠ ç»´åº¦
- âœ… æ•°å­¦ä¸Šä¼˜é›… (ç±»ä¼¼NeRFçš„position encoding)

**åŸç†**:
```
AF=0.02 â†’ [sin(2Ï€fâ‚*0.02), cos(2Ï€fâ‚*0.02), ..., sin(2Ï€fâ‚ƒâ‚‚*0.02), cos(2Ï€fâ‚ƒâ‚‚*0.02)]
       â†’ Linear(64 â†’ 192)
       â†’ 192ç»´embeddingå‘é‡

ä¸åŒAFå€¼ä¼šäº§ç”Ÿå®Œå…¨ä¸åŒçš„embedding pattern
```

---

### æ–¹æ¡ˆ5: Hybrid Approach (æœ€å…¨é¢) â­â­â­

```python
class HybridAFIntegration(nn.Module):
    def __init__(self, embed_size=192):
        super().__init__()
        # 1. AF Embedding (early fusion)
        self.af_embedding = AFEmbedding(embed_size)

        # 2. AF Conditioning (modulation)
        self.af_scale = nn.Linear(1, embed_size)
        self.af_shift = nn.Linear(1, embed_size)

    def forward(self, token_emb, af):
        # Early fusion: AF embedding
        af_emb = self.af_embedding(af)  # [B, L, D]
        emb = token_emb + af_emb

        # Modulation: AF-based scale and shift
        scale = torch.sigmoid(self.af_scale(af.unsqueeze(-1)))  # [B, L, D]
        shift = self.af_shift(af.unsqueeze(-1))

        return emb * scale + shift
```

**ä¼˜ç‚¹**:
- âœ… ç»“åˆäº†additiveå’Œmultiplicativeä¸¤ç§æ–¹å¼
- âœ… æœ€å¤§é™åº¦åˆ©ç”¨AFä¿¡æ¯
- âœ… çµæ´»æ€§æœ€é«˜

**ç¼ºç‚¹**:
- å‚æ•°é‡ç•¥å¤š
- ç¨å¾®å¤æ‚

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### ä¸»æ¨è: æ–¹æ¡ˆ4 (AF Continuous Embedding)

**ç†ç”±**:
1. ç±»ä¼¼BERTçš„positional embeddingï¼Œç†è®ºæˆç†Ÿ
2. Fourier featuresæ•°å­¦ä¼˜é›…ï¼Œè¡¨è¾¾èƒ½åŠ›å¼º
3. ä¸å¢åŠ ç»´åº¦ï¼Œä¸å½±å“åç»­æ¶æ„
4. ä¸token embeddingåœ°ä½å¹³ç­‰

### å¤‡é€‰: æ–¹æ¡ˆ5 (Hybrid)

å¦‚æœéœ€è¦æœ€å¤§åŒ–AFçš„å½±å“åŠ›

---

## ğŸ“ å…·ä½“å®ç°

### å®Œæ•´çš„AFEmbeddingå®ç°

```python
import torch
import torch.nn as nn
import math

class AFEmbedding(nn.Module):
    """
    Allele Frequency Embedding using Fourier Features

    å°†è¿ç»­çš„AFå€¼ (0-1) ç¼–ç ä¸ºé«˜ç»´å‘é‡

    ç±»ä¼¼äº:
    - BERTçš„PositionalEmbedding (ä½†AFæ˜¯æ•°æ®é©±åŠ¨çš„)
    - NeRFçš„Positional Encoding (ä½†è¿™é‡Œæ˜¯å¯å­¦ä¹ çš„)
    """
    def __init__(self, embed_size=192, num_basis=32, learnable_basis=True):
        super().__init__()
        self.embed_size = embed_size
        self.num_basis = num_basis

        if learnable_basis:
            # å¯å­¦ä¹ çš„basis frequencies
            self.basis_freqs = nn.Parameter(
                torch.randn(num_basis) * 10.0  # åˆå§‹åŒ–ä¸ºè¾ƒå¤§èŒƒå›´
            )
        else:
            # å›ºå®šçš„basis (ç±»ä¼¼NeRF)
            freqs = 2.0 ** torch.arange(num_basis, dtype=torch.float32)
            self.register_buffer('basis_freqs', freqs)

        # å°†Fourier featuresæŠ•å½±åˆ°embed_size
        self.projection = nn.Sequential(
            nn.Linear(num_basis * 2, embed_size),  # sin + cos
            nn.LayerNorm(embed_size),
            nn.GELU(),
            nn.Linear(embed_size, embed_size)
        )

        self._init_weights()

    def _init_weights(self):
        for module in self.projection:
            if isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)
                nn.init.constant_(module.bias, 0.0)

    def forward(self, af):
        """
        Args:
            af: [B, L] - Allele frequency (0-1)

        Returns:
            [B, L, embed_size] - AF embedding
        """
        # Fourier features
        af_expanded = af.unsqueeze(-1) * self.basis_freqs  # [B, L, num_basis]
        af_sin = torch.sin(2 * math.pi * af_expanded)
        af_cos = torch.cos(2 * math.pi * af_expanded)

        # Concat sin and cos
        af_features = torch.cat([af_sin, af_cos], dim=-1)  # [B, L, 2*num_basis]

        # Project to embed_size
        af_emb = self.projection(af_features)  # [B, L, embed_size]

        return af_emb

    def visualize_encoding(self, af_values):
        """å¯è§†åŒ–ä¸åŒAFå€¼çš„ç¼–ç """
        with torch.no_grad():
            af_tensor = torch.tensor(af_values).unsqueeze(0)  # [1, L]
            embeddings = self.forward(af_tensor)  # [1, L, D]
            return embeddings.squeeze(0).numpy()  # [L, D]
```

### ä¿®æ”¹BERTEmbedding

```python
class BERTEmbedding(nn.Module):
    """
    BERT Embedding with AF integration

    ç»„æˆ:
        1. Token Embedding
        2. Position Embedding
        3. AF Embedding (æ–°å¢!)
    """
    def __init__(self, vocab_size, embed_size, dropout=0.1):
        super().__init__()

        # Token embedding
        self.tokenizer = nn.Embedding(vocab_size, embed_size, padding_idx=0)

        # Position embedding
        self.position = PositionalEmbedding(embed_size)

        # AF embedding (æ–°å¢!)
        self.af_embedding = AFEmbedding(embed_size)

        self.embed_size = embed_size
        self.dropout = nn.Dropout(dropout)

    def forward(self, seq, af=None, use_pos=True):
        """
        Args:
            seq: [B, L] - Token sequences
            af: [B, L] - Allele frequencies (å¯é€‰)
            use_pos: bool - æ˜¯å¦ä½¿ç”¨position embedding

        Returns:
            [B, L, D] - Final embeddings
        """
        # Token embedding
        token_emb = self.tokenizer(seq)  # [B, L, D]

        # Position embedding
        if use_pos:
            token_emb = token_emb + self.position(seq)

        # AF embedding (å¦‚æœæä¾›)
        if af is not None:
            af_emb = self.af_embedding(af)  # [B, L, D]
            token_emb = token_emb + af_emb  # â† å…³é”®: åŠ æ€§èåˆ!

        return self.dropout(token_emb)
```

---

## ğŸ” ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡æœ‰æ•ˆ

### 1. æ•°å­¦åŸç†

**Fourier Featuresçš„è¡¨è¾¾èƒ½åŠ›**:

```python
f(af) = Î£[w_i * sin(2Ï€ * freq_i * af) + b_i * cos(2Ï€ * freq_i * af)]
```

- å¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•° (Universal approximation)
- ä¸åŒé¢‘ç‡æ•æ‰ä¸åŒå°ºåº¦çš„æ¨¡å¼:
  - ä½é¢‘: æ•æ‰common (0.3-0.5) vs rare (0-0.05) çš„å¤§è¶‹åŠ¿
  - é«˜é¢‘: æ•æ‰fine-grainedå·®å¼‚ (0.01 vs 0.02)

### 2. ä¸å…¶ä»–embeddingå¯¹é½

```python
Token embedding:   learned vector [D]
Position embedding: learned vector [D]
AF embedding:      learned vector [D]  â† åœ°ä½å¹³ç­‰!

Final: token + position + af = [D]
```

æ‰€æœ‰ä¿¡æ¯éƒ½åœ¨åŒä¸€ä¸ªç»´åº¦ç©ºé—´ï¼Œæ²¡æœ‰ç¨€é‡Šï¼

### 3. å¯è§£é‡Šæ€§

```python
AF = 0.02 (rare):
  â†’ sin(2Ï€*f*0.02) for various f
  â†’ äº§ç”Ÿä¸€ä¸ª"rare pattern"çš„embedding

AF = 0.45 (common):
  â†’ sin(2Ï€*f*0.45) for various f
  â†’ äº§ç”Ÿä¸€ä¸ª"common pattern"çš„embedding

æ¨¡å‹å¯ä»¥å­¦ä¹ :
  "rare pattern" â†’ ç‰¹æ®Šå¤„ç†
  "common pattern" â†’ å¸¸è§„å¤„ç†
```

---

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | AFç»´åº¦å æ¯” | ä¿¡æ¯ä¿ç•™ | å¤æ‚åº¦ | æ¨èåº¦ |
|------|-----------|---------|--------|--------|
| å½“å‰ (Late Fusion) | 0.5% | âŒ ä½ | ä½ | âŒ |
| Improved Late Fusion | 50% | âš ï¸ ä¸­ | ä¸­ | âš ï¸ |
| Early Fusion (Simple) | 100% | âœ… é«˜ | ä½ | âœ… |
| **Continuous Embedding** | **100%** | **âœ… é«˜** | **ä¸­** | **â­â­â­** |
| Hybrid | 100%+ | âœ… æœ€é«˜ | é«˜ | âœ…âœ… |

---

## ğŸ¯ æœ€ç»ˆæ¨è

### é‡‡ç”¨æ–¹æ¡ˆ4: AF Continuous Embedding

**å®æ–½æ­¥éª¤**:
1. å®ç°`AFEmbedding` class
2. ä¿®æ”¹`BERTEmbedding`åŠ å…¥AF
3. åœ¨æ‰€æœ‰ä½¿ç”¨embeddingçš„åœ°æ–¹ä¼ å…¥AF
4. é¢„ç¼–ç æ—¶ä¹Ÿä½¿ç”¨AF

**ä¼˜ç‚¹**:
- âœ… AFä¿¡æ¯ä¸è¢«ç¨€é‡Š
- âœ… ç†è®ºæˆç†Ÿ (ç±»ä¼¼positional encoding)
- âœ… å®ç°ç›¸å¯¹ç®€å•
- âœ… ä¸ç ´åç°æœ‰æ¶æ„

**é¢„æœŸæ•ˆæœ**:
- AFçš„å½±å“åŠ›æå‡100-200å€ (ä»0.5%åˆ°100%)
- Rare variantè¯†åˆ«èƒ½åŠ›æ˜¾è‘—æå‡
- æ•´ä½“F1é¢„æœŸæå‡2-5%

---

## ğŸ“ ä¸ç°æœ‰æ¶æ„çš„æ•´åˆ

### ç§»é™¤EmbeddingFusionModule?

**å»ºè®®**: ä¿ç•™ï¼Œä½†ç®€åŒ–

```python
class SimplifiedEmbeddingFusionModule(nn.Module):
    """
    ç°åœ¨åªéœ€è¦å¤„ç†POS
    AFå·²ç»åœ¨Embeddingå±‚å¤„ç†äº†
    """
    def __init__(self, emb_size):
        super().__init__()
        self.pos_feat = PositionFeatModule()
        self.fusion = nn.Linear(emb_size + 1, emb_size)
        self.norm = nn.LayerNorm(emb_size)

    def forward(self, emb, pos):
        # embå·²ç»åŒ…å«äº†AFä¿¡æ¯!
        pos_feat = self.pos_feat(pos).unsqueeze(-1)
        fused = self.fusion(torch.cat([emb, pos_feat], dim=-1))
        return self.norm(emb + fused)
```

æˆ–è€…å®Œå…¨ç§»é™¤ï¼Œåªåœ¨Embeddingå±‚åšï¼š
```python
emb = BERTEmbedding(tokens, af=af, use_pos=True)  # ä¸€æ­¥åˆ°ä½!
```

---

**åˆ›å»ºæ—¶é—´**: 2025-12-02
**æ¨èæ–¹æ¡ˆ**: AF Continuous Embedding (Fourier Features)
**é¢„æœŸæå‡**: F1 +2-5%, Rare F1 +5-10%
