# Embedding RAG å®Œæ•´è§£æ

## ğŸ¯ ä½ çš„æ ¸å¿ƒé—®é¢˜

### Q1: Referencesç¼–ç æ˜¯ä»€ä¹ˆå±‚åšçš„ï¼Ÿ
**ç­”æ¡ˆ**: `BERTEmbedding` å±‚ (Line 45-47 in bert.py)

```python
self.embedding = BERTEmbedding(vocab_size=vocab_size,
                               embed_size=dims,
                               dropout=dropout)
```

**BERTEmbeddingåŒ…å«**:
- `tokenizer`: nn.Embedding(vocab_size, embed_size) - **å¯å­¦ä¹ çš„å‚æ•°**
- `position`: PositionalEmbedding - **å¯å­¦ä¹ çš„ä½ç½®ç¼–ç **

### Q2: è¿™ä¸ªå±‚éœ€è¦è®­ç»ƒå—ï¼Ÿ
**ç­”æ¡ˆ**: **éœ€è¦ï¼** è¿™æ˜¯å…³é”®ä¼˜åŠ¿ï¼

**ä¸¤ç§ç­–ç•¥**:

**ç­–ç•¥A: å›ºå®šé¢„ç¼–ç ** (Phase 1, ç®€å•)
```python
# åˆå§‹åŒ–æ—¶,ç”¨å½“å‰embeddingç¼–ç ä¸€æ¬¡
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    ref_embeddings = embedding_layer(ref_sequences)

# è®­ç»ƒæ—¶embeddingå±‚ä¼šæ›´æ–°
# ä½†ref_embeddingsä¿æŒå›ºå®š (ä¸ä¼šè‡ªåŠ¨æ›´æ–°)
```

**ç­–ç•¥B: å®šæœŸåˆ·æ–°** (Phase 2, æœ€ä¼˜)
```python
# æ¯ä¸ªepochç»“æŸå
def refresh_reference_embeddings():
    with torch.no_grad():
        # ç”¨æ›´æ–°åçš„embeddingé‡æ–°ç¼–ç 
        ref_embeddings = current_embedding_layer(ref_sequences)
        # æ›´æ–°FAISS index
```

---

## ğŸ“Š å½“å‰æ¶æ„ vs Embedding RAG å¯¹æ¯”

### å½“å‰æ¶æ„ (æœ‰RAG)

```
æ¯ä¸ªTraining Batch:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Query Sequences                                 â”‚
â”‚   hap_1, hap_2 [B, L] (raw tokens)                     â”‚
â”‚   â†“                                                     â”‚
â”‚   embedding()        â†’ [B, L, D]   â† è¿‡ä¸€æ¬¡            â”‚
â”‚   â†“                                                     â”‚
â”‚   10 x Transformer   â†’ [B, L, D]   â† è¿‡10å±‚            â”‚
â”‚                                                         â”‚
â”‚ Memory: 9 GB (batch=16)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: RAG Retrieved Sequences (é—®é¢˜æ‰€åœ¨!)              â”‚
â”‚   rag_h1, rag_h2 [B, L] (raw tokens from FAISS)        â”‚
â”‚   â†“                                                     â”‚
â”‚   embedding()        â†’ [B, L, D]   â† åˆè¿‡ä¸€æ¬¡!          â”‚
â”‚   â†“                                                     â”‚
â”‚   10 x Transformer   â†’ [B, L, D]   â† åˆè¿‡10å±‚!          â”‚
â”‚                                                         â”‚
â”‚ Memory: 9 GB (é‡å¤!)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Fusion                                          â”‚
â”‚   EnhancedRareVariantFusion(query_emb, rag_emb)        â”‚
â”‚   â†“                                                     â”‚
â”‚   Classifiers                                           â”‚
â”‚                                                         â”‚
â”‚ Memory: 1 GB                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Memory: 9 + 9 + 1 = 19 GB per batch
Total Time: 100ms (query) + 100ms (RAG) + 10ms (fusion) = 210ms
```

### Embedding RAGæ¶æ„

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
åˆå§‹åŒ–æ—¶ (ä¸€æ¬¡æ€§, ~10 minutes):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pre-encode ALL Reference Sequences                      â”‚
â”‚                                                         â”‚
â”‚ For each window (e.g., 150 windows):                   â”‚
â”‚   ref_sequences [num_haps, L]  (e.g., 1000 haplotypes)â”‚
â”‚   â†“                                                     â”‚
â”‚   embedding()  â†’ [num_haps, L, D]  â† åªç¼–ç ä¸€æ¬¡!        â”‚
â”‚   â†“                                                     â”‚
â”‚   Flatten      â†’ [num_haps, L*D]                       â”‚
â”‚   â†“                                                     â”‚
â”‚   Build FAISS index on L*D dimensional space           â”‚
â”‚   â†“                                                     â”‚
â”‚   Store: ref_embeddings[window_idx] = embeddings.cpu()â”‚
â”‚                                                         â”‚
â”‚ Storage: ~500 MB in CPU RAM (not GPU!)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ¯ä¸ªTraining Batch:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Encode Query (ONLY embedding layer)            â”‚
â”‚   hap_1, hap_2 [B, L] (raw tokens)                     â”‚
â”‚   â†“                                                     â”‚
â”‚   embedding()   â†’ [B, L, D]    â† åªè¿‡embeddingå±‚!       â”‚
â”‚   (ä¸è¿‡Transformer!)                                    â”‚
â”‚                                                         â”‚
â”‚ Memory: 0.5 GB                                          â”‚
â”‚ Time: 10 ms                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Retrieve Pre-encoded Embeddings                â”‚
â”‚   query_emb [B, L, D]                                   â”‚
â”‚   â†“                                                     â”‚
â”‚   Flatten â†’ [B, L*D]                                    â”‚
â”‚   â†“                                                     â”‚
â”‚   FAISS.search(query_emb_flat) â†’ retrieve indices      â”‚
â”‚   â†“                                                     â”‚
â”‚   rag_emb = ref_embeddings[indices]  â† ç›´æ¥å–!         â”‚
â”‚   (å·²ç»æ˜¯embedding,æ— éœ€è¿‡BERT!)                          â”‚
â”‚                                                         â”‚
â”‚ Memory: 0.5 GB (åªå­˜embedding)                          â”‚
â”‚ Time: 5 ms (FAISSæå¿«)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Fusion + Transformer                           â”‚
â”‚   query_emb_fused = Fusion(query_emb, rag_emb)         â”‚
â”‚   â†“                                                     â”‚
â”‚   10 x Transformer â†’ [B, L, D]  â† åªè¿‡ä¸€æ¬¡!             â”‚
â”‚   â†“                                                     â”‚
â”‚   Classifiers                                           â”‚
â”‚                                                         â”‚
â”‚ Memory: 9 GB (åªæœ‰fused embeddingsè¿‡Transformer)        â”‚
â”‚ Time: 100 ms                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Memory: 0.5 + 0.5 + 9 = 10 GB per batch (vs 19 GB)
Total Time: 10 + 5 + 100 = 115 ms (vs 210 ms)
Speedup: 1.8x faster
Batch size: 16 â†’ 48 (3x larger)
```

---

## ğŸ”¬ å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. é¢„ç¼–ç è¿‡ç¨‹ (åˆå§‹åŒ–)

```python
class EmbeddingRAGDataset(RAGTrainDataset):
    def _build_embedding_index(self, ref_vcf_path, embedding_layer):
        """
        é¢„ç¼–ç æ‰€æœ‰reference sequences

        Args:
            ref_vcf_path: å‚è€ƒé¢æ¿VCFè·¯å¾„
            embedding_layer: å½“å‰æ¨¡å‹çš„embeddingå±‚ (å¯å­¦ä¹ å‚æ•°)
        """
        print("â–£ Building Embedding-based RAG Index")

        # 1. åŠ è½½reference data (å’Œä¹‹å‰ä¸€æ ·)
        ref_gt, ref_pos = self._load_ref_data(ref_vcf_path)
        # ref_gt: [num_variants, num_samples*2] - æ‰€æœ‰referenceçš„åŸºå› å‹

        # 2. å¯¹æ¯ä¸ªwindowé¢„ç¼–ç 
        self.ref_embeddings = {}  # å­˜å‚¨é¢„ç¼–ç ç»“æœ
        self.embedding_indexes = []  # FAISSç´¢å¼•

        with torch.no_grad():  # é‡è¦: ä¸è®¡ç®—æ¢¯åº¦
            for w_idx in tqdm(range(self.window_count)):
                # 2.1 è·å–è¯¥windowçš„reference sequences
                window_slice = slice(
                    self.window.window_info[w_idx, 0],
                    self.window.window_info[w_idx, 1]
                )
                window_refs = ref_gt[window_slice]  # [L, num_samples*2]
                window_pos = ref_pos[window_slice]   # [L]

                # 2.2 è½¬ç½®: [num_haplotypes, L]
                num_haps = window_refs.shape[1]
                ref_haps = window_refs.T  # [num_haps, L]

                # 2.3 ç¼–ç ä¸ºembeddings
                # æ³¨æ„: åªè¿‡embeddingå±‚,ä¸è¿‡transformer!
                ref_tokens = torch.LongTensor(ref_haps).to(device)
                ref_emb = embedding_layer(ref_tokens)  # [num_haps, L, D]

                # 2.4 Flatten: [num_haps, L*D]
                ref_emb_flat = ref_emb.reshape(num_haps, -1).cpu().numpy()

                # 2.5 æ„å»ºFAISS index (åœ¨L*Dç»´ç©ºé—´)
                index = faiss.IndexFlatL2(ref_emb_flat.shape[1])
                index.add(ref_emb_flat)
                self.embedding_indexes.append(index)

                # 2.6 å­˜å‚¨embeddings (åœ¨CPU,èŠ‚çœGPUå†…å­˜)
                self.ref_embeddings[w_idx] = ref_emb.cpu()

        print(f"âœ“ Pre-encoded {self.window_count} windows")
        print(f"  Storage: {self._calculate_storage_size()} MB in CPU RAM")
```

**æ•°æ®å¯¹é½ä¿è¯**:
- âœ… æ¯ä¸ªwindowçš„ref_embeddingså’ŒFAISS indexä¸€ä¸€å¯¹åº”
- âœ… FAISSè¿”å›çš„indexå¯ä»¥ç›´æ¥ç”¨äºå–ref_embeddings
- âœ… Windowè¾¹ç•Œå’ŒåŸæ•°æ®å®Œå…¨ä¸€è‡´

### 2. è®­ç»ƒæ—¶æ£€ç´¢ (collate_fn)

```python
def embedding_rag_collate_fn(batch_list, dataset, embedding_layer):
    """
    æ–°çš„collateå‡½æ•°: åœ¨embedding spaceæ£€ç´¢
    """
    # 1. æ ‡å‡†collate
    batch = default_collate(batch_list)
    B = len(batch_list)

    # 2. åªè¿‡embeddingå±‚ç¼–ç query
    with torch.no_grad():  # è¿™é‡Œä¸éœ€è¦æ¢¯åº¦ (æ£€ç´¢æ“ä½œ)
        query_h1_emb = embedding_layer(batch['hap_1'])  # [B, L, D]
        query_h2_emb = embedding_layer(batch['hap_2'])

    # 3. å¯¹æ¯ä¸ªæ ·æœ¬åœ¨å…¶windowæ£€ç´¢
    retrieved_h1_embs = []
    retrieved_h2_embs = []

    for i in range(B):
        window_idx = batch['window_idx'][i]

        # 3.1 Flatten query embedding
        query_h1_flat = query_h1_emb[i].reshape(-1).cpu().numpy()  # [L*D]
        query_h2_flat = query_h2_emb[i].reshape(-1).cpu().numpy()

        # 3.2 FAISSæ£€ç´¢ (åœ¨embedding space)
        D1, I1 = dataset.embedding_indexes[window_idx].search(
            query_h1_flat.reshape(1, -1), k=1
        )
        D2, I2 = dataset.embedding_indexes[window_idx].search(
            query_h2_flat.reshape(1, -1), k=1
        )

        # 3.3 è·å–pre-encoded embedding
        retrieved_idx1 = I1[0, 0]
        retrieved_idx2 = I2[0, 0]

        rag_h1_emb = dataset.ref_embeddings[window_idx][retrieved_idx1]  # [L, D]
        rag_h2_emb = dataset.ref_embeddings[window_idx][retrieved_idx2]

        retrieved_h1_embs.append(rag_h1_emb)
        retrieved_h2_embs.append(rag_h2_emb)

    # 4. Stackå¹¶æ·»åŠ åˆ°batch
    batch['rag_h1_emb'] = torch.stack(retrieved_h1_embs)  # [B, L, D]
    batch['rag_h2_emb'] = torch.stack(retrieved_h2_embs)

    return batch
```

**æ•°æ®å¯¹é½ä¿è¯**:
- âœ… query_embå’Œrag_embçš„shapeå®Œå…¨ä¸€è‡´: [B, L, D]
- âœ… éƒ½æ˜¯æ¥è‡ªåŒä¸€ä¸ªwindow,ä½ç½®ä¿¡æ¯å¯¹é½
- âœ… FAISSæ£€ç´¢ä¿è¯æœ€ç›¸ä¼¼çš„haplotype

### 3. æ¨¡å‹Forward (æ— éœ€é‡å¤è¿‡BERT)

```python
class BERTWithEmbeddingRAG(BERT):
    def forward(self, x):
        # 1. Embeddingå±‚ (ä¼šè¢«è®­ç»ƒæ›´æ–°)
        hap_1_emb = self.embedding(x['hap_1'])  # [B, L, D]
        hap_2_emb = self.embedding(x['hap_2'])

        # 2. è·å–pre-encoded RAG embeddings (æ¥è‡ªcollate_fn)
        rag_h1_emb = x.get('rag_h1_emb', None)
        rag_h2_emb = x.get('rag_h2_emb', None)

        # 3. Fusion (å¦‚æœæœ‰RAG)
        if rag_h1_emb is not None:
            # æ³¨æ„: rag_embå·²ç»æ˜¯embedding,ä¸éœ€è¦å†è¿‡BERT!
            hap_1_fused = self.rag_fusion(
                hap_1_emb,
                rag_h1_emb.to(hap_1_emb.device),
                x['af']
            )
            hap_2_fused = self.rag_fusion(
                hap_2_emb,
                rag_h2_emb.to(hap_2_emb.device),
                x['af']
            )
        else:
            hap_1_fused = hap_1_emb
            hap_2_fused = hap_2_emb

        # 4. Transformer (åªè¿‡ä¸€æ¬¡!)
        for transformer in self.transformer_blocks:
            hap_1_fused = transformer(hap_1_fused)

        for transformer in self.transformer_blocks:
            hap_2_fused = transformer(hap_2_fused)

        return hap_1_fused, hap_2_fused
```

---

## ğŸ“ Embeddingå±‚çš„å¯å­¦ä¹ æ€§

### è®­ç»ƒä¸­ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

```
Iteration 1:
  embedding.weight = W1  (åˆå§‹å‚æ•°)
  ref_embeddings = embedding(ref_seqs) using W1  (é¢„ç¼–ç )

Training:
  loss.backward()
  optimizer.step()
  embedding.weight = W2  (æ›´æ–°åçš„å‚æ•°)

  ä½†ref_embeddingsä»ç„¶æ˜¯ç”¨W1ç¼–ç çš„! (å›ºå®š)

Iteration 1000:
  embedding.weight = W1000 (å·²ç»å˜äº†å¾ˆå¤š)
  ref_embeddingsä»ç„¶æ˜¯W1 (è¿‡æ—¶äº†!)
```

### è§£å†³æ–¹æ¡ˆ: å®šæœŸåˆ·æ–°

```python
def refresh_reference_embeddings(model, dataset):
    """
    ç”¨æ›´æ–°åçš„embeddingé‡æ–°ç¼–ç references
    """
    print("Refreshing reference embeddings...")

    with torch.no_grad():
        for w_idx in range(dataset.window_count):
            # ç”¨å½“å‰çš„embeddingé‡æ–°ç¼–ç 
            ref_tokens = dataset.ref_tokens[w_idx]  # å­˜å‚¨çš„raw tokens
            ref_emb = model.embedding(ref_tokens)   # ç”¨æœ€æ–°çš„Wç¼–ç 

            # æ›´æ–°å­˜å‚¨
            dataset.ref_embeddings[w_idx] = ref_emb.cpu()

            # é‡å»ºFAISS index
            ref_emb_flat = ref_emb.reshape(-1, L*D).cpu().numpy()
            dataset.embedding_indexes[w_idx].reset()
            dataset.embedding_indexes[w_idx].add(ref_emb_flat)

    print("âœ“ Refreshed all reference embeddings")


# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in dataloader:
        # æ­£å¸¸è®­ç»ƒ
        loss.backward()
        optimizer.step()

    # æ¯ä¸ªepochç»“æŸåˆ·æ–° (æˆ–æ¯Nä¸ªepochs)
    refresh_reference_embeddings(model, train_dataset)
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨

| æŒ‡æ ‡ | å½“å‰RAG | Embedding RAG (å›ºå®š) | Embedding RAG (åˆ·æ–°) |
|------|---------|---------------------|---------------------|
| **å†…å­˜** | 19 GB | 10 GB | 10 GB |
| **é€Ÿåº¦** | 210 ms/batch | 115 ms/batch | 115 ms/batch |
| **Batch size** | 16 | 48 | 48 |
| **Referenceå¯å­¦ä¹ ** | âœ… Yes | âŒ å›ºå®š | âœ… Yes (æ¯epoch) |
| **å®æ–½éš¾åº¦** | N/A | ç®€å• | ä¸­ç­‰ |
| **è®­ç»ƒåˆå§‹åŒ–** | 0 | 10 min | 10 min |

---

## ğŸš€ å®æ–½æµç¨‹ (ä¿ç•™å½“å‰ä»£ç )

### Step 1: å¤‡ä»½å½“å‰ä»£ç 

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
cp -r src src_original_rag
cp -r run_*.sh backup_scripts/

# æˆ–åˆ›å»ºgitåˆ†æ”¯
git checkout -b embedding-rag
git commit -m "Checkpoint before Embedding RAG"
```

### Step 2: ä¿®æ”¹ä»£ç  (Phase 1 - å›ºå®šç¼–ç )

#### 2.1 ä¿®æ”¹ `rag_train_dataset.py`

æ–°å¢æ–¹æ³•:
```python
def _build_embedding_index(self, ref_vcf_path, embedding_layer):
    # ... (ä¸Šé¢çš„è¯¦ç»†ä»£ç )
```

ä¿®æ”¹ `from_file`:
```python
@classmethod
def from_file(cls, vocab, ..., use_embedding_rag=False):
    dataset = cls(...)

    if use_embedding_rag:
        # éœ€è¦ä¼ å…¥embedding layer
        # ä¸´æ—¶åˆ›å»ºä¸€ä¸ªembedding layeræˆ–ä»checkpointåŠ è½½
        dataset._build_embedding_index(ref_vcf_path, embedding_layer)
    else:
        dataset._build_faiss_indexes(ref_vcf_path)

    return dataset
```

#### 2.2 ä¿®æ”¹ `collate_fn`

æ–°å¢ `embedding_rag_collate_fn` (ä¸Šé¢çš„è¯¦ç»†ä»£ç )

#### 2.3 ä¿®æ”¹ `bert.py`

æ–°å¢ `BERTWithEmbeddingRAG` class (ä¸Šé¢çš„è¯¦ç»†ä»£ç )

#### 2.4 åˆ›å»ºæ–°çš„training script

`run_v18_embedding_rag.sh`:
```bash
--use_embedding_rag true
--dims 256  # å¯ä»¥ç”¨æ›´å¤§æ¨¡å‹äº†!
--layers 12
--train_batch_size 32  # 3å€äºV17
```

### Step 3: æµ‹è¯•éªŒè¯

```bash
# æµ‹è¯•æ•°æ®åŠ è½½
python -c "from src.dataset.rag_train_dataset import *; test_embedding_rag()"

# æµ‹è¯•å†…å­˜
nvidia-smi -l 1 &
bash run_v18_embedding_rag.sh

# å¯¹æ¯”V17
grep "Epoch 1" logs/v17_extreme_memfix/latest.log
grep "Epoch 1" logs/v18_embedding_rag/latest.log
```

---

## âœ… å®‰å…¨æ€§ä¿è¯

1. **æ•°æ®å¯¹é½**:
   - âœ… Window indexä¸¥æ ¼å¯¹åº”
   - âœ… FAISS indexå’ŒembeddingsåŒæ­¥
   - âœ… Shapeæ£€æŸ¥: [B, L, D]

2. **å¯å›é€€**:
   - âœ… ä¿ç•™åŸä»£ç  (`src_original_rag/`)
   - âœ… å¯åˆ‡æ¢: `use_embedding_rag=false` å›åˆ°åŸç‰ˆ
   - âœ… Gitåˆ†æ”¯ç®¡ç†

3. **æ¸è¿›å¼**:
   - Phase 1: å›ºå®šç¼–ç  (ç®€å•,ç¨³å®š)
   - Phase 2: å®šæœŸåˆ·æ–° (ä¼˜åŒ–,å¯é€‰)
   - å¯ä»¥å…ˆéªŒè¯Phase 1,ç¡®è®¤æœ‰æ•ˆåå†åšPhase 2

---

## ğŸ¯ ä½ çš„å†³å®š

ç†è§£äº†Embedding RAGå,ä½ è§‰å¾—:

**é€‰é¡¹A**: å…ˆè·‘V17,æ˜å¤©çœ‹ç»“æœå†è¯´
**é€‰é¡¹B**: æˆ‘ç°åœ¨å°±å¼€å§‹å®ç°Embedding RAG Phase 1
**é€‰é¡¹C**: æœ‰å…¶ä»–é—®é¢˜æƒ³å…ˆé—®æ¸…æ¥š

å‘Šè¯‰æˆ‘ä½ çš„å†³å®š! ğŸš€
