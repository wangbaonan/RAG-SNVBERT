# ğŸš¨ ä¸¥é‡å‘ç°: AFä¿¡æ¯æµå­˜åœ¨æ ¹æœ¬æ€§é—®é¢˜

## ğŸ“‹ é—®é¢˜å‘ç°

ä½ çš„é—®é¢˜éå¸¸å°–é”ä¸”æ­£ç¡®ï¼ç»è¿‡æ·±å…¥å®¡æŸ¥ï¼Œæˆ‘å‘ç°äº†ä¸€ä¸ª**æ ¹æœ¬æ€§çš„æ¶æ„é—®é¢˜**ã€‚

---

## âŒ æ ¸å¿ƒé—®é¢˜: AFä¿¡æ¯åœ¨Embedding RAGä¸­ä¸¢å¤±è¯­ä¹‰

### é—®é¢˜1: EmbeddingFusionModuleçš„AFä½¿ç”¨æ–¹å¼

**ä»£ç ä½ç½®**: `src/model/fusion.py` Line 338-356

```python
def forward(self, emb : torch.Tensor, pos : torch.Tensor, af : torch.Tensor):
    """
    emb.shape == (batch, seq_len, emb_dim)  # å·²ç»æ˜¯é«˜ç»´embedding [B, L, D=192]
    pos.shape == (batch, seq_len)           # åŸå§‹ä½ç½® [B, L]
    af.shape == (batch, seq_len)            # åŸå§‹é¢‘ç‡ [B, L]
    """
    # poså¤„ç†: é€šè¿‡CNNæå–ç‰¹å¾
    pos_feat = self.pos_feat(pos)  # [B, L] â†’ [B, L, 1]
    pos_feat = pos_feat.unsqueeze(-1)

    # afå¤„ç†: ç›´æ¥unsqueeze!
    af_feat = af.unsqueeze(-1)  # [B, L] â†’ [B, L, 1]  â† é—®é¢˜!

    # Concat
    all_feat = torch.cat((emb, pos_feat, af_feat), dim=-1)  # [B, L, D+2]

    # Fusion
    all_feat = self.act(self.fusion(all_feat))  # Linear(D+2 â†’ D)

    return self.norm(emb + all_feat)  # æ®‹å·®è¿æ¥
```

**é—®é¢˜åˆ†æ**:

1. **Embeddingå·²ç»æ˜¯192ç»´é«˜ç»´ç©ºé—´**
2. **AFåªæ˜¯1ç»´æ ‡é‡** (0-1ä¹‹é—´çš„é¢‘ç‡å€¼)
3. **ç›´æ¥concat**: `[192ç»´å‘é‡, 1ç»´pos, 1ç»´af]` â†’ `[194ç»´]`
4. **LinearæŠ•å½±å›192ç»´**: ä¿¡æ¯è¢«ä¸¥é‡ç¨€é‡Š

**ä¸ºä»€ä¹ˆè¿™æ˜¯é—®é¢˜?**

```
ç»´åº¦å¯¹æ¯”:
  Embedding: 192ç»´ learned representation
  POS: 1ç»´ (ä½†ç»è¿‡CNNå¤„ç†ï¼Œæœ‰ä¸€å®šç‰¹å¾)
  AF: 1ç»´ raw scalar

ä¿¡æ¯å æ¯”:
  Embedding: 192/194 = 99%
  POS + AF: 2/194 = 1%  â† AFåªå 0.5%!

Linearå±‚å­¦ä¹ :
  W * [emb (192ç»´), pos (1ç»´), af (1ç»´)]

  å³ä½¿W[:, -1]æ˜¯AFçš„æƒé‡ï¼Œå®ƒåªèƒ½å­¦åˆ°:
  "åœ¨è¿™ä¸ª192ç»´ç©ºé—´ä¸­ï¼ŒAFè¿™ä¸ªæ ‡é‡å¦‚ä½•çº¿æ€§åŠ æƒ"

  ä½†æ— æ³•å­¦åˆ°:
  "AFçš„ä¸åŒå€¼å¦‚ä½•å¯¹åº”åˆ°ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´"
```

---

### é—®é¢˜2: Referenceçš„AFé—®é¢˜æ›´ä¸¥é‡

**å½“å‰V18çš„ä¿®å¤**:
```python
# Query
query_emb_raw = embedding(query_tokens)  # [B, L, D]
query_emb = emb_fusion(query_emb_raw, query_pos, query_af)  # â† Queryæœ‰è‡ªå·±çš„AF

# Retrieved
retrieved_emb_raw = ref_embeddings[idx]  # [B, L, D] é¢„ç¼–ç çš„
retrieved_emb = emb_fusion(retrieved_emb_raw, query_pos, query_af)  # â† ç”¨Queryçš„AF!
```

**é—®é¢˜**: Retrieved referenceç”¨çš„æ˜¯**Queryçš„AF**ï¼Œè€Œä¸æ˜¯Referenceè‡ªå·±çš„AF!

**ä¸ºä»€ä¹ˆè¿™æ˜¯é—®é¢˜?**

```
å‡è®¾:
  Query: æŸä¸ªæ ·æœ¬åœ¨æŸä½ç‚¹çš„AF=0.45 (common)
  Retrieved: Referenceæ ·æœ¬åœ¨è¯¥ä½ç‚¹çš„AF=0.02 (rare)

å½“å‰åšæ³•:
  retrieved_emb = emb_fusion(retrieved_emb_raw, query_pos, query_af=0.45)

  ä½†Referenceå®é™…AFæ˜¯0.02!

ç»“æœ:
  æ¨¡å‹æŠŠrare referenceå½“æˆcommonæ¥å¤„ç†äº†!
```

---

## ğŸ” æ ¹æœ¬é—®é¢˜: AFçš„ä¸¤ç§è§’è‰²æ··æ·†

### AFçš„ä¸¤ç§ä½¿ç”¨åœºæ™¯

#### åœºæ™¯A: Token-levelç‰¹å¾å¢å¼º
```python
# ç›®çš„: è®©embeddingçŸ¥é“"è¿™ä¸ªä½ç‚¹çš„é¢‘ç‡æ˜¯å¤šå°‘"
embedding = learned_vector + af_encoding

# åˆç†æ€§: âœ“
# AFæ˜¯ä½ç‚¹å›ºæœ‰å±æ€§ï¼Œåº”è¯¥ç¼–ç è¿›embedding
```

#### åœºæ™¯B: Rare variantç‰¹æ®Šå¤„ç†
```python
# ç›®çš„: å¯¹rare variants (MAF<0.05)ç»™äºˆç‰¹æ®Šæƒé‡
if MAF < 0.05:
    apply_higher_weight()

# åˆç†æ€§: âœ“
# Rare variantséœ€è¦ç‰¹æ®Šattention
```

### å½“å‰æ¶æ„çš„é—®é¢˜

```python
# 1. EmbeddingFusionModule (åœºæ™¯A)
emb_fusion(emb [192D], af [1D])
â†’ è¾“å‡º: [192D]

é—®é¢˜: AFåªå 0.5%çš„ä¿¡æ¯ï¼Œå‡ ä¹è¢«embeddingæ·¹æ²¡

# 2. EnhancedRareVariantFusion (åœºæ™¯B)
rag_fusion(query_emb, retrieved_emb, global_af, pop_af)
â†’ ä½¿ç”¨AFåšæƒé‡è°ƒåˆ¶

é—®é¢˜:
  - global_af: Queryçš„é¢‘ç‡
  - retrieved_emb: ç”¨Queryçš„AFåšçš„fusion
  - ä¸¢å¤±äº†Referenceè‡ªå·±çš„é¢‘ç‡ä¿¡æ¯!
```

---

## ğŸ“Š å…·ä½“ä¾‹å­è¯´æ˜é—®é¢˜

### ä¾‹å­: Rare variant imputation

```
Queryæ ·æœ¬:
  ä½ç‚¹chr21:12345, genotype=0/1 (het)
  è¯¥ä½ç‚¹åœ¨äººç¾¤ä¸­MAF=0.48 (common)

Retrieved reference:
  ä½ç‚¹chr21:12345, genotype=1/1 (hom alt)
  è¯¥referenceæ¥è‡ªç‰¹æ®Šäººç¾¤ï¼Œåœ¨è¯¥äººç¾¤ä¸­MAF=0.02 (rare)

æœŸæœ›è¡Œä¸º:
  æ¨¡å‹åº”è¯¥çŸ¥é“:
    - Queryæ˜¯common variantï¼ŒæŒ‰å¸¸è§„å¤„ç†
    - Referenceæ˜¯rare variantï¼Œéœ€è¦ç‰¹æ®Šé‡è§†
    - ä¸¤è€…AFä¸åŒï¼Œfusionæ—¶åº”è¯¥è€ƒè™‘è¿™ä¸ªå·®å¼‚

å½“å‰V18è¡Œä¸º:
  1. Query: emb_fusion(query_emb, pos, query_af=0.48)
  2. Retrieved: emb_fusion(retrieved_emb, pos, query_af=0.48)  â† é”™äº†!
     åº”è¯¥æ˜¯retrieved_af=0.02!

  3. RAG fusion:
     ç”¨query_af=0.48åšæƒé‡

ç»“æœ:
  - æ¨¡å‹ä¸çŸ¥é“Referenceæ˜¯rare variant
  - ä¸¢å¤±äº†å…³é”®çš„é¢‘ç‡å·®å¼‚ä¿¡æ¯
  - Imputationè´¨é‡ä¸‹é™
```

---

## ğŸ’¡ ä¸ºä»€ä¹ˆä¹‹å‰çš„Token RAGæ²¡è¿™ä¸ªé—®é¢˜

### V17 (Token RAG)

```python
# Query
query_emb = embedding(query_tokens)
query_fused = emb_fusion(query_emb, query_pos, query_af)  # â† Queryçš„AF

# Retrieved (é‡è¦!)
rag_tokens = retrieved_raw_tokens  # ä»reference panelè·å–
rag_emb = embedding(rag_tokens)    # é‡æ–°embedding
rag_fused = emb_fusion(rag_emb, rag_pos, rag_af)  # â† Referenceçš„AF!

# Fusion
output = rag_fusion(query_fused, rag_fused, query_af, query_af_p)
```

**å…³é”®**: V17å¯¹retrieved tokensåšäº†å®Œæ•´çš„embeddingå’Œemb_fusionï¼Œä½¿ç”¨çš„æ˜¯**Reference panelçš„çœŸå®AF**!

### V18 (Embedding RAG - å½“å‰)

```python
# Query
query_emb = embedding(query_tokens)
query_fused = emb_fusion(query_emb, query_pos, query_af)  # â† Queryçš„AF

# Retrieved (é—®é¢˜!)
rag_emb_pre = ref_embeddings[idx]  # é¢„ç¼–ç çš„ (ç”¨çš„ä»€ä¹ˆAF?)
rag_fused = emb_fusion(rag_emb_pre, query_pos, query_af)  # â† ç”¨Queryçš„AF!

# Fusion
output = rag_fusion(query_fused, rag_fused, query_af, query_af_p)
```

**é—®é¢˜**:
1. é¢„ç¼–ç æ—¶ç”¨çš„æ˜¯ä»€ä¹ˆAF? (ç›®å‰ä»£ç é‡Œ**æ²¡æœ‰ç”¨AF**!)
2. Fusionæ—¶ç”¨çš„æ˜¯Queryçš„AFï¼Œä¸æ˜¯Referenceçš„AF

---

## ğŸ” æ£€æŸ¥é¢„ç¼–ç è¿‡ç¨‹

### å½“å‰é¢„ç¼–ç ä»£ç 

**ä½ç½®**: `src/dataset/embedding_rag_dataset.py` Line 198-210

```python
def _build_embedding_indexes(self, ref_vcf_path: str, embedding_layer):
    with torch.no_grad():
        for w_idx in range(self.window_count):
            # ...è·å–ref_tokens [num_haps, L]

            ref_tokens_tensor = torch.LongTensor(ref_tokenized).to(device)
            ref_embeddings = embedding_layer(ref_tokens_tensor)  # [num_haps, L, D]

            # â† å®Œå…¨æ²¡æœ‰ç”¨AFä¿¡æ¯!
            # â† æ²¡æœ‰emb_fusion!

            self.ref_embeddings_windows.append(ref_embeddings.cpu())
```

**ç»“è®º**: é¢„ç¼–ç çš„embeddings**å®Œå…¨æ²¡æœ‰AFä¿¡æ¯**!

---

## ğŸ¯ é—®é¢˜æ€»ç»“

### é—®é¢˜1: EmbeddingFusionModuleä¸­AFä¿¡æ¯è¢«ç¨€é‡Š (è®¾è®¡é—®é¢˜)

```python
[192ç»´embedding, 1ç»´af] â†’ Linear(193â†’192)
AFåªå 0.5%çš„ä¿¡æ¯ï¼Œå‡ ä¹è¢«å¿½ç•¥
```

**ä¸¥é‡æ€§**: ä¸­ç­‰
**å½±å“**: æ‰€æœ‰ç‰ˆæœ¬(V17, V18)éƒ½æœ‰è¿™ä¸ªé—®é¢˜

### é—®é¢˜2: Referenceçš„AFä¿¡æ¯å®Œå…¨ä¸¢å¤± (V18ç‰¹æœ‰)

```python
# é¢„ç¼–ç : æ²¡æœ‰AF
ref_emb = embedding(ref_tokens)  # â† æ²¡æœ‰AF!

# ä½¿ç”¨: ç”¨äº†é”™è¯¯çš„AF
ref_fused = emb_fusion(ref_emb, query_pos, query_af)  # â† ç”¨Queryçš„AF!
```

**ä¸¥é‡æ€§**: ä¸¥é‡
**å½±å“**: ä»…V18

### é—®é¢˜3: RAG fusionä½¿ç”¨é”™è¯¯çš„AFä¿¡æ¯

```python
rag_fusion(query_emb, retrieved_emb, query_af, query_af_p)
# åº”è¯¥ä¼ å…¥: (query_emb, retrieved_emb, query_af, retrieved_af)
```

**ä¸¥é‡æ€§**: ä¸¥é‡
**å½±å“**: V18

---

## ğŸ’Š è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ä¿®å¤V18 - ä¿ç•™Referenceçš„AF (ç«‹å³å¯è¡Œ)

#### 1.1 åœ¨é¢„ç¼–ç æ—¶å­˜å‚¨AFä¿¡æ¯

```python
# _build_embedding_indexes
def _build_embedding_indexes(self, ref_vcf_path: str, embedding_layer):
    # é™¤äº†å­˜å‚¨embeddingsï¼Œè¿˜è¦å­˜å‚¨AF
    self.ref_af_windows = []  # æ–°å¢!

    with torch.no_grad():
        for w_idx in range(self.window_count):
            # è·å–è¯¥windowçš„AF
            window_af = self.freq[current_slice]  # [L]

            # æ‰©å±•åˆ°æ‰€æœ‰haplotypes
            ref_af = window_af.unsqueeze(0).expand(num_haps, -1)  # [num_haps, L]

            self.ref_af_windows.append(ref_af)  # å­˜å‚¨AF

            # é¢„ç¼–ç  (æš‚æ—¶ä¸ç”¨AF)
            ref_embeddings = embedding_layer(ref_tokens)
            self.ref_embeddings_windows.append(ref_embeddings.cpu())
```

#### 1.2 åœ¨collate_fnä¸­è¿”å›Referenceçš„AF

```python
def embedding_rag_collate_fn(batch_list, dataset, embedding_layer, k_retrieve=1):
    # ...æ£€ç´¢embeddings

    for i, sample in enumerate(group):
        # è·å–retrieved embedding
        ref_idx = I1[i, 0]
        retrieved_emb = ref_embeddings[ref_idx]  # [L, D]

        # è·å–retrieved AF (æ–°å¢!)
        retrieved_af = dataset.ref_af_windows[window_idx][ref_idx]  # [L]

        sample['rag_emb_h1'] = retrieved_emb
        sample['rag_af_h1'] = retrieved_af  # æ–°å¢!
```

#### 1.3 åœ¨Model Forwardä¸­ä½¿ç”¨Referenceçš„AF

```python
def forward(self, x: dict) -> tuple:
    # Query
    query_emb_raw = self.embedding(x['hap_1'])
    query_emb = self.emb_fusion(query_emb_raw, x['pos'], x['af'])

    # Retrieved
    if 'rag_emb_h1' in x:
        rag_emb_raw = x['rag_emb_h1'].to(device)

        # ä½¿ç”¨Retrievedè‡ªå·±çš„AF! (æ–°å¢)
        rag_af = x.get('rag_af_h1', x['af'])  # å¦‚æœæ²¡æœ‰ï¼Œfallbackåˆ°queryçš„
        rag_emb = self.emb_fusion(rag_emb_raw, x['pos'], rag_af)  # â† ä¿®å¤!

        # Fusionæ—¶ä¼ å…¥ä¸¤ä¸ªAF
        hap_1_fused = self.rag_fusion(
            query_emb,
            rag_emb.unsqueeze(1),
            x['af'],      # Queryçš„AF
            rag_af        # Retrievedçš„AF (æ–°å¢!)
        )
```

**ä½†æ˜¯**: è¿™éœ€è¦ä¿®æ”¹`EnhancedRareVariantFusion`çš„æ¥å£!

---

### æ–¹æ¡ˆ2: æ”¹è¿›EmbeddingFusionModuleçš„AFç¼–ç  (æ›´æ ¹æœ¬)

#### é—®é¢˜: AFåªå 0.5%ä¿¡æ¯

#### è§£å†³: AF Encoding Layer

```python
class ImprovedEmbeddingFusionModule(nn.Module):
    def __init__(self, emb_size):
        super().__init__()

        # POSå¤„ç† (ä¿æŒä¸å˜)
        self.pos_feat = PositionFeatModule()

        # AF Encoding (æ–°å¢!) - æŠŠ1ç»´AFç¼–ç åˆ°emb_sizeç»´
        self.af_encoder = nn.Sequential(
            nn.Linear(1, 32),
            nn.GELU(),
            nn.Linear(32, emb_size),
            nn.LayerNorm(emb_size)
        )

        # Fusion
        self.fusion = nn.Linear(emb_size * 3, emb_size)  # emb + pos + af_encoded
        self.act = nn.GELU()
        self.norm = nn.LayerNorm(emb_size)

    def forward(self, emb, pos, af):
        # POS
        pos_feat = self.pos_feat(pos).unsqueeze(-1)  # [B, L, 1]

        # AF Encoding (æ–°å¢!)
        af_encoded = self.af_encoder(af.unsqueeze(-1))  # [B, L, 1] â†’ [B, L, D]

        # Concat
        all_feat = torch.cat([emb, pos_feat, af_encoded], dim=-1)  # [B, L, 3D]

        # Fusion
        fused = self.act(self.fusion(all_feat))  # [B, L, 3D] â†’ [B, L, D]

        return self.norm(emb + fused)
```

**ä¼˜ç‚¹**:
- AFè¢«ç¼–ç åˆ°emb_sizeç»´åº¦ï¼Œä¿¡æ¯ä¸ä¼šè¢«ç¨€é‡Š
- å¯ä»¥å­¦ä¹ AFçš„éçº¿æ€§è¡¨ç¤º
- ä¸åŒAFå€¼å¯ä»¥æ˜ å°„åˆ°ä¸åŒçš„å­ç©ºé—´

**ç¼ºç‚¹**:
- éœ€è¦é‡æ–°è®­ç»ƒ
- å¢åŠ å‚æ•°é‡

---

### æ–¹æ¡ˆ3: åœ¨é¢„ç¼–ç æ—¶å°±åšå®Œæ•´çš„emb_fusion (å®Œç¾ä½†å¤æ‚)

è¿™å°±æ˜¯æˆ‘ä¹‹å‰æåˆ°çš„"æ–¹æ¡ˆB"ï¼Œä½†ç°åœ¨çœ‹æ¥**æ›´åŠ å¿…è¦**ï¼

```python
def _build_embedding_indexes(self, ref_vcf_path, embedding_layer, emb_fusion_layer):
    with torch.no_grad():
        for w_idx in range(self.window_count):
            # è·å–reference tokens, pos, af
            ref_tokens = ...  # [num_haps, L]
            ref_pos = ...     # [num_haps, L]
            ref_af = ...      # [num_haps, L]

            # å®Œæ•´çš„embedding pipeline
            ref_emb_raw = embedding_layer(ref_tokens)  # [num_haps, L, D]
            ref_emb_fused = emb_fusion_layer(ref_emb_raw, ref_pos, ref_af)  # â† å®Œæ•´!

            # å­˜å‚¨fused embeddings
            self.ref_embeddings_windows.append(ref_emb_fused.cpu())
```

**ä¼˜ç‚¹**:
- Referenceä¿ç•™äº†å®Œæ•´çš„AFä¿¡æ¯
- æ£€ç´¢åœ¨æ­£ç¡®çš„ç‰¹å¾ç©ºé—´

**ç¼ºç‚¹**:
- é¢„ç¼–ç æ—¶é—´å¢åŠ (éœ€è¦è¿‡emb_fusion)
- åˆ·æ–°æ—¶ä¹Ÿéœ€è¦è¿‡emb_fusion

---

## ğŸ¯ æ¨èè¡ŒåŠ¨

### çŸ­æœŸ (ç«‹å³ä¿®å¤V18)

**æ–¹æ¡ˆ1**: ä¼ é€’Referenceçš„AFåˆ°model
- ä¿®æ”¹collate_fn: è¿”å›`rag_af_h1/h2`
- ä¿®æ”¹model forward: ç”¨referenceçš„AFåšemb_fusion

**å·¥ä½œé‡**: ä¸­ç­‰ (2-3å°æ—¶)
**æ•ˆæœ**: ä¿®å¤AFä¿¡æ¯ä¸¢å¤±é—®é¢˜

### ä¸­æœŸ (æ”¹è¿›AFç¼–ç )

**æ–¹æ¡ˆ2**: æ”¹è¿›EmbeddingFusionModule
- å®ç°AF Encoder
- AFä»1ç»´ç¼–ç åˆ°emb_sizeç»´

**å·¥ä½œé‡**: ä¸­ç­‰ (2-3å°æ—¶)
**æ•ˆæœ**: AFä¿¡æ¯ä¸å†è¢«ç¨€é‡Š

### é•¿æœŸ (æœ€ä¼˜æ–¹æ¡ˆ)

**æ–¹æ¡ˆ3**: é¢„ç¼–ç æ—¶åšå®Œæ•´emb_fusion
- ä¿®æ”¹é¢„ç¼–ç é€»è¾‘
- å­˜å‚¨fully-fused embeddings

**å·¥ä½œé‡**: è¾ƒå¤§ (4-6å°æ—¶)
**æ•ˆæœ**: ç†è®ºæœ€ä¼˜

---

## ğŸ“Š å½“å‰V17 vs V18 çœŸå®å¯¹æ¯”

| æ–¹é¢ | V17 (Token RAG) | V18 (Embedding RAG - å½“å‰) |
|------|----------------|--------------------------|
| **Query AF** | âœ… æ­£ç¡®ä½¿ç”¨ | âœ… æ­£ç¡®ä½¿ç”¨ |
| **Retrieved AF** | âœ… ä½¿ç”¨Referenceçš„çœŸå®AF | âŒ ç”¨Queryçš„AF (é”™è¯¯!) |
| **AFç¼–ç ** | âš ï¸ è¢«ç¨€é‡Š (0.5%ä¿¡æ¯) | âš ï¸ è¢«ç¨€é‡Š (0.5%ä¿¡æ¯) |
| **å†…å­˜** | 19 GB | 15 GB |
| **é€Ÿåº¦** | 210 ms | 120 ms |
| **æ£€ç´¢è´¨é‡** | è¾ƒå¥½ (ç”¨äº†æ­£ç¡®AF) | è¾ƒå·® (AFä¿¡æ¯é”™è¯¯) |

**ç»“è®º**: V18è™½ç„¶å¿«ï¼Œä½†**æ£€ç´¢è´¨é‡å¯èƒ½ä¸å¦‚V17**ï¼Œå› ä¸ºAFä¿¡æ¯å¤„ç†æœ‰ä¸¥é‡é—®é¢˜ï¼

---

## âš ï¸ ä¸¥é‡æ€§è¯„ä¼°

### é—®é¢˜ä¸¥é‡æ€§: ğŸ”´ ä¸¥é‡

1. **Reference AFä¸¢å¤±**: å¯¼è‡´æ¨¡å‹ä¸çŸ¥é“retrieved referenceçš„é¢‘ç‡ç‰¹å¾
2. **Rare variantå¤„ç†é”™è¯¯**: Rare referenceè¢«å½“æˆcommonå¤„ç†
3. **å¯èƒ½æ¯”V17æ›´å·®**: å°½ç®¡é€Ÿåº¦å¿«ï¼Œä½†å‡†ç¡®ç‡å¯èƒ½ä¸‹é™

### å»ºè®®:

**æš‚åœV18è®­ç»ƒ**ï¼Œå…ˆä¿®å¤AFé—®é¢˜ï¼

---

**åˆ›å»ºæ—¶é—´**: 2025-12-02
**ä¸¥é‡æ€§**: ğŸ”´ Critical
**å¿…é¡»ä¿®å¤**: Yes
