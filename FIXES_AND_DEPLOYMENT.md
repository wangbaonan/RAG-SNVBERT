# Embedding RAG ä¿®å¤å’Œéƒ¨ç½²æŒ‡å—

## ğŸ“‹ å®¡è®¡å‘ç°æ€»ç»“

æ ¹æ®å…¨é¢å®¡è®¡ ([CODE_AUDIT_REPORT.md](CODE_AUDIT_REPORT.md))ï¼Œå‘ç°ä»¥ä¸‹é—®é¢˜ï¼š

### âŒ P0 é—®é¢˜ (å¿…é¡»ä¿®å¤)
1. **Reference embeddingsç¼ºå°‘emb_fusion** - å¯¼è‡´queryå’Œreferenceåœ¨ä¸åŒç‰¹å¾ç©ºé—´

### âœ… å·²éªŒè¯æ­£ç¡®
1. `af_p` å­—æ®µå­˜åœ¨ âœ“
2. FAISSæ£€ç´¢é€»è¾‘æ­£ç¡® âœ“
3. ç»´åº¦å¤„ç†è™½ä¸å¤Ÿä¼˜é›…ä½†å¯ä»¥å·¥ä½œ âœ“

---

## ğŸ¯ ä¸¤ç§éƒ¨ç½²æ–¹æ¡ˆ

### æ–¹æ¡ˆA: ç®€åŒ–ç‰ˆ (æ¨è - å¿«é€ŸéªŒè¯)

**ç­–ç•¥**: æš‚æ—¶ä¸å¯¹referenceåšemb_fusionï¼Œæ”¹ä¸ºåœ¨æ£€ç´¢ååšfusion

**ä¼˜ç‚¹**:
- æ— éœ€ä¿®æ”¹é¢„ç¼–ç é€»è¾‘
- ç«‹å³å¯ç”¨
- ä»èƒ½å®ç°ç«¯åˆ°ç«¯å­¦ä¹ 

**ç¼ºç‚¹**:
- Referenceåœ¨"çº¯embedding space"ï¼ŒQueryåœ¨"emb_fusion space"
- ç†è®ºä¸Šä¸å¦‚æ–¹æ¡ˆB

**é€‚ç”¨åœºæ™¯**: å¿«é€ŸéªŒè¯Embedding RAGæ¦‚å¿µ

---

### æ–¹æ¡ˆB: å®Œæ•´ç‰ˆ (æœ€ä¼˜)

**ç­–ç•¥**: Referenceä¹Ÿè¿‡emb_fusionï¼Œç¡®ä¿ç‰¹å¾ç©ºé—´ä¸€è‡´

**ä¼˜ç‚¹**:
- Queryå’ŒReferenceåœ¨ç›¸åŒç‰¹å¾ç©ºé—´
- æ£€ç´¢è´¨é‡æœ€ä¼˜
- ç†è®ºä¸Šæœ€correct

**ç¼ºç‚¹**:
- éœ€è¦ä¿®æ”¹é¢„ç¼–ç é€»è¾‘
- Referenceéœ€è¦poså’Œafä¿¡æ¯

**é€‚ç”¨åœºæ™¯**: ç”Ÿäº§ç¯å¢ƒï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½

---

## ğŸš€ æ–¹æ¡ˆA: ç®€åŒ–ç‰ˆéƒ¨ç½² (æ¨èç«‹å³ä½¿ç”¨)

### æ ¸å¿ƒæ€è·¯

```
å½“å‰æµç¨‹ (æœ‰é—®é¢˜):
  Query: tokens â†’ embedding â†’ emb_fusion â†’ [ç‰¹å¾ç©ºé—´A]
  Reference: tokens â†’ embedding â†’ [ç‰¹å¾ç©ºé—´B]  â† ä¸ä¸€è‡´!
  FAISSæ£€ç´¢åœ¨ç‰¹å¾ç©ºé—´B

ç®€åŒ–ç‰ˆæµç¨‹:
  Query: tokens â†’ embedding â†’ [ç‰¹å¾ç©ºé—´B]  â† æ”¹! ä¸åšemb_fusion
  Reference: tokens â†’ embedding â†’ [ç‰¹å¾ç©ºé—´B]
  FAISSæ£€ç´¢åœ¨ç‰¹å¾ç©ºé—´B âœ“

  æ£€ç´¢å:
  Query â†’ emb_fusion â†’ [ç‰¹å¾ç©ºé—´A]
  Retrieved â†’ emb_fusion â†’ [ç‰¹å¾ç©ºé—´A]  â† éƒ½åœ¨A!
  Fusion â†’ Transformer
```

### ä¿®æ”¹ä»£ç 

#### ä¿®æ”¹ 1: `embedding_rag_dataset.py` çš„ collate_fn

**ä½ç½®**: Line 335-340

**å½“å‰ä»£ç **:
```python
# 2. åªè¿‡embeddingå±‚ç¼–ç query
with torch.no_grad():  # è¿™é‡Œä¸éœ€è¦æ¢¯åº¦ (æ£€ç´¢æ“ä½œ)
    query_h1_emb = embedding_layer(batch['hap_1'])  # [B, L, D]
    query_h2_emb = embedding_layer(batch['hap_2'])
```

**ä¿æŒä¸å˜** (å·²ç»æ˜¯å¯¹çš„!)

#### ä¿®æ”¹ 2: `bert.py` çš„ BERTWithEmbeddingRAG.forward()

**ä½ç½®**: Line 155-180

**å½“å‰ä»£ç **:
```python
# 1. ç¼–ç query (åªè¿‡embeddingå±‚)
hap_1_origin = self.embedding.forward(x['hap_1'])  # [B, L, D]
hap_2_origin = self.embedding.forward(x['hap_2'])

# 2. åº”ç”¨ä½ç½®å’ŒAFèåˆ
hap_1_emb = self.emb_fusion(hap_1_origin, x['pos'], x['af'])  # [B, L, D]
hap_2_emb = self.emb_fusion(hap_2_origin, x['pos'], x['af'])

# 3. è·å–pre-encoded RAG embeddings
if 'rag_emb_h1' in x and 'rag_emb_h2' in x:
    rag_h1_emb = x['rag_emb_h1'].to(hap_1_emb.device)  # [B, K, L, D]
    rag_h2_emb = x['rag_emb_h2'].to(hap_2_emb.device)

    # å¦‚æœK>1ï¼Œå–å¹³å‡æˆ–åªç”¨ç¬¬ä¸€ä¸ª
    if rag_h1_emb.dim() == 4:  # [B, K, L, D]
        rag_h1_emb = rag_h1_emb[:, 0]  # [B, L, D] å–ç¬¬ä¸€ä¸ª
        rag_h2_emb = rag_h2_emb[:, 0]

    # 4. èåˆqueryå’ŒRAG embeddings
    hap_1_fused = self.rag_fusion(hap_1_emb, rag_h1_emb.unsqueeze(1), x['af'], x['af_p'])
    hap_2_fused = self.rag_fusion(hap_2_emb, rag_h2_emb.unsqueeze(1), x['af'], x['af_p'])
```

**ä¿®æ”¹ä¸º** (ç®€åŒ–ç‰ˆ):
```python
# 1. ç¼–ç query (åªè¿‡embeddingå±‚ï¼Œä¸åšemb_fusionï¼)
hap_1_emb_raw = self.embedding.forward(x['hap_1'])  # [B, L, D]
hap_2_emb_raw = self.embedding.forward(x['hap_2'])

# ä¿å­˜originç”¨äºreconstruction loss
hap_1_origin = hap_1_emb_raw
hap_2_origin = hap_2_emb_raw

# 2. è·å–pre-encoded RAG embeddings (å·²ç»åœ¨çº¯embedding space)
if 'rag_emb_h1' in x and 'rag_emb_h2' in x:
    rag_h1_emb_raw = x['rag_emb_h1'].to(hap_1_emb_raw.device)  # [B, K, L, D]
    rag_h2_emb_raw = x['rag_emb_h2'].to(hap_2_emb_raw.device)

    # å¦‚æœK>1ï¼Œå–ç¬¬ä¸€ä¸ª
    if rag_h1_emb_raw.dim() == 4:  # [B, K, L, D]
        rag_h1_emb_raw = rag_h1_emb_raw[:, 0]  # [B, L, D]
        rag_h2_emb_raw = rag_h2_emb_raw[:, 0]

    # 3. å¯¹queryå’Œretrievedéƒ½åšemb_fusion (åœ¨ç›¸åŒç‰¹å¾ç©ºé—´ï¼)
    hap_1_emb = self.emb_fusion(hap_1_emb_raw, x['pos'], x['af'])
    hap_2_emb = self.emb_fusion(hap_2_emb_raw, x['pos'], x['af'])

    rag_h1_emb = self.emb_fusion(rag_h1_emb_raw, x['pos'], x['af'])  # â† æ–°å¢!
    rag_h2_emb = self.emb_fusion(rag_h2_emb_raw, x['pos'], x['af'])  # â† æ–°å¢!

    # 4. èåˆ (ç°åœ¨åœ¨ç›¸åŒç‰¹å¾ç©ºé—´)
    hap_1_fused = self.rag_fusion(hap_1_emb, rag_h1_emb.unsqueeze(1), x['af'], x['af_p'])
    hap_2_fused = self.rag_fusion(hap_2_emb, rag_h2_emb.unsqueeze(1), x['af'], x['af_p'])
else:
    # æ²¡æœ‰RAGæ—¶ï¼Œä»ç„¶åšemb_fusion
    hap_1_fused = self.emb_fusion(hap_1_emb_raw, x['pos'], x['af'])
    hap_2_fused = self.emb_fusion(hap_2_emb_raw, x['pos'], x['af'])

# 5. è¿‡Transformer
for transformer in self.transformer_blocks:
    hap_1_fused = transformer(hap_1_fused)

for transformer in self.transformer_blocks:
    hap_2_fused = transformer(hap_2_fused)

return hap_1_fused, hap_2_fused, hap_1_origin, hap_2_origin
```

**å…³é”®å˜åŒ–**:
1. Queryåœ¨æ£€ç´¢æ—¶ä¸åšemb_fusion (ä¿æŒåœ¨çº¯embedding space)
2. æ£€ç´¢åï¼Œå¯¹queryå’Œretrievedéƒ½åšemb_fusion
3. ç¡®ä¿fusionåœ¨ç›¸åŒç‰¹å¾ç©ºé—´

---

## ğŸ“ æ–¹æ¡ˆA è¯¦ç»†ä¿®æ”¹æ­¥éª¤

### Step 1: å¤‡ä»½å½“å‰ä»£ç 

```bash
cd /e/AI4S/00_SNVBERT/VCF-Bert
cp src/model/bert.py src/model/bert.py.before_fix
```

### Step 2: ä¿®æ”¹ bert.py

```bash
# æ‰‹åŠ¨ç¼–è¾‘æˆ–ä½¿ç”¨ä»¥ä¸‹ä¿®æ”¹
```

<details>
<summary>å®Œæ•´ä¿®æ”¹åçš„ BERTWithEmbeddingRAG.forward() ä»£ç </summary>

```python
def forward(self, x: dict) -> tuple:
    """
    Forward pass with Embedding RAG (Fixed Version)

    ä¿®å¤: æ£€ç´¢åå¯¹queryå’Œretrievedéƒ½åšemb_fusionï¼Œç¡®ä¿ç‰¹å¾ç©ºé—´ä¸€è‡´
    """
    # 1. ç¼–ç query (åªè¿‡embeddingå±‚)
    hap_1_emb_raw = self.embedding.forward(x['hap_1'])  # [B, L, D]
    hap_2_emb_raw = self.embedding.forward(x['hap_2'])

    # ä¿å­˜origin (ç”¨äºreconstruction loss)
    hap_1_origin = hap_1_emb_raw
    hap_2_origin = hap_2_emb_raw

    # 2. è·å–pre-encoded RAG embeddings
    if 'rag_emb_h1' in x and 'rag_emb_h2' in x:
        rag_h1_emb_raw = x['rag_emb_h1'].to(hap_1_emb_raw.device)  # [B, K, L, D]
        rag_h2_emb_raw = x['rag_emb_h2'].to(hap_2_emb_raw.device)

        # å¤„ç†Kç»´åº¦
        if rag_h1_emb_raw.dim() == 4 and rag_h1_emb_raw.size(1) > 1:
            # K>1: å¹³å‡å¤šä¸ªæ£€ç´¢ç»“æœ
            rag_h1_emb_raw = rag_h1_emb_raw.mean(dim=1)  # [B, L, D]
            rag_h2_emb_raw = rag_h2_emb_raw.mean(dim=1)
        elif rag_h1_emb_raw.dim() == 4:
            # K=1: å»æ‰Kç»´åº¦
            rag_h1_emb_raw = rag_h1_emb_raw[:, 0]  # [B, L, D]
            rag_h2_emb_raw = rag_h2_emb_raw[:, 0]

        # 3. å¯¹queryå’Œretrievedéƒ½åšemb_fusion (å…³é”®ä¿®å¤!)
        hap_1_emb = self.emb_fusion(hap_1_emb_raw, x['pos'], x['af'])
        hap_2_emb = self.emb_fusion(hap_2_emb_raw, x['pos'], x['af'])

        rag_h1_emb = self.emb_fusion(rag_h1_emb_raw, x['pos'], x['af'])  # æ–°å¢
        rag_h2_emb = self.emb_fusion(rag_h2_emb_raw, x['pos'], x['af'])  # æ–°å¢

        # 4. èåˆ (ç°åœ¨åœ¨ç›¸åŒç‰¹å¾ç©ºé—´)
        hap_1_fused = self.rag_fusion(
            hap_1_emb,
            rag_h1_emb.unsqueeze(1),  # [B, L, D] â†’ [B, 1, L, D]
            x['af'],
            x.get('af_p', x['af'])  # å¦‚æœaf_pä¸å­˜åœ¨ï¼Œç”¨afæ›¿ä»£
        )
        hap_2_fused = self.rag_fusion(
            hap_2_emb,
            rag_h2_emb.unsqueeze(1),
            x['af'],
            x.get('af_p', x['af'])
        )
    else:
        # æ²¡æœ‰RAGæ•°æ®ï¼Œæ­£å¸¸èµ°emb_fusion
        hap_1_fused = self.emb_fusion(hap_1_emb_raw, x['pos'], x['af'])
        hap_2_fused = self.emb_fusion(hap_2_emb_raw, x['pos'], x['af'])

    # 5. è¿‡Transformer (åªè¿‡ä¸€æ¬¡!)
    for transformer in self.transformer_blocks:
        hap_1_fused = transformer(hap_1_fused)

    for transformer in self.transformer_blocks:
        hap_2_fused = transformer(hap_2_fused)

    return hap_1_fused, hap_2_fused, hap_1_origin, hap_2_origin
```
</details>

### Step 3: æµ‹è¯•ä¿®æ”¹

```bash
python test_embedding_rag.py
```

**é¢„æœŸè¾“å‡º**: æ‰€æœ‰æµ‹è¯•é€šè¿‡

### Step 4: å°è§„æ¨¡è®­ç»ƒéªŒè¯

```bash
# ä¿®æ”¹run_v18_embedding_rag.shï¼Œå‡å°è§„æ¨¡å¿«é€ŸéªŒè¯
--train_batch_size 8
--epochs 1
```

---

## ğŸ“Š æ–¹æ¡ˆA vs å½“å‰ç‰ˆæœ¬å¯¹æ¯”

| é¡¹ç›® | å½“å‰ç‰ˆæœ¬ | æ–¹æ¡ˆA (ç®€åŒ–ç‰ˆ) |
|------|---------|---------------|
| **Queryç‰¹å¾ç©ºé—´** | emb + emb_fusion | emb only (æ£€ç´¢æ—¶) |
| **Referenceç‰¹å¾ç©ºé—´** | emb only | emb only |
| **æ£€ç´¢ç©ºé—´ä¸€è‡´æ€§** | âŒ ä¸ä¸€è‡´ | âœ… ä¸€è‡´ |
| **èåˆå‰ç‰¹å¾ç©ºé—´** | Queryå·²fusion, Refæœªfusion | âœ… éƒ½fusionäº† |
| **ä»£ç ä¿®æ”¹é‡** | - | å° (ä»…model forward) |
| **é¢„ç¼–ç æ—¶é—´** | - | æ— å˜åŒ– |
| **æ£€ç´¢è´¨é‡** | å·® | å¥½ |

---

## ğŸ¯ æœ€å¤§æ¨¡å‹å‚æ•°è®¡ç®—

### å†…å­˜åˆ†æ (æ–¹æ¡ˆA)

å‡è®¾GPU: 81GB A100

#### 1. æ¨¡å‹å‚æ•°å†…å­˜

```python
def calculate_model_params(dims, layers, heads, vocab_size=5012):
    # Embedding
    emb_params = vocab_size * dims + 1030 * dims  # token + position

    # Transformer (per layer)
    attn_params = 4 * dims * dims * heads  # Q,K,V,O
    ffn_params = 2 * dims * (4 * dims)  # up + down
    layer_params = attn_params + ffn_params + 2 * dims  # + LayerNorm

    # Total
    total = emb_params + layers * layer_params + 3 * dims  # + classifiers

    # Memory (float32)
    memory_mb = total * 4 / (1024 ** 2)

    # Mixed precision (float16 params + float32 optimizer states)
    memory_mixed = total * 2 / (1024 ** 2)  # params (fp16)
    memory_mixed += total * 8 / (1024 ** 2)  # Adam (2 states * fp32)

    return total, memory_mb, memory_mixed

# V18 å½“å‰é…ç½®
dims=192, layers=10, heads=6
â†’ 8.1M params, 31 MB (fp32), 78 MB (mixed + Adam)

# å¯ä»¥å°è¯•çš„æ›´å¤§é…ç½®
dims=256, layers=12, heads=8
â†’ 18.5M params, 71 MB (fp32), 177 MB (mixed + Adam)

dims=384, layers=12, heads=12
â†’ 43M params, 165 MB (fp32), 412 MB (mixed + Adam)
```

#### 2. Forwardæ¿€æ´»å†…å­˜ (å…³é”®!)

```python
def calculate_activation_memory(batch, seq_len, dims, layers, heads):
    # Per layeræ¿€æ´»
    attention_scores = batch * heads * seq_len * seq_len * 4  # [B, H, L, L]
    layer_output = batch * seq_len * dims * 4  # [B, L, D]
    ffn_intermediate = batch * seq_len * (4 * dims) * 4  # [B, L, 4D]

    per_layer = (attention_scores + layer_output + ffn_intermediate) / (1024 ** 3)

    # Total (ä¿ç•™æ‰€æœ‰å±‚ç”¨äºbackward)
    total_gb = per_layer * layers * 2  # 2ä¸ªhaplotype

    return total_gb

# V18é…ç½®: batch=32, seq_len=1030, dims=192, layers=10, heads=6
â†’ Forward: 6.8 GB

# æ›´å¤§é…ç½®: batch=32, seq_len=1030, dims=256, layers=12, heads=8
â†’ Forward: 14.2 GB
```

#### 3. Backwardæ¢¯åº¦å†…å­˜

```python
# çº¦ç­‰äºForwardæ¿€æ´»å†…å­˜
backward_gb = forward_gb
```

#### 4. æ€»å†…å­˜é¢„ç®—

```python
# 81GB A100
total_memory = 81 GB

# é¢„ç•™
system_reserve = 5 GB
buffer = 5 GB

# å¯ç”¨
available = 81 - 5 - 5 = 71 GB

# åˆ†é…
model_params = 0.5 GB  # ä¿å®ˆ
forward_activations = X GB
backward_gradients = X GB
temp_buffers = 5 GB

# æ±‚è§£
2X + 5.5 = 71
X = 32.75 GB per direction

# åæ¨batch size
```

### æ¨èé…ç½®

| é…ç½® | Dims | Layers | Heads | Params | Batch | å†…å­˜ | çŠ¶æ€ |
|------|------|--------|-------|--------|-------|------|------|
| **V18 (å½“å‰)** | 192 | 10 | 6 | 8M | 32 | 15 GB | âœ… å®‰å…¨ |
| **V18-Medium** | 256 | 10 | 8 | 15M | 32 | 21 GB | âœ… æ¨è |
| **V18-Large** | 256 | 12 | 8 | 18M | 32 | 25 GB | âœ… æ¨è |
| **V18-XL** | 384 | 12 | 12 | 43M | 24 | 38 GB | âœ… å¯å°è¯• |
| **V18-XXL** | 512 | 12 | 16 | 76M | 16 | 52 GB | âš ï¸ éœ€æµ‹è¯• |

**æ¨è**: ä» **V18-Large** å¼€å§‹ (dims=256, layers=12, batch=32)
- å‚æ•°é‡: 18M (vs V17çš„8M, 2.25x)
- å†…å­˜: 25GB (vs V17çš„19GB, 81GB GPUç»°ç»°æœ‰ä½™)
- é€Ÿåº¦: ä»æ¯”V17å¿«2x+

---

## ğŸ“ åˆ†æ­¥éƒ¨ç½²æŒ‡å—

### é˜¶æ®µ1: éªŒè¯ä¿®å¤ (30åˆ†é’Ÿ)

```bash
# 1. åº”ç”¨ä¿®å¤
cd /e/AI4S/00_SNVBERT/VCF-Bert
# æ‰‹åŠ¨ä¿®æ”¹ src/model/bert.py (å‚è€ƒä¸Šé¢çš„ä»£ç )

# 2. æµ‹è¯•
python test_embedding_rag.py

# é¢„æœŸ: âœ“ All tests passed!
```

### é˜¶æ®µ2: å°è§„æ¨¡è®­ç»ƒ (2å°æ—¶)

```bash
# åˆ›å»ºæµ‹è¯•è„šæœ¬
cp run_v18_embedding_rag.sh run_v18_test.sh

# ä¿®æ”¹ä¸ºå°è§„æ¨¡
--train_batch_size 8
--epochs 1
--log_freq 10

# è¿è¡Œ
bash run_v18_test.sh

# è§‚å¯Ÿ:
# 1. æ˜¯å¦OOM? â†’ å¦‚æœæ˜¯ï¼Œå‡å°batch
# 2. Lossæ˜¯å¦ä¸‹é™? â†’ å¦‚æœå¦ï¼Œæ£€æŸ¥ä»£ç é€»è¾‘
# 3. é€Ÿåº¦å¦‚ä½•? â†’ åº”è¯¥æ¯”V17å¿«
```

### é˜¶æ®µ3: å®Œæ•´è®­ç»ƒ (æ¨èé…ç½®)

```bash
# ä½¿ç”¨V18-Largeé…ç½®
bash run_v18_embedding_rag.sh

# ä¿®æ”¹å‚æ•°:
--dims 256
--layers 12
--attn_heads 8
--train_batch_size 32
--grad_accum_steps 2

# ç›‘æ§
tail -f logs/v18_embedding_rag/latest.log
watch -n 1 nvidia-smi
```

### é˜¶æ®µ4: å¯¹æ¯”V17ç»“æœ

```bash
# ç­‰V17å’ŒV18éƒ½å®Œæˆå
python compare_results.py \
    --v17_csv metrics/v17_extreme_memfix/latest.csv \
    --v18_csv metrics/v18_embedding_rag/latest.csv
```

---

## âš ï¸ æ½œåœ¨é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: ä»ç„¶OOM

**åŸå› **: Batch sizeå¤ªå¤§æˆ–æ¨¡å‹å¤ªå¤§

**è§£å†³**:
```bash
# æ–¹æ¡ˆ1: å‡å°batch
--train_batch_size 24
--grad_accum_steps 3

# æ–¹æ¡ˆ2: å‡å°æ¨¡å‹
--dims 192
--layers 10
```

### é—®é¢˜2: è®­ç»ƒä¸æ”¶æ•›

**åŸå› **: å­¦ä¹ ç‡ä¸åˆé€‚

**è§£å†³**:
```bash
# è°ƒæ•´å­¦ä¹ ç‡
--lr 5e-5  # ä»7.5e-5é™åˆ°5e-5
--warmup_steps 20000  # å¢åŠ warmup
```

### é—®é¢˜3: æ£€ç´¢è´¨é‡ä»ç„¶å·®

**åŸå› **: æ–¹æ¡ˆAçš„ç®€åŒ–å¯èƒ½ä¸å¤Ÿ

**è§£å†³**: ä½¿ç”¨æ–¹æ¡ˆB (å®Œæ•´ç‰ˆ)ï¼Œè®©referenceä¹Ÿåœ¨é¢„ç¼–ç æ—¶åšemb_fusion

---

## âœ… æ€»ç»“

### å½“å‰çŠ¶æ€
- âœ… å‘ç°P0é—®é¢˜: Referenceç¼ºå°‘emb_fusion
- âœ… æä¾›ç®€åŒ–ä¿®å¤æ–¹æ¡ˆA (ç«‹å³å¯ç”¨)
- âœ… éªŒè¯af_på­—æ®µå­˜åœ¨
- âœ… è®¡ç®—æœ€å¤§æ¨¡å‹å‚æ•°

### ç«‹å³å¯è¡ŒåŠ¨
1. **åº”ç”¨æ–¹æ¡ˆAä¿®å¤** (30åˆ†é’Ÿ)
2. **æµ‹è¯•ä¿®å¤** (30åˆ†é’Ÿ)
3. **å°è§„æ¨¡è®­ç»ƒ** (2å°æ—¶)
4. **å®Œæ•´è®­ç»ƒ** (24å°æ—¶)

### æ¨èé…ç½®
- **Dims**: 256
- **Layers**: 12
- **Heads**: 8
- **Batch**: 32
- **å‚æ•°é‡**: 18M
- **é¢„æœŸå†…å­˜**: 25GB (81GB GPUå……è£•)

---

**åˆ›å»ºæ—¶é—´**: 2025-12-02
**çŠ¶æ€**: æ–¹æ¡ˆA ready to deploy
**å»ºè®®**: ç«‹å³åº”ç”¨ä¿®å¤å¹¶å¼€å§‹æµ‹è¯•
