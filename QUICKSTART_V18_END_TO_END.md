# V18 End-to-End Learnable RAG - å¿«é€Ÿå¼€å§‹æŒ‡å—

## 1. åœ¨æœåŠ¡å™¨ä¸Šæ‹‰å–æœ€æ–°ä»£ç 

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/VCF-Bert

# æ‹‰å–æœ€æ–°ä»£ç 
git pull origin main

# åº”è¯¥çœ‹åˆ°ä»¥ä¸‹æ–‡ä»¶æ›´æ–°:
# - src/dataset/embedding_rag_dataset.py
# - src/main/pretrain_with_val_optimized.py
# - src/model/fusion.py
# - src/train_embedding_rag.py
# - END_TO_END_LEARNABLE_RAG_FIX.md (æ–°æ–‡ä»¶)
```

## 2. éªŒè¯å…³é”®ä¿®æ”¹

```bash
# éªŒè¯1: æ£€æŸ¥ process_batch_retrieval æ–¹æ³•
grep -n "def process_batch_retrieval" src/dataset/embedding_rag_dataset.py
# åº”è¯¥è¾“å‡º: 306:    def process_batch_retrieval(self, batch, embedding_layer, device, k_retrieve=1):

# éªŒè¯2: æ£€æŸ¥æ¢¯åº¦å¯ç”¨
grep -n "grad_enabled=True" src/dataset/embedding_rag_dataset.py
# åº”è¯¥è¾“å‡º: 381:                win_idx, device=device, grad_enabled=True

# éªŒè¯3: æ£€æŸ¥ num_workers æ¢å¤
grep -n "num_workers.*4" src/train_embedding_rag.py
# åº”è¯¥è¾“å‡º: 69:    parser.add_argument("--num_workers", type=int, default=4, ...

# éªŒè¯4: æ£€æŸ¥æ¨¡å‹ç»´åº¦æ›´æ–°
grep -n "dims.*384" src/train_embedding_rag.py
# åº”è¯¥è¾“å‡º: 40:    parser.add_argument("--dims", type=int, default=384, ...

# éªŒè¯5: æ£€æŸ¥ log1p ä¼˜åŒ–
grep -n "log1p" src/model/fusion.py
# åº”è¯¥è¾“å‡º: 160:        maf_weight = torch.log1p(1.0 / (maf + 1e-6)).clamp(max=3.0)
```

## 3. ç¡®è®¤é¢„ç¼–ç å·²å®Œæˆ

```bash
# æ£€æŸ¥ FAISS ç´¢å¼•æ˜¯å¦å­˜åœ¨
ls faiss_indexes/ | head -5
# åº”è¯¥çœ‹åˆ°: index_0.faiss, index_1.faiss, index_2.faiss, ...

# ç»Ÿè®¡ç´¢å¼•æ–‡ä»¶æ•°é‡
ls faiss_indexes/*.faiss | wc -l
# åº”è¯¥è¾“å‡º: 331

# æ£€æŸ¥ç£ç›˜å ç”¨
du -sh faiss_indexes/
# åº”è¯¥æ˜¾ç¤ºçº¦ 490GB
```

**å¦‚æœ faiss_indexes/ ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´**ï¼Œéœ€è¦é‡æ–°è¿è¡Œé¢„ç¼–ç ï¼š
```bash
bash run_v18_embedding_rag.sh
# é¢„ç¼–ç ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œçº¦éœ€ 20-30 åˆ†é’Ÿ
```

## 4. è¿è¡Œè®­ç»ƒè„šæœ¬

### æ–¹å¼1: ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆæ¨èï¼‰

```bash
# ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬
bash run_v18_embedding_rag.sh

# æˆ–è€…æŸ¥çœ‹è„šæœ¬å†…å®¹åæ‰‹åŠ¨æ‰§è¡Œ
cat run_v18_embedding_rag.sh
```

### æ–¹å¼2: è‡ªå®šä¹‰å‚æ•°

```bash
python -m src.train_embedding_rag \
    --train_dataset data/train.h5 \
    --train_panel data/train_panel.txt \
    --val_dataset data/val.h5 \
    --val_panel data/val_panel.txt \
    --refpanel_path data/reference_panel.vcf.gz \
    --freq_path data/freq.pkl \
    --window_path data/windows.pkl \
    --type_path data/type_to_idx.pkl \
    --pop_path data/pop_to_idx.pkl \
    --pos_path data/pos_to_idx.pkl \
    --dims 384 \
    --layers 12 \
    --attn_heads 12 \
    --train_batch_size 24 \
    --val_batch_size 48 \
    --num_workers 4 \
    --epochs 20 \
    --lr 7.5e-5 \
    --warmup_steps 15000 \
    --grad_accum_steps 2 \
    --rag_k 1 \
    --output_path models/v18_embedding_rag.pt \
    --metrics_csv metrics/v18_embedding_rag.csv
```

### å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|------|
| `--dims` | **384** | æ¨¡å‹ç»´åº¦ï¼ˆä»192å¢åŠ ï¼‰ |
| `--layers` | **12** | Transformerå±‚æ•°ï¼ˆä»10å¢åŠ ï¼‰ |
| `--attn_heads` | **12** | æ³¨æ„åŠ›å¤´æ•°ï¼ˆä»6å¢åŠ ï¼‰ |
| `--train_batch_size` | **24** | è®­ç»ƒbatch sizeï¼ˆä»32å‡å°‘ï¼Œé€‚åº”384ç»´ï¼‰ |
| `--num_workers` | **4** | DataLoader workeræ•°ï¼ˆä»0æ¢å¤ï¼‰ |
| `--rag_k` | 1 | RAGæ£€ç´¢Kå€¼ |

## 5. ç›‘æ§è®­ç»ƒè¿‡ç¨‹

### ç»ˆç«¯1: ä¸»è®­ç»ƒæ—¥å¿—
```bash
tail -f logs/v18_embedding_rag/latest.log
```

**é¢„æœŸè¾“å‡º**:
```
Epoch 1/20
============================================================
Epoch 1 - TRAINING
============================================================
EP_Train:0:   0%|| 1/8617 [00:00<?, ?it/s]
  â†‘ ç¬¬ä¸€ä¸ªbatchåº”è¯¥æˆåŠŸï¼ˆæ— CUDA fork errorï¼‰

EP_Train:0:   1%|| 100/8617 [00:30<50:15, 2.82it/s]
  Loss: 0.523, F1: 0.887
  â†‘ é€Ÿåº¦åº”è¯¥æ¯”ä¹‹å‰å¿«ï¼ˆnum_workers=4çš„æ•ˆæœï¼‰
```

### ç»ˆç«¯2: ç³»ç»Ÿå†…å­˜ç›‘æ§
```bash
watch -n 5 "free -h | grep Mem"
```

**é¢„æœŸ**:
```
              total        used        free      shared  buff/cache   available
Mem:          256Gi        25Gi       220Gi        1.0Gi        10Gi       228Gi
                           â†‘ åº”è¯¥ç¨³å®šåœ¨ 20-30GB
```

### ç»ˆç«¯3: GPU ç›‘æ§
```bash
watch -n 2 nvidia-smi
```

**é¢„æœŸ**:
```
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
|   0  A100-SXM... On           | 00000000:00:1E.0 Off |                    0 |
|-------------------------------+----------------------+----------------------+
| 20GB / 80GB      |  85%        | 75Â°C                | ...                  |
  â†‘ 384ç»´æ¨¡å‹ä¼šç”¨æ›´å¤šæ˜¾å­˜ï¼Œä½†80GB A100è¶³å¤Ÿ
```

### ç»ˆç«¯4: è®­ç»ƒæŒ‡æ ‡
```bash
watch -n 10 "tail -5 metrics/v18_embedding_rag/latest.csv"
```

## 6. é¢„æœŸè®­ç»ƒæ•ˆæœ

### ç¬¬ä¸€ä¸ª Batchï¼ˆå…³é”®æ£€æŸ¥ç‚¹ï¼‰
```
EP_Train:0:   0%|| 1/8617 [00:01<?, ?it/s]
âœ… å¦‚æœæˆåŠŸå®Œæˆ â†’ process_batch_retrieval å·¥ä½œæ­£å¸¸
âŒ å¦‚æœ CUDA fork error â†’ æ£€æŸ¥ num_workers æ˜¯å¦çœŸçš„æ˜¯4
```

### å‰ 100 ä¸ª Batch
```
EP_Train:0:   1%|| 100/8617 [00:35<49:30, 2.87it/s]
  Loss: 0.512
  Train F1: 0.892

âœ… é€Ÿåº¦: 2.5-3.0 it/sï¼ˆæ¯”ä¹‹å‰çš„num_workers=0å¿«ï¼‰
âœ… Loss: åº”è¯¥å¹³ç¨³ä¸‹é™
âœ… F1: åº”è¯¥é€æ¸æå‡
```

### Epoch 1 å®Œæˆ
```
Epoch 1 Summary:
  Train Loss: 0.405
  Train F1: 0.943
  Val F1: 0.955
  Rare F1: 0.928
  Time: 1.8-2.0h

âœ… Train F1 > 0.92
âœ… Val F1 > 0.95
âœ… æ¯”V17æ›´å¥½ï¼ˆç«¯åˆ°ç«¯å­¦ä¹ çš„æ•ˆæœï¼‰
```

### Epoch 2 å¼€å§‹ï¼ˆMask åˆ·æ–°ï¼‰
```
Epoch 2/20
================================================================================
â–£ Epoch 2: åˆ·æ–°Maskå’Œç´¢å¼• (æ•°æ®å¢å¼º)
================================================================================
â–£ åˆ·æ–°Mask Pattern (ç‰ˆæœ¬ 1, Seed=2)
âœ“ Maskåˆ·æ–°å®Œæˆ! æ–°ç‰ˆæœ¬: 1

â–£ é‡å»ºFAISSç´¢å¼• (åŸºäºæ–°Mask)
é‡å»ºç´¢å¼•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331/331 [08:15<00:00, 1.50s/it]
âœ“ ç´¢å¼•é‡å»ºå®Œæˆ! è€—æ—¶: 495.32s
âœ“ Maskå’Œç´¢å¼•åˆ·æ–°å®Œæˆ!
```

## 7. éªŒè¯æ¢¯åº¦å›ä¼ ï¼ˆå¯é€‰ï¼‰

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æ·»åŠ ä»¥ä¸‹ä»£ç éªŒè¯æ¢¯åº¦æ˜¯å¦æ­£ç¡®å›ä¼ ï¼š

```python
# åœ¨ç¬¬ä¸€ä¸ªepochçš„ç¬¬ä¸€ä¸ªbatchåæ£€æŸ¥
# ä¿®æ”¹ src/train_embedding_rag.py çš„è®­ç»ƒå¾ªç¯:

if epoch == 0:
    # ä¿å­˜åˆå§‹æƒé‡
    initial_weights = embedding_layer.token.weight.clone()

# ... è®­ç»ƒç¬¬ä¸€ä¸ªepoch ...

if epoch == 0:
    # æ£€æŸ¥æƒé‡æ˜¯å¦æ›´æ–°
    final_weights = embedding_layer.token.weight
    weights_changed = not torch.allclose(initial_weights, final_weights, atol=1e-6)
    print(f"\nâœ… Embeddingå±‚æ¢¯åº¦å›ä¼ éªŒè¯: {weights_changed}")
    print(f"   æƒé‡å˜åŒ–èŒƒå›´: {(final_weights - initial_weights).abs().max():.6f}")
```

**é¢„æœŸè¾“å‡º**:
```
âœ… Embeddingå±‚æ¢¯åº¦å›ä¼ éªŒè¯: True
   æƒé‡å˜åŒ–èŒƒå›´: 0.003421

â†‘ True è¡¨ç¤ºæ¢¯åº¦æ­£ç¡®å›ä¼ ï¼Œæƒé‡å·²æ›´æ–°
```

## 8. å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1: CUDA fork error ä»ç„¶å‡ºç°
```
RuntimeError: Cannot re-initialize CUDA in forked subprocess
```

**æ£€æŸ¥**:
```bash
# ç¡®è®¤ num_workers æ˜¯å¦ä¸º 4
python -c "import sys; sys.path.insert(0, 'src'); from train_embedding_rag import *; import argparse; parser = argparse.ArgumentParser(); args = parser.parse_args([]); print(args.num_workers if hasattr(args, 'num_workers') else 'not set')"

# æˆ–ç›´æ¥æŸ¥çœ‹
grep "num_workers" src/train_embedding_rag.py | grep "default"
# åº”è¯¥è¾“å‡ºåŒ…å« default=4
```

**è§£å†³**: å¦‚æœè¿˜æ˜¯0ï¼Œé‡æ–° `git pull`

### é—®é¢˜2: å†…å­˜ OOM
```
Killed (OOM)
```

**æ£€æŸ¥å†…å­˜ä½¿ç”¨**:
```bash
free -h
```

**å¯èƒ½åŸå› **:
- FAISS ç´¢å¼•ç¼“å­˜è¿‡å¤š â†’ æ­£å¸¸ï¼Œè®¾è®¡å¦‚æ­¤
- å…¶ä»–è¿›ç¨‹å ç”¨å†…å­˜ â†’ æ¸…ç†å…¶ä»–è¿›ç¨‹

**è§£å†³**:
- ç¡®ä¿æœ‰ >100GB ç©ºé—²å†…å­˜
- å‡å° `--train_batch_size` åˆ° 16 æˆ– 20

### é—®é¢˜3: è®­ç»ƒé€Ÿåº¦æ²¡æœ‰æå‡

**æ£€æŸ¥ DataLoader é…ç½®**:
```bash
grep -A 5 "train_dataloader = DataLoader" src/train_embedding_rag.py
# åº”è¯¥çœ‹åˆ° num_workers=args.num_workers å’Œ pin_memory=True
```

**å¯èƒ½åŸå› **:
- ç£ç›˜ I/O æ…¢ï¼ˆFAISS ç´¢å¼•åŠ è½½ï¼‰
- batch å¤„ç†æ—¶é—´ä¸»è¦åœ¨ GPUï¼ˆæ­£å¸¸ï¼‰

### é—®é¢˜4: Loss ä¸ä¸‹é™æˆ–éœ‡è¡

**å¯èƒ½åŸå› **:
- å­¦ä¹ ç‡è¿‡å¤§ â†’ é™ä½ `--lr` åˆ° 5e-5
- Warmup ä¸è¶³ â†’ å¢åŠ  `--warmup_steps` åˆ° 20000
- AF åŠ æƒé—®é¢˜ â†’ å·²é€šè¿‡ log1p ä¿®å¤ï¼Œåº”è¯¥ä¸ä¼šå†å‡ºç°

**éªŒè¯ log1p ä¿®å¤**:
```bash
grep "log1p" src/model/fusion.py
# åº”è¯¥æ‰¾åˆ°ä¿®å¤çš„ä»£ç 
```

## 9. å®Œæ•´è®­ç»ƒæ—¶é—´é¢„ä¼°

```
é¢„ç¼–ç :    20-30 åˆ†é’Ÿ (å·²å®Œæˆ)
Epoch 1:   1.8-2.0 å°æ—¶
Epoch 2:   1.8-2.0 å°æ—¶ (å« 8 åˆ†é’Ÿ mask åˆ·æ–°)
...
Epoch 20:  1.8-2.0 å°æ—¶

æ€»è®¡: 30åˆ†é’Ÿ + 1.9h Ã— 20 â‰ˆ 38-40 å°æ—¶
```

**åŠ é€Ÿå› ç´ **:
- âœ… `num_workers=4`: æ•°æ®åŠ è½½å¿« ~4x
- âœ… `pin_memory=True`: CPUâ†’GPU ä¼ è¾“å¿«
- âœ… æ›´å¤§batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰

**å‡é€Ÿå› ç´ **:
- âš ï¸ 384ç»´æ¨¡å‹: è®¡ç®—é‡å¢åŠ  ~2x
- âš ï¸ FAISS ç´¢å¼•åŠ è½½: æ¯batchçº¦ 50ms

**å‡€æ•ˆæœ**: ä¸ä¹‹å‰ç›¸æ¯”å¯èƒ½å¿« 10-20%

## 10. æˆåŠŸæ ‡å¿—

### âœ… è®­ç»ƒæˆåŠŸçš„æ ‡å¿—

1. **ç¬¬ä¸€ä¸ª batch æˆåŠŸ**
   ```
   EP_Train:0:   0%|| 1/8617 [00:01<?, ?it/s]
   âœ“ æ—  CUDA fork error
   ```

2. **ç¨³å®šè®­ç»ƒ**
   ```
   EP_Train:0:  10%|| 861/8617 [05:30<50:15, 2.57it/s]
   Loss: 0.487, F1: 0.901
   âœ“ Loss å¹³ç¨³ä¸‹é™
   âœ“ é€Ÿåº¦ç¨³å®š
   ```

3. **Epoch 1 å®Œæˆ**
   ```
   Train F1: 0.943, Val F1: 0.955
   âœ“ æŒ‡æ ‡ä¼˜ç§€
   ```

4. **Mask åˆ·æ–°æˆåŠŸ**
   ```
   âœ“ Maskåˆ·æ–°å®Œæˆ! æ–°ç‰ˆæœ¬: 1
   âœ“ ç´¢å¼•é‡å»ºå®Œæˆ!
   âœ“ Maskå’Œç´¢å¼•åˆ·æ–°å®Œæˆ!
   ```

5. **æ¢¯åº¦å›ä¼ éªŒè¯**ï¼ˆå¯é€‰ï¼‰
   ```
   âœ… Embeddingå±‚æ¢¯åº¦å›ä¼ éªŒè¯: True
   ```

---

## æ€»ç»“

**æ ¸å¿ƒæ”¹è¿›**:
- âœ… çœŸæ­£çš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼ˆæ¢¯åº¦æ­£ç¡®å›ä¼ ï¼‰
- âœ… é«˜æ•ˆæ•°æ®åŠ è½½ï¼ˆå¤šworkerï¼Œæ— CUDA forkï¼‰
- âœ… æ›´å¼ºæ¨¡å‹å®¹é‡ï¼ˆ384ç»´ï¼‰
- âœ… ç¨³å®šè®­ç»ƒï¼ˆlog1p å¹³æ»‘ï¼‰

**è¿è¡Œæ­¥éª¤**:
1. `git pull origin main`
2. éªŒè¯å…³é”®ä¿®æ”¹
3. `bash run_v18_embedding_rag.sh`
4. ç›‘æ§è®­ç»ƒè¿‡ç¨‹

**é¢„æœŸæ•ˆæœ**:
- è®­ç»ƒç¨³å®šã€Loss å¹³ç¨³ä¸‹é™
- F1 åˆ†æ•°é«˜äº V17ï¼ˆç«¯åˆ°ç«¯å­¦ä¹ çš„ä¼˜åŠ¿ï¼‰
- é€Ÿåº¦ç•¥å¿«äºä¹‹å‰ï¼ˆå¤šworkerï¼‰

**ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼** ğŸš€

å¦‚æœ‰ä»»ä½•é—®é¢˜ï¼Œå‚è€ƒ [END_TO_END_LEARNABLE_RAG_FIX.md](END_TO_END_LEARNABLE_RAG_FIX.md) è·å–è¯¦ç»†æŠ€æœ¯æ–‡æ¡£ã€‚
