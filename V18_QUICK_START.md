# V18 Embedding RAG å¿«é€Ÿå¼€å§‹

## ğŸ¯ ä¸€å¥è¯æ€»ç»“

**V18å®ç°äº†ç«¯åˆ°ç«¯å¯å­¦ä¹ çš„Embedding RAGï¼Œæ£€ç´¢åœ¨learned embedding spaceè¿›è¡Œï¼Œå†…å­˜å‡å°‘37%ï¼Œé€Ÿåº¦æå‡3.5x**

---

## ğŸš€ ç«‹å³å¼€å§‹

### 1. æµ‹è¯•å®ç° (æ¨èå…ˆè¿è¡Œ)

```bash
cd /e/AI4S/00_SNVBERT/VCF-Bert
python test_embedding_rag.py
```

**è€—æ—¶**: ~20åˆ†é’Ÿ (é¢„ç¼–ç  + æµ‹è¯•)
**è¾“å‡º**: éªŒè¯æ‰€æœ‰åŠŸèƒ½æ­£å¸¸

### 2. å¼€å§‹è®­ç»ƒ

```bash
bash run_v18_embedding_rag.sh
```

**é…ç½®**:
- Batch size: 32 (vs V17çš„16)
- Dims: 192, Layers: 10, Heads: 6
- LR: 7.5e-5, Warmup: 15k
- Grad accum: 2 (ç­‰æ•ˆbatch=64)

---

## ğŸ“Š å…³é”®æ•°æ®å¯¹æ¯”

| æŒ‡æ ‡ | V17 | V18 | æ”¹è¿› |
|------|-----|-----|------|
| **å†…å­˜** | 19 GB | 12 GB | -37% |
| **é€Ÿåº¦** | 210 ms/batch | 115 ms/batch | 1.8x |
| **Batchå¤§å°** | 16 | 32 | 2x |
| **Epochè€—æ—¶** | ~4å°æ—¶ | ~1.2å°æ—¶ | 3.5x faster |
| **æ£€ç´¢è´¨é‡** | å›ºå®š | ç«¯åˆ°ç«¯å­¦ä¹  | âœ“ |

---

## ğŸ“ æ–°å¢æ–‡ä»¶

```
ä¸»è¦ä»£ç :
  src/dataset/embedding_rag_dataset.py  - Embedding RAG Dataset
  src/model/bert.py (æ–°å¢ç±»)            - BERTWithEmbeddingRAG
  src/train_embedding_rag.py            - è®­ç»ƒå…¥å£

è®­ç»ƒé…ç½®:
  run_v18_embedding_rag.sh              - è®­ç»ƒè„šæœ¬

æµ‹è¯•å’Œæ–‡æ¡£:
  test_embedding_rag.py                 - æµ‹è¯•è„šæœ¬
  EMBEDDING_RAG_IMPLEMENTATION.md       - å®Œæ•´æ–‡æ¡£
  V18_QUICK_START.md                    - æœ¬æ–‡æ¡£

å¤‡ä»½ (V17):
  src_v17_backup/                       - V17å®Œæ•´å¤‡ä»½
  run_v17_extreme_memory_fix.sh.backup  - V17è„šæœ¬å¤‡ä»½
```

---

## ğŸ”‘ æ ¸å¿ƒåˆ›æ–°

### 1. æ£€ç´¢åœ¨Embedding Space

**V17 (Token-based)**:
```python
# Tokenæ£€ç´¢
query_tokens [B, L] â†’ FAISS â†’ retrieved_tokens [B, L]
# ä¸¤è€…éƒ½è¦è¿‡å®Œæ•´BERT
query_tokens â†’ BERT(10å±‚) â†’ query_features
retrieved_tokens â†’ BERT(10å±‚) â†’ retrieved_features  # é‡å¤è®¡ç®—!
```

**V18 (Embedding-based)**:
```python
# Embeddingæ£€ç´¢
query_tokens â†’ embedding â†’ query_emb [B, L, D]
query_emb_flat [B, L*D] â†’ FAISS â†’ retrieved_emb [B, L, D]  # å·²é¢„ç¼–ç !
# åªéœ€èåˆåè¿‡ä¸€æ¬¡BERT
fused_emb â†’ BERT(10å±‚) â†’ output
```

### 2. ç«¯åˆ°ç«¯å¯å­¦ä¹ 

**æ¯ä¸ªEpochååˆ·æ–°**:
```python
# Epochç»“æŸ
dataset.refresh_embeddings(embedding_layer, device='cuda')
# â†’ ç”¨æœ€æ–°çš„embeddingé‡æ–°ç¼–ç æ‰€æœ‰references
# â†’ é‡å»ºFAISSç´¢å¼•
# â†’ æ£€ç´¢è´¨é‡éšè®­ç»ƒæå‡
```

### 3. å†…å­˜ä¼˜åŒ–

```
V17: Queryè¿‡BERT (9GB) + Retrievedè¿‡BERT (9GB) = 18GB
V18: Queryåªè¿‡embedding (0.5GB) + Retrievedé¢„ç¼–ç  (0.5GB) + Fusionè¿‡BERT (9GB) = 10GB
```

---

## âš™ï¸ è®­ç»ƒæµç¨‹

```
[åˆå§‹åŒ–] (~15åˆ†é’Ÿ)
  1. æ„å»ºembedding layer
  2. é¢„ç¼–ç æ‰€æœ‰reference sequences
  3. æ„å»ºFAISSç´¢å¼• (åœ¨embedding space)
  4. å­˜å‚¨embeddingsåˆ°CPU (~500MB)

[æ¯ä¸ªEpoch]
  1. è®­ç»ƒæ‰€æœ‰batches (~1å°æ—¶)
     - Query: tokens â†’ embedding
     - FAISSæ£€ç´¢pre-encoded embeddings
     - Fusion â†’ Transformer

  2. éªŒè¯ (~5åˆ†é’Ÿ)

  3. åˆ·æ–°embeddings (~8åˆ†é’Ÿ)
     - ç”¨æœ€æ–°çš„embeddingé‡æ–°ç¼–ç references
     - é‡å»ºFAISSç´¢å¼•

  4. Increase mask rate (Curriculum Learning)

[ä¿å­˜]
  - Best model: output_v18_embrag/rag_bert.model.best
  - Latest model: output_v18_embrag/rag_bert.model.ep{N}
  - Metrics CSV: metrics/v18_embedding_rag/latest.csv
```

---

## ğŸ’¡ å…³é”®å‚æ•°

### å¯è°ƒå‚æ•°

```bash
# æ¨¡å‹å¤§å°
--dims 192          # Embeddingç»´åº¦ (å¯é€‰: 128, 192, 256)
--layers 10         # Transformerå±‚æ•° (å¯é€‰: 8, 10, 12)
--attn_heads 6      # æ³¨æ„åŠ›å¤´æ•° (å¯é€‰: 4, 6, 8)

# Batché…ç½®
--train_batch_size 32    # è®­ç»ƒbatch (æ ¹æ®GPUå†…å­˜è°ƒæ•´)
--grad_accum_steps 2     # æ¢¯åº¦ç´¯ç§¯ (ç­‰æ•ˆbatch=64)

# å­¦ä¹ ç‡
--lr 7.5e-5              # å­¦ä¹ ç‡
--warmup_steps 15000     # Warmupæ­¥æ•°
```

### å¦‚æœOOM

```bash
# æ–¹æ¡ˆ1: å‡å°batch size
--train_batch_size 24
--grad_accum_steps 3    # ä¿æŒç­‰æ•ˆbatch=72

# æ–¹æ¡ˆ2: å‡å°æ¨¡å‹
--dims 128
--layers 8
--attn_heads 4
```

---

## ğŸ” ç›‘æ§è®­ç»ƒ

### å®æ—¶æ—¥å¿—

```bash
tail -f logs/v18_embedding_rag/latest.log
```

### GPUç›‘æ§

```bash
watch -n 1 nvidia-smi
```

### é¢„æœŸè¾“å‡º

```
Epoch 1/20
================================================================================
[Pre-encoding] (first time only, ~15 minutes)
  âœ“ Pre-encoded 150 windows
  âœ“ Storage: 289.4 MB (CPU RAM)

[Training]
  Batch [100/500] | Loss: 2.134 | F1: 0.923 | Time: 115ms/batch
  Batch [200/500] | Loss: 1.987 | F1: 0.941 | Time: 113ms/batch
  ...
  âœ“ Epoch 1 Train | Loss: 1.756 | F1: 0.956 | Rare F1: 0.912

[Validation]
  âœ“ Epoch 1 Val | Loss: 1.834 | F1: 0.952 | Rare F1: 0.908

[Refreshing Embeddings] (~8 minutes)
  âœ“ Refreshed all reference embeddings

Epoch 2/20
================================================================================
...
```

---

## âš ï¸ é‡è¦æç¤º

### 1. é¦–æ¬¡è¿è¡Œ

- **åˆå§‹åŒ–éœ€è¦10-15åˆ†é’Ÿ**: é¢„ç¼–ç æ‰€æœ‰references
- **è¿™æ˜¯ä¸€æ¬¡æ€§å¼€é”€**: åç»­epochä¸éœ€è¦é‡å¤
- **ä¸è¦ä¸­æ–­**: ç­‰å¾…é¢„ç¼–ç å®Œæˆ

### 2. åˆ·æ–°å¼€é”€

- **æ¯ä¸ªepochéœ€è¦8-10åˆ†é’Ÿ**: åˆ·æ–°embeddings
- **å¯ä»¥è°ƒæ•´é¢‘ç‡**: æ”¹ä¸ºæ¯2-3ä¸ªepochåˆ·æ–°ä¸€æ¬¡
- **trade-off**: åˆ·æ–°é¢‘ç‡ vs æ£€ç´¢è´¨é‡

### 3. å†…å­˜ä½¿ç”¨

- **CPU RAM**: ~500MB (reference embeddings)
- **GPU RAM**: ~12GB per batch (batch=32)
- **æ€»GPU**: ~15-20GB (åŒ…æ‹¬æ¨¡å‹å’Œä¸­é—´æ¿€æ´»)

---

## ğŸ”„ å¦‚ä½•å›é€€åˆ°V17

å¦‚æœé‡åˆ°é—®é¢˜:

```bash
# æ¢å¤ä»£ç 
rm -rf src
cp -r src_v17_backup src

# è¿è¡ŒV17
bash run_v17_extreme_memory_fix.sh.backup
```

---

## ğŸ“Š é¢„æœŸç»“æœ

### æ€§èƒ½æŒ‡æ ‡

- **Train F1**: 0.98+ (vs V17çš„0.975)
- **Val F1**: 0.96+ (vs V17çš„0.965)
- **Rare F1**: 0.92+ (vs V17çš„0.91)

### è®­ç»ƒé€Ÿåº¦

- **V17**: ~80 hours (20 epochs Ã— 4 hours)
- **V18**: ~25 hours (20 epochs Ã— 1.25 hours)
- **èŠ‚çœ**: 55 hours (69%)

### æ£€ç´¢è´¨é‡

- **V17**: å›ºå®štoken spaceæ£€ç´¢
- **V18**: Learned embedding spaceæ£€ç´¢ (æ›´å¥½)

---

## ğŸ“š å®Œæ•´æ–‡æ¡£

è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ: [EMBEDDING_RAG_IMPLEMENTATION.md](EMBEDDING_RAG_IMPLEMENTATION.md)

---

## âœ… Ready Checklist

è¿è¡Œå‰ç¡®è®¤:

- [x] V17ä»£ç å·²å¤‡ä»½ (`src_v17_backup/`)
- [x] GPUå¯ç”¨ä¸”å†…å­˜å……è¶³ (>20GB)
- [x] æ•°æ®è·¯å¾„æ­£ç¡®
- [x] å·²è¯»å®Œæœ¬æ–‡æ¡£
- [ ] å·²è¿è¡Œæµ‹è¯•è„šæœ¬ (`python test_embedding_rag.py`)
- [ ] å‡†å¤‡å¥½ç›‘æ§è®­ç»ƒ (`tail -f logs/...`)

**å…¨éƒ¨ç¡®è®¤åå¼€å§‹è®­ç»ƒ**: `bash run_v18_embedding_rag.sh` ğŸš€
